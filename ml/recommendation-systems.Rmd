# Recommendation systems

Recommendation systems use ratings that _users_ have given _items_ to make specific recommendations to users. Companies like Amazon, that sell many products to many customers and permit these customers to rate their products, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user. 

Netflix uses a recommendation system to predict how many _stars_ a user will give a specific movie. One star suggests it is not a good movie, whereas five stars suggests it is an excellent movie.

Here, we provide the basics of how these recommendations are predicted, motivated by some of the approaches taken by the winners of the  _Netflix challenges_.  On October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, 
[the winners were announced](http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/). You can read a good summary of how the winning algorithm was put together [here](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) 
and a more detailed explanation [here](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf).  We will now show you some of the data analysis strategies used by the winning team.

## Movielens data

The Netflix data is not publicly available, but the [GroupLens research lab](https://grouplens.org/) generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the __dslabs__ package:

```{r}
library(dslabs)
data("movielens")
```

We can see this table is in tidy format with thousands of rows:

```{r}
head(movielens)
```

Each row represents a rating given by one user to one movie. 

We can see the number of unique users that provided ratings and for how many unique movies they provided them for:


```{r}
movielens %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

If we multiply those two numbers, we get a number larger than 5 million, yet our data table has about 100,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The `gather` function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let's show the matrix for a few users:

```{r, echo=FALSE}
keep <- movielens %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId

tab <- movielens %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:20)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)

tab %>% knitr::kable()
```

You can think of the task of a recommendation system as filling in the `NA`s in the table above. To see how _sparse_ the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.

```{r sparsity-of-movie-recs, echo=FALSE, fig.width=4, fig.height=4}
users <- sample(unique(movielens$userId), 100)
rafalib::mypar()
movielens %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
  abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

This machine learning challenge is more complicated than what we have studied up to now because each outcome $Y$ has a different set of predictors. To see this, note that if we are predicting the rating for movie $i$ by user $u$, in principle, all other ratings related to movie $i$ and by user $u$ may used as predictors, but different users rate a different number of movies and different movies. Furthermore, we may be able to use information from other movies that we have determined are similar to movie $i$ or from users determined to be similar to user $u$. So, in essence, the entire matrix can be used as predictors for each cell. 

Let's look at some of the general properties of the data to better understand the challenges.

The first thing we notice is that some movies get rated more than others. Here is the distribution:

```{r movie-id-hist}
movielens %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies")
```

This should not surprise us given that there are blockbuster movies watched by millions and artsy, independent movies watched by just a few. 

Our second observation is that some users are more active than others at rating movies:

```{r movie-user-hist}
movielens %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Users")
```


## Recommendation systems as a machine learning challenge

To see how this is a type of machine learning, notice that we need to build an algorithm with data we have collected that will then be applied outside our control, as users look for movie recommendations. So let's create a test set to assess the accuracy of the models we implement.

```{r}
library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]
```

To make sure we don't include users and movies in the test set that do not appear in the training set, we remove these entries using the `semi_join` function:

```{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```


### Loss function 

The Netflix challenge used the typical error loss. They decided on a winner based on the residual mean squared error (RMSE) on a test set. We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{i,u}$. The RMSE is then defined as: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.

Remember that we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good.

Let's write a function that computes the RMSE for vectors of ratings and their corresponding predictors:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

### A first model

Let's start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. So what number should this prediction be? We can use a model based approach. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:


$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

with $\varepsilon_{i}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the "true" rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of $\mu$ and, in this case, is the average of all ratings:

```{r}
mu_hat <- mean(train_set$rating)
mu_hat
```

If we predict all unknown ratings with $\hat{\mu}$ or `mu` above, we obtain the following RMSE: 

```{r}
naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse
```

Keep in mind that if you plug in any other number, you get a higher RMSE. For example:

```{r}
predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)
```


From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better! 

As we go along, we will be comparing different approaches. Let's start by creating a results table with this naive approach:

```{r}
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
```

## Modeling movie effects

We know from experience that some movies are just generally rated higher than others. 

So our intuition that different movies are rated differently is confirmed by data. We can augment our previous model by adding the term $b_i$ to represent average ranking for movie $i$: 

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

In statistics, we usually call the $b$s as effects. However, in the Netflix challenge papers, they refer to them as "bias", thus the $b$ notation.

We can again use least squared to estimate the $b_i$ in the following way:

```{r, eval=FALSE}
fit <- lm(rating ~ as.factor(userId), data = movielens)
```

Because there are thousands of $b_i$, each movie gets one, the `lm()` function will be very slow here. We therefore don't recommend running the code above. But in this particular situation, we know that the least square estimate $\hat{b}_i$ is just the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$. So we can compute them this way (we will drop the `hat` notation in the code to represent estimates going forward):

```{r}
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

We can see that these estimates vary substantially:

```{r movie-effects}
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

Remember $\hat{\mu}=3.5$ so a $b_i = 1.5$ implies a perfect five star rating.

Let's see how much our prediction improves once we predict using $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i$:

```{r}
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```

We already see an improvement. But can we make it better?

### User effects

Let's compute the average rating for user $u$, for those that have rated over 100 movies. 

```{r user-effect-hist}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

Notice that there is substantial variability across users 
as well: some users are very cranky and others love every movie.
This implies that a further improvement to our model may be:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

where $b_u$ is a user-specific effect. So now if a cranky user (negative $b_u$) rates a great movie (positive $b_i$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5. 

To fit this model, we could again use `lm`:

```{r, eval = FALSE}
lm(rating ~ as.factor(movieId) + as.factor(userId))
```

but, for the reasons described earlier, we won't. Instead, we will compute an approximation but computing $\hat{\mu}$ and $\hat{b}_i$ and estimating $\hat{b}_u$ as the average of $y_{u,i} - \hat{\mu} - \hat{b}_i$

```{r}
user_avgs <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

We can now construct predictors and see how much the RMSE improves:

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred


model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```


## Exercises {-}

1. Load the `movielens` data. 

    ```{r, eval=FALSE}
    data("movielens")
    ```

    Compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.

  
2. We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it. 

    Among movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating.
 
  
3. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.

  

4. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use?

    A. Fill in the missing values with average rating of all movies.
    
    B. Fill in the missing values with 0.
    
    C. Fill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set.
    
    D. None of the above.
    
  
5. The `movielens` dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column `date` with the date. Hint: use the `as_datetime` function in the __lubridate__ package.


    
6. Compute the average rating for each week and plot this average against day. Hint: use the `round_date` function before you `group_by`.

  
7. The plot shows some evidence of a time effect. If we define $d_{u,i}$ as the day for user's $u$ rating of movie $i$, which of the following models is most appropriate:


    A. $Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}$.
    
    B. $Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}$.
    
    C. $Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta_i + \varepsilon_{u,i}$.
    
    D. $Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}$, with $f$ a smooth function of $d_{u,i}$.
    
  
8. The `movielens` data also has a `genres` column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.

  
9. The plot shows strong evidence of a genre effect. If we define $g_{u,i}$ as the genre for user's $u$ rating of movie $i$, which of the following models is most appropriate:


    A. $Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}$.
    
    B. $Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}$.
    
    C. $Y_{u,i} = \mu + b_i + b_u + \sum{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$.
    
    D. $Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}$, with $f$ a smooth function of $d_{u,i}$.

  


# Regularization

## Motivation

Despite the large movie to movie variation our improvement in RMSE was only about 5%. Let's explore where we made mistakes in our first model (using only movies). Here are the 10 largest mistakes:


```{r}
test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(residual = rating - (mu + b_i)) %>%
  arrange(desc(abs(residual))) %>% 
  select(title,  residual) %>% slice(1:10) %>% knitr::kable()
```

These all seem like obscure movies. Many of them have large predictions. Let's look at the top 10 worst and best movies based on $\hat{b}_i$. First, let's create a database that connects `movieId` to movie title:

```{r}
movie_titles <- movielens %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate:

```{r}
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()
```

And here are the 10 worst:

```{r}
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()
```

They all seem to be quite obscure. Let's look at how often they are rated.

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()

train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

The supposed "best" and "worst" movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of $b_i$, negative or positive, are more likely. 

These are noisy estimates that we should not trust, 
especially when it comes to prediction. Large errors can 
increase our RMSE, so we would rather be conservative
when unsure.

In previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.

Regularization permits us to penalize large estimates that 
come from small sample sizes. It has commonalities with the 
Bayesian approach that shrunk predictions. The general 
idea is to add a penalty for large values of $b_i$ to the sum of squares equation that we minimize. So having many large $b_i$ makes it harder to minimize.

## Penalized Least Squares

One way to think about this is that if we were to fit an effect to every rating, we could, of course, make the sum of squares equation by simply making each $b$ match its respective rating $Y$. This would yield an unstable estimate that changes drastically with new instances of $Y$. Remember that $Y$ is a random variable. By penalizing the equation, we optimize to be bigger when the estimated $b$ are far from 0; we then shrink the estimate towards 0. This is similar to the Bayesian approach we saw earlier.

To estimate the $b_i$ we now minimize this equation:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2$$
The first term is just least squares and the second is a penalty that gets larger when many $b_i$ are large. Using calculus we can actually show that the values of $b_i$ that minimize this equation are:

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $n_i$ is the number of ratings made for movie $i$.

This approach will have our desired effect: when $n_i$ is very large, which will give us a stable estimate, then $\lambda$ is effectively ignored since $n_i+\lambda \approx n_i$. However, when $n_i$ is small, then the estimate $\hat{b}_i(\lambda)$ is shrunken towards 0. The larger $\lambda$, the more we shrink.

Let's compute these regularized estimates of $b_i$ using 
$\lambda=3$. Later, we will see why we picked 3. 

```{r}
lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates.

```{r regularization-shrinkage}
data_frame(original = movie_avgs$b_i, 
           regularlized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
    ggplot(aes(original, regularlized, size=sqrt(n))) + 
        geom_point(shape=1, alpha=0.5)
```

Let's look at the top 10 best movies based on $\hat{b}_i(\lambda)$:

```{r}
train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

These make much more sense! Here are the top 10 worst movies:

```{r}
train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Do we improve our results?

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

This provides a very large improvement. 

## Choosing the penalty terms

Note that $\lambda$ is a tuning parameter. We can use cross-validation to choose it.

```{r best-penalty}
lambdas <- seq(0, 10, 0.25)

mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]
```

However, while we show this as an illustration, in practice we should be using full cross validation just on the train set, without using the test set until the final assessment.

We can use regularization for the estimate user effects as well. We are minimizing:

$$
\frac{1}{N} \sum_{u,i} \left(y_{i,u} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

The estimates that minimize this can be found similarly to what we did above. Here we use cross-validation to pick a $\lambda$:

```{r best-lambdas}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
    return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)  
```

For the full model, the optimal $\lambda$ is:

```{r}
lambda <- lambdas[which.min(rmses)]
lambda
```


```{r}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()

```

## Exercises {-}

An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let's simulate a dataset for 100 schools. First, let's simulate the number of students in each school.

```{r, eval=FALSE}
set.seed(1986)
n <- round(2^rnorm(1000, 8, 1))
```

Now let's assign a _true_ quality for each school completely independent from size. This is the parameter we want to estimate. 


```{r, eval=FALSE}
mu <- round(80 + 2*rt(1000, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:100), 
                      size = n, 
                      quality = mu,
                      rank = rank(-mu))
```

We can see that the top 10 schools are: 

```{r, eval=FALSE}
schools %>% top_n(10, quality) %>% arrange(desc(quality))
```

Now let's have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:

```{r, eval=FALSE}
scores <- sapply(1:nrow(schools), function(i){
  scores <- rnorm(schools$size[i], schools$quality[i], 30)
  scores
})
schools <- schools %>% mutate(score = sapply(avg_score, mean))
```


1. What are the top schools based on the average score? Show just the ID, size and the average score.


2. Compare the median school size to the median school size of the top 10 schools based on the score.


3. So according to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.


4. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what's going on. Highlight the top 10 schools based on the _true_ quality. Use the log scale transform for the size.


5. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference chapters! In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.

    Let's use regularization to pick the best schools. Remember regularization _shrinks_ deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:

    ```{r, eval=FALSE}
    overall <- mean(sapply(scores, mean))
    ```
    
    and then define, for each school, how it deviates from that average. Write code that estimates the score above the average for each school but dividing by $n + \alpha$ instead of $n$, with $n$ the schools size and $\alpha$ a regularization parameters. Try $\alpha = 3$

6. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4.  Is there a better $\lambda$? Find the $\lambda$ that minimizes the RMSE = $1/100 \sum_{i=1}^{100} (\mbox{quality} - \mbox{estimate})^2$


7. Rank the schools based on the average obtained with the best $\alpha$. Note that no small school is incorrectly included.


8.  A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don't subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean. 

# Matrix factorization

Matrix factorization is a widely used concept in machine learning. It is very much related to factor analysis, singular value decomposition (SVD) and principal component analysis (PCA). Here we describe the concept in the context of movie recommendation systems.

We have described how the model:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

accounts for movie to movie differences through the $b_i$ and user to user differences through the $b_u$. But this model leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals:

$$
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
$$

To see this, we will convert the data into a matrix so that each user gets a row and each movie gets a column so that $y_{u,i}$ is the entry in row $u$ and column $i$. For illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. We also keep Scent of a Woman (`movieId == 3252`) becuase we use it for a specific example:

```{r}
train_small <- movielens %>% 
  group_by(movieId) %>%
  filter(n() >= 50 | movieId == 3252) %>% ungroup() %>% 
  group_by(userId) %>%
  filter(n() >= 50) %>% ungroup()

y <- train_small %>% 
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()
```

We add row names and column names:

```{r}
rownames(y)<- y[,1]
y <- y[,-1]

movie_titles <- movielens %>% 
  select(movieId, title) %>%
  distinct()

colnames(y) <- with(movie_titles, title[match(colnames(y), movieId)])
```

and convert them to residuals by removing the column and row effects:

```{r}
y <- sweep(y, 2, colMeans(y, na.rm=TRUE))
y <- sweep(y, 1, rowMeans(y, na.rm=TRUE))
```

If the model above explains all the signals, and the $\varepsilon$ are just noise, then the residuals for different movies should be independent from each other. But they are not. Here is an example:

```{r godfathers-cor, warning=FALSE, message=FALSE}
m_1 <- "Godfather, The"
m_2 <- "Godfather: Part II, The"
qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2)
```

This plot says that users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. A similar relationship is seen when comparing The Godfather and Goodfellas:

```{r godfather-goodfellas-cor, warning=FALSE, message=FALSE}
m_1 <- "Godfather, The"
m_3 <- "Goodfellas"
qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3)
```

Although not as strong, there is still correlation. We see correlations between other movies as well:

```{r got-mail-sleepless-in-sea-cor, warning=FALSE, message=FALSE}
m_4 <- "You've Got Mail" 
m_5 <- "Sleepless in Seattle" 
qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5)
```

We can see a pattern:

```{r}
x <- y[, c(m_1, m_2, m_3, m_4, m_5)]
colnames(x)[1:2] <- c("Godfather", "Godfather 2")
cor(x, use="pairwise.complete") %>% knitr::kable()
```

There seems to be people that like romantic comedies more than expected, while others that like gangster movies more than expected.

These results tell us that there is structure in the data. But how can we model this?

## Factors

Here is an illustration, using a simulation, of how we can use some structure to predict the $r_{u,i}$. Suppose our residuals `r` look like this:

```{r}
q <- matrix(c(1 , 1, 1, -1, -1), ncol=1)
rownames(q) <- c("Godfather", "Godfather 2", m_3, m_4, m_5)
p <- matrix(rep(c(2,0,-2), c(3,5,4)), ncol=1)
rownames(p) <- 1:nrow(p)

set.seed(1)
r <- jitter(p %*% t(q))
round(r, 1)
```

There seems to be pattern here. In fact, we can see very strong correlation patterns:

```{r}
cor(r) 
```

We can create vectors `Q` and `P`, that can explain much of the structure we see. The `Q` would look like this:

```{r}
t(q) 
```

and it narrows down movies to two groups: gangster and romance. We can also reduce the users to three groups: 

```{r}
p
```

those that like gangster movies, but hate romance ones, the reverse, and those that don't care. The main point here is that we can reconstruct this vector of length 60 with a couple of vectors totaling 17 values. If these are the residuals for users $u=1,\dots,12$ for movies $i=1,\dots,5$ we can write the following mathematical formula for our residuals $r_{u,i}$.


$$
r_{u,i} \approx p_u q_i 
$$

This implies that we can explain more variability by modifying our previous model for movie recommendations to:

$$
Y_{u,i} = \mu + b_i + b_u + p_u q_i + \varepsilon_{i,j}
$$

However, what we motivated the need for the $p_u q_i$ term with a simple simulation. 
The structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have other factors. For example, we may have:


```{r}
set.seed(1)
m_6 <- "Scent of a Woman"
q <- cbind(c(1 , 1, 1, -1, -1, -1), 
           c(1 , 1, -1, -1, -1, 1))
rownames(q) <- c("Godfather", "Godfather 2", m_3, m_4, m_5, m_6)
p <- cbind(rep(c(2,0,-2), c(3,5,4)), 
          c(-1,1,1,0,0,1,1,1,0,-1,-1,-1))/2
rownames(p) <- 1:nrow(p)

r <- jitter(p %*% t(q), factor=1)
round(r, 1)
```

Now we see another factor: love, hates, or doesn't care about Al Pacino. The correlation is a bit more complicated now

```{r}
cor(r)
```

Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:

```{r}
six_movies <- c(m_1, m_2, m_3, m_4, m_5, m_6)
x <- y[,six_movies]
colnames(x)[1:2] <- c("Godfather", "Godfather 2")
cor(x, use="pairwise.complete")
```

To explain this more complicated structure, we need two factors. For example something like this:

```{r}
t(q) 
```

one for ganster versus romance and another for Al Pacino versus no Al Pacino.
We also need two sets of coefficients:

```{r}
p
```

to explain the $3\times 3$ types of users.

The model with two factors has more parameters, but still less than the original data:

$$
Y_{u,i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \varepsilon_{i,j}
$$

Note that this is not a linear model. To fit this model we need to use an algorithm other those used by `lm` to find the parameters that minimize the least squares. Also, the winning algorithms for the Netflix challenge used regularization to penalize for large values of $p$ and $q$, rather than using least squares. Implementing this approach is beyond the scope of this book.


## Connection to SVD and PCA

The decomposition:

$$
r_{u,i} \approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}
$$

is very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors $p$ and $q$ that permit us to rewrite the matrix $\mbox{r}$ with $m$ rows and $n$ columns as:

$$
r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \dots + p_{u,m} q_{m,i} 
$$

with the variability of each term decreasing and with the $p$s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability.

Let's see an example with the movie data. To compute the decomposition, we will make the residuals with NAs equal to 0:

```{r}
y[is.na(y)] <- 0
pca <- prcomp(y)
```

The $q$ vectors are called the principal components and they are stored in this matrix:

```{r}
dim(pca$rotation)
```

While the $p$, or the user effects, are here:

```{r}
dim(pca$x)
```

We can see the variability of each of the vectors:

```{r, pca-sds}
plot(pca$sdev)
```

and see that just the first few already explain a large percent:

```{r var-expained-pca}
var_explained <- cumsum(pca$sdev^2/sum(pca$sdev^2))
plot(var_explained)
```

We also notice that the first two principal components are related to the structure in opinions about movies:

```{r movies-pca, out.width="100%", height = 5}
library(ggrepel)

pcs <- data.frame(pca$rotation, name = colnames(y))

highlight <- filter(pcs, PC1 < -0.1 | PC1 > 0.1 | PC2 < -0.075 | PC2 > 0.1)

pcs %>%  ggplot(aes(PC1, PC2)) + geom_point() + 
  geom_text_repel(aes(PC1, PC2, label=name),
                  data = highlight, size = 2)
```

Just by looking at the top 10 in each direction, we see a meaningful pattern. The first PC shows the difference between critically acclaimed movies on one side:

```{r}
pcs %>% select(name, PC1) %>% arrange(PC1) %>% slice(1:10)
```

and Hollywood blockbusters on the other:

```{r}
pcs %>% select(name, PC1) %>% arrange(desc(PC1)) %>% slice(1:10)
```

While the second PC seems to go from artsy, independent films:

```{r}
pcs %>% select(name, PC2) %>% arrange(PC2) %>% slice(1:10)
```

to nerd favorites:

```{r}
pcs %>% select(name, PC2) %>% arrange(desc(PC2)) %>% slice(1:10)
```

Fitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the __recommenderlab__ package. The details are beyond the scope of this book.


## Exercises {-}

In this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.

The SVD tells us that we can _decompose_ an $N\times p$ matrix $Y$ with $p < N$ as 

$$ Y = U D V^{\top} $$

With $U$ and $V$ _orthogonal_ of dimensions $N\times p$ and $p\times p$ respectively and $D$ a $p \times p$ _diagonal_ matrix with the values of the diagonal decreasing: 

$$d_{1,1} \geq d_{2,2} \geq \dots d_{p,p}$$. 

In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:


```{r, eval=FALSE}
set.seed(1987)
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))
``` 

Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just a random independent numbers. Are all students just about as good? Does being good in one subject  imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this $100 \times 24$ dataset. 

You can visualize the 24 test scores for the 100 students by plotting an image:

```{r, eval=FALSE}
my_image <- function(x, zlim = range(x), ...){
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
  cols <- 1:ncol(x)
  rows <- 1:nrow(x)
  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
        xlab="", ylab="",  col = colors, zlim = zlim, ...)
  abline(h=rows + 0.5, v = cols + 0.5)
  axis(side = 1, cols, colnames(x), las = 2)
}

my_image(y)
```

1. How would you describe the data based on this figure?

    A. The test scores are all independent of each other.
    B. The students that test well are at the top of the image and there seems to be three groupings by subject.
    C. The students that are good at math are not good at science.
    D. The students that are good at math are not good at humanities.
    
2. You can examine the correlation between the test scores directly like this:

    ```{r, eval=FALSE}
    my_image(cor(y), zlim = c(-1,1))
    range(cor(y))
    axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
    ```

    Which of the following best describes what you see?

    A. The test scores are independent.
    B. Math and Science are highly correlated but the humanities are not.
    C. There is high correlation between tests in the same subject but no correlation across subjects.
    D. There is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.
    

3. Remember that orthogonality means that $U^{\top}U$ and  $V^{\top}V$ are equal to the identity matrix. This implies that we can also rewrite the decomposition as

    $$ Y V = U D \mbox{ or } U^{\top}Y = D V^{\top}$$ 

    We can think of $YV$ and $U^{\top}V$ as two transformation of Y that preserve the total variability of $Y$ since $U$ and $V$ are orthogonal. 

    Use the function `svd` to compute the SVD of `y`. This function will return $U$, $V$ and the diagonal entries of $D$. 

    ```{r, eval=FALSE}
    s <- svd(y)
    names(s)
    ```
    
    You can check that the SVD works by typing:

    ```{r, eval=FALSE}
    y_svd <- s$u %*% diag(s$d) %*% t(s$v)
    max(abs(y - y_svd))
    ```

    Compute the sum of squares of the columns of $Y$ and store them in `ss_y`. Then the sum of squares of columns of the transformed $YV$ and store them in $ss_yv$. Confirm that `sum(ss_y)` is equal to `sum(ss_yv)`

 
4. We see that the total sum of squares is preserved. This is because $V$ is orthogonal. Now to start understanding how $YV$ is useful 
plot `ss_y` against the column number and then do the same for $ss_yv$. What do you observe?

5. We see that the variability of the columns of $YV$ is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn't have to compute `ss_yv` because we already have the answer. How? Remember that $YV = UD$ and because $U$ is orthogonal, we know that the sum of squares of the columns of $UD$ are the diagonal entries of $D$ squared. Confirm this by plotting the square root of $ss_yv$ versus the diagonal entries of $D$

 
6. So from the above we know that the sum of squares of the columns of $Y$ (the total sum of squares) add up to the sum of `s$d^2` and that the transformation $YV$ gives us columns with sums of squares equal to `s$d^2`. Now compute what percent of the total variability is explained by just the first three columns of $YV$.

    
7. We see that almost 99% of the variability is explained by the first three columns of $YV  = UD$. So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let's show a useful computational trick to avoid creating the matrix `diag(s$d)`. To motivate this, we note that if we write $U$ out in its columns $[U_1, U_2, \dots, U_p]$ then $UD$ is equal to 

    $$UD = [U_1 d_{1,1}, U_2 d_{2,2}, \dots, U_p d_{p,p}]$$

    Use the `sweep` function to compute $UD$ without constructing `diag(s$d)` nor matrix multiplication.

    

8. We know that $U_1 d_{1,1}$, the first column of $UD$, has the most variability of all the columns of $UD$. Earlier we saw an image of $Y$ 

    ```{r, eval=FALSE}
    my_image(y)
    ```

    in which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against $U_1 d_{1,1}$ and describe what you find.

 

9. We note that the signs in SVD are arbitrary because: 

    $$ U D V^{\top} = (-U) D (-V)^{\top} $$


    With this in mind we see that the first column of $UD$ is almost identical to the average score for each student except for the sign.

    This implies that multiplying $Y$ by the first column of $V$ must be performing a similar operation to taking the average. Make an image plot of $V$ and describe the first column relative to others and how this relates to taking an average.

 
10. We already saw that we can rewrite $UD$ as

    $$U_1 d_{1,1} + U_2 d_{2,2} + \dots + U_p d_{p,p}$$

    with $U_j$ the j-th column of $U$. This implies that we can rewrite the entire SVD as:

    $$Y = U_1 d_{1,1} V_1 ^{\top} + U_2 d_{2,2} V_2 ^{\top} + \dots + U_p d_{p,p} V_p ^{\top}$$

    with $V_j$ the jth column of $V$. Plot $U_1$, then plot $V_1^{\top}$ using the same range for the y-axis limits, then make an image of $U_1 d_{1,1} V_1 ^{\top}$ and compare it to the image of $Y$. Hint: use the `my_image` function defined above. Use the `drop=FALSE` argument to assure the subsets of matrices are matrices.

  

11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the a $100 \times 24$ matrix. This is our first matrix factorization:

    $$ Y \approx d_{1,1} U_1 V_1^{\top}$$

    We know it explains `s$d[1]^2/sum(s$d^2) * 100` percent of the total variability.  Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:

    ```{r, eval=FALSE}
    resid <- y - with(s,(u[, 1, drop=FALSE]*d[1]) %*% t(v[, 1, drop=FALSE]))
    my_image(cor(resid), zlim = c(-1,1))
    axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
    ```
    
    Now that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let's explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot $U_2$, then plot $V_2^{\top}$ using the same range for the y-axis limits, then make an image of $U_2 d_{2,2} V_2 ^{\top}$ and compare it to the image of `resid`.

  
12. The second column clearly relates to a student's difference in ability in math/science versus the arts. We can see this most clearly from the plot of `s$v[,2]`. Adding the matrix we obtain with these two columns will help with our approximation:


    $$ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} $$

    We know it will explain 

        ```{r,eval=FALSE}
        sum(s$d[1:2]^2)/sum(s$d^2) * 100
        ```

    percent of the total variability. We can compute new residuals like this:
    
    ```{r,eval=FALSE}
    resid <- y - with(s,sweep(u[, 1:2], 2, d[1:2], FUN="*") %*% t(v[, 1:2]))
    my_image(cor(resid), zlim = c(-1,1))
    axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
    ```
    
    and see that the structure that is left is driven by the differences between math and science. Confirm this by plotting  $U_3$, then plot $V_3^{\top}$ using the same range for the y-axis limits, then make an image of $U_3 d_{3,3} V_3 ^{\top}$ and compare it to the image of `resid`.


  
13. The third column clearly relates to a student's difference in ability in math and science. We can see this most clearly from the plot of `s$v[,3]`. Adding the matrix we obtain with these two columns will help with our approximation:


    $$ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}$$

    We know it will explain:

    ```{r,eval=FALSE}
    sum(s$d[1:3]^2)/sum(s$d^2) * 100
    ```
    
    percent of the total variability. We can compute new residuals like this:

    ```{r,eval=FALSE}
    resid <- y - with(s,sweep(u[, 1:3], 2, d[1:3], FUN="*") %*% t(v[, 1:3]))
    my_image(cor(resid), zlim = c(-1,1))
    axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
    ```

    We no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:

    $$ Y =  d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top} + \varepsilon$$

    with $\varepsilon$ a matrix of independent identically distributed errors. This model is useful because we summarize of $100 \times 24$ observations with $3 \times (100+24+1) = 375$ numbers. Furthermore, the three components of the model have useful interpretations: 1 - the overall ability of a student, 2 - the difference in ability between the math/sciences and arts and 3 - the remaining differences between the three subjects. The sizes $d_{1,1}, d_{2,2}$ and $d_{3,3}$ tells us the variability explained by each component. Finally, note that the components $d_{j,j} U_j V_j^{\top}$ are equivalent to the jth principal component. Finish the exercise by plotting an image of  $Y$, an image of $d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}$ and an image of the residuals, all with the same `zlim`.

  

14. Advanced. The `movielens` dataset included` in the __dslabs__ package is a small subset of a larger dataset with millions of ratings. You can find the entire latest dataset here [https://grouplens.org/datasets/movielens/20m/](https://grouplens.org/datasets/movielens/20m/). Create your own recommendation system using all the tools we have shown you.
