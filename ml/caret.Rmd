# The caret package

We have already learned about regression, logistic regression, and kNN as machine learning algorithms. In later sections, we learn several others. And this is just a small subset of all the algorithms out there. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The __caret__ package tries to consolidate these differences and provide consistency. It currently includes 237 different methods which are summarized [here](https://topepo.github.io/caret/available-models.html). Keep in mind that caret does not include the packages and, to implement a package through caret, you still need to install the library. The required package for each method is included [here](https://topepo.github.io/caret/train-models-by-tag.html).

The __caret__ package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this incredibly helpful package. We will use the 2 or 7 example to illustrate:

```{r}
data("mnist_27")
```


## The `train` functon

The `train` function lets us train different algorithms using similar syntax. So, for example, we can type:

```{r}
library(caret)
train_glm <- train(y ~ ., method = "glm", data = mnist_27$train)
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
```

To make predictions, we can use the output of this function directly without needing to look at the specifics of `predict.glm` and `predict.knn`. Instead, we can learn how to obtain predictions from `predict.train`.

So the code looks the same for both methods:
```{r}
y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw")
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
```

This permits us to quickly compare the algorithms. For example, we can compare the accuracy like this:

```{r}
confusionMatrix(y_hat_glm, mnist_27$test$y)$overall["Accuracy"]
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```

## Cross validation

When an algorithm includes a tuning parameter, `train` automatically uses cross validation to decide among a few default values. To find out what parameter or parameters are optimized, you can read [this](http://topepo.github.io/caret/available-models.html) or study the output of: 

```{r, eval=FALSE}
getModelInfo("knn")
```

We can also use a quick look up like this:

```{r}
modelLookup("knn")
```

If we run it with default values: 

```{r}
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
```

you can quickly see the results of the cross-validation using the `ggplot` function. The argument `highlight` highlights the max:

```{r caret-highlight}
ggplot(train_knn, highlight = TRUE)
```

By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. For the `kNN` method, the default is to try $k=5,7,9$. To change this we use the `tuneGrid` parameters. The grid of values must be supplied by a data frame with the parameter names as specified in the `modelLookup` output. 

Here, we present an example where we try out 30 values between 9 and 67. To do this with caret, we need to define a column named `k`, so we use this: 
`data.frame(k = seq(9, 67, 2))`.

Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples. Since we are fitting $30 \times 25 = 750$ kNN models, running this code will take several seconds:

```{r train-knn-plot}
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```

To access the parameter that maximized the accuracy, you can use this:

```{r}
train_knn$bestTune
```

and the best performing model like this:

```{r}
train_knn$finalModel
```

The function `predict` will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:

```{r}
confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),
                mnist_27$test$y)$overall["Accuracy"]
```

If we want to change of how we perform cross validation, we can use the `trainControl` function. We can make the code above go a bit faster by using, for example, 10-fold cross validation. This means we have 10 samples using 10% of the observations each. We accomplish this using the following code:

```{r cv-10-fold-accuracy-estimate}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)),
                   trControl = control)
ggplot(train_knn_cv, highlight = TRUE)
```

We notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.

We can also use the standard deviation bars obtained from the cross validation samples:

```{r accuracy-with-sd-bars}
train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD, 
                    ymax = Accuracy + AccuracySD))
```


## Example: fitting with loess

```{r, echo=FALSE}
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
  geom_raster(show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5),color="black")
}
```

The best fitting kNN model approximates the true conditional probability:
```{r mnist27-optimal-knn-fit, echo=FALSE}
plot_cond_prob(predict(train_knn, mnist_27$true_p, type = "prob")[,2])
```

However, we do see that the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the [available models](https://topepo.github.io/caret/available-models.html) we see that we can use the `gamLoess` method. 

[Here](https://topepo.github.io/caret/train-models-by-tag.html), we see that we need to install the __gam__ package if we have not done so already:

```{r, eval=FALSE}
install.packages("gam")
```

Then we see that we have two parameters to optimize:

```{r}
modelLookup("gamLoess")
```

We will stick to a degree of 1. But to try out different values for the span, we still have to include a column in the table with the name `degree` so we can do this:

```{r}
grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)
```

We will use the default cross validation control parameters.

```{r loess-accuracy, warning=FALSE, message=FALSE}
train_loess <- train(y ~ ., 
                   method = "gamLoess", 
                   tuneGrid=grid,
                   data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)
```

We can see that the method performs similar to kNN:

```{r}
confusionMatrix(data =predict(train_loess, mnist_27$test), 
                reference = mnist_27$test$y)$overall["Accuracy"]
```

and produces a smoother estimate of the conditional probability:

```{r gam-smooth, warning=FALSE}
plot_cond_prob(predict(train_loess, mnist_27$true_p, type = "prob")[,2])
```


