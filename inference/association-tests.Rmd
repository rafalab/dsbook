# Association Tests

```{r, echo=FALSE}
library(tidyverse)
library(dslabs)
set.seed(1)
```

The statistical tests we have studied up to now leave out a
substantial portion of data types. Specifically, we have not discussed inference for binary, categorical and ordinal data. To give a
very specific example, consider the following case study.


A [2014 PNAS paper](http://www.pnas.org/content/112/40/12349.abstract) analyzed success rates from funding agencies in the Netherlands and concluded that their:

> results reveal gender bias favoring male applicants over female applicants in the prioritization of their "quality of researcher" (but not "quality of proposal") evaluations and success rates, as well as in the language use in instructional and evaluation materials.

The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:

```{r}
data("research_funding_rates")
research_funding_rates %>% 
  select(discipline, contains("total"))
```

We have these values for each gender:
```{r}
names(research_funding_rates)
```

We can compute the totals that were successful and the totals that were not as follows:

```{r}
totals <- research_funding_rates %>% 
  select(-discipline) %>% 
  summarize_all(funs(sum)) %>%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 
```

So we see that a larger percent of men than women received awards:

```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

But could this be due just to random variability? 
Here we learn how to perform inference for this type of data.


## Lady Tasting Tea


[R.A. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) was one of the first to formalize hypothesis testing. The "Lady Testing Tea" is one of the most famous examples. 

The story is as follows: an acquaintance of Fisher's claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of
cups of tea: one with milk poured first, the other after. The order
was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.

As an example, suppose she picked 3 out of 4 correctly, do we believe
she has a special ability? The basic question we ask is: if the tester is actually guessing, what
are the chances that she gets 3 or more correct? Just as we have done
before, we can compute a probability under the null hypothesis that she
is guessing 4 of each. Under this null hypothesis, we can
think of this particular example as picking 4 balls out of an urn
with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.

Under the null hypothesis that she is simply guessing, each ball
has the same chance of being picked. We can then use combinations to
figure out each probability. The probability of picking 3 is
${4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70$. The probability of
picking all 4 correct is
${4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70$.
Thus, the chance of observing a 3 or something more extreme,
under the null hypothesis, is $\approx 0.24$. This is the p-value. The
procedure that produced this p-value is called _Fisher's exact test_ and
it uses the *hypergeometric distribution*. 

## Two-by-two tables

The data from the experiment is usually summarized by a table like this:

```{r}
tab <- matrix(c(3,1,1,3),2,2)
rownames(tab)<-c("Poured Before","Poured After")
colnames(tab)<-c("Guessed before","Guessed after")
tab
```

These are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence.

The function `fisher.test` performs the inference calculations above and can be obtained like this:

```{r}
fisher.test(tab, alternative="greater")
```

## Chi-square Test

Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four before tea and four after tea and the lady knew this, so the answers would also have to be four and four. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two
by two table and also permits us to use the hypergeometric
distribution. In general, this is not the case. Nonetheless, there is
another approach, the Chi-squared test, which is described below. 


Imagine we have `r sum(totals)` applicants, some are men and some are women and some get funded, whereas other don't. We saw that the success rates for men and woman were:

```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

respectively. Would we see this again if we randomly assign funding at the rate:

```{r}
funding_rate <- totals %>%
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) %>%
  .$percent_total
funding_rate
```

The Chi-square test answers this question. The first step is to create the two-by-two data table:

```{r}
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
```

The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:

```{r}
data.frame(awarded = c("no", "yes"), 
       men = (totals$no_men + totals$yes_men) * 
         c(1 - funding_rate, funding_rate),
       women = (totals$no_women + totals$yes_women)*
         c(1 - funding_rate, funding_rate))

```

We can see that more men than expected and less women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see
a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. 
The R function `chisq.test` takes a two by two table and returns the results from the test:

```{r}
chisq_test <- two_by_two %>% 
  select(-awarded) %>%
  chisq.test()
chisq_test
```

We see that the p-value is 0.0509:

```{r}
chisq_test$p.value
```


## The Odds Ratio

An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as $X = 1$ if you are a male and 0 otherwise, and $Y=1$ if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined:

$$\mbox{Pr}(Y=1 \mid X=1) / \mbox{Pr}(Y=0 \mid X=1)$$

and can be computed like this:
```{r}
odds_men <- (two_by_two$men[2] / sum(two_by_two$men)) / 
  (two_by_two$men[1] / sum(two_by_two$men))
```

And the odds of being funded if you are a woman is:


$$\mbox{Pr}(Y=1 \mid X=0) / \mbox{Pr}(Y=0 \mid X=0)$$


and can be computed like this:
```{r}
odds_women <- (two_by_two$women[2] / sum(two_by_two$women)) / 
  (two_by_two$women[1] / sum(two_by_two$women))
```

The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women?

```{r}
odds_men / odds_women
```

We often see two by two tables written out as 

```{r, echo=FALSE}
mat <- cbind(c(" a "," c "), c(" b "," d "))
colnames(mat) <- c("Men","Women")
rownames(mat) <- c("Awarded", "Not Awarded")
mat %>% knitr::kable()
```

In this case, the odds ratio is $\frac{a/c}{b/d}$
which is equivalent to $(ad) / (bc)$

## Large samples, small p-values

As mentioned earlier, reporting only p-values is not an appropriate
way to report the results of data analysis. In scientific journals, for example, 
some studies seem to overemphasize p-values. Some of these studies have large sample sizes
and report impressively small p-values.  Yet when one looks closely at
the results, we realize odds ratios are quite modest: barely bigger
than 1. In this case the difference may not be *practically significant* or *scientifically significant*.

Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean very large odds ratio.

Notice what happens to the p-value if we multiply our two-by-two table by 10:

```{r}
two_by_two %>% 
  select(-awarded) %>%
  mutate(men = men*10, women = women*10) %>%
  chisq.test() 
```

Yet the odds ratio is unchanged.

## Confidence intervals for the odds ratio

Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a
ratio, but a ratio of ratios. Therefore, there is no simple way of
using, for example, the CLT. 
  
However, statistical theory tells us that when all four entries of the two by two table are large enough, then the log of the odds ratio is approximately normal with standard error 

$$
\sqrt{1/a + 1/b + 1/c + 1/d} 
$$

This implies that a 95% confidence interval for the log odds ratio can be formed by:

$$
\log \{ (ad) / (cd) \} \pm 1.96 \sqrt{1/a + 1/b + 1/c + 1/d} 
$$

By exponentiating these two numbers we can construct a confidence interval of the odds ratio. 

Using R we can compute this confidence interval as follows:
```{r}
log_or <- log( odds_men / odds_women )
se <- two_by_two %>% 
  select(-awarded) %>%
  summarize(se = sqrt(sum(1/men) + sum(1/women))) %>%
  .$se
ci <- log_or + c(-1,1) * qnorm(0.975) * se
ci
```

If we want to convert it back to the odds ratio scale, we can exponentiate:

```{r}
exp(ci)
```

Note that 0 is not included in the confidence interval for the log odds ratio (1 not included for odds ratio) which must mean that the p-value is smaller then 0.05. We can confirm this using:

```{r}
2*(1 - pnorm(log_or, 0, se))
```

This is a slightly different p-value that with the chi-squared test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio consult the book by
[McCullagh and Nelder, 1989](https://books.google.com/books?hl=en&lr=&id=h9kFH2_FfBkC).


## Exercises {-}

1. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi squared test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.

  

2. Why did we use the Chi square test instead of Fisher's exact test in the previous exercise?

    A. It actually does not matter, since they give the exact same p-value.
    
    B. Fisher's exact and the Chi squareare different names for the same test.
    
    C. Because the sum of the rows and columns of the two by two table ere not fixedso the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher's exact test is rarely applicable with observational data.
    
    D. Because the Chi square test runs faster.

  
3. Compute the odds ratio of losing under pressure along with a confidence interval.

  
4. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this?

    A. We made a mistake in our code.
    
    B. These are not t-tests so the connection between p-value and confidence intervals does not apply.
    
    C. Different approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better.
    
    D. We should use the Fisher exact test to get confidence intervals.
    
  
5. Multiply the two by two table by 2 and see if the p-value and confidence retrieval are a better match.

    
    


