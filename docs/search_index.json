[
["index.html", "Introduction to Data Science Preface", " Introduction to Data Science Data Analysis and Prediction Algorithms with R Rafael A. Irizarry 2018-11-13 Preface This book started out as the class notes used in the HarvardX Data Science Series. The link for the online version of the book is https://rafalab.github.io/dsbook/ The R markdown code used to generate the book are available on GitHub. Note that the individual files are not self contained since we run the code included in this file before each one while creating the book. In particular, most chapters require these packages to be loaded: library(tidyverse) library(dslabs) The graphical theme used for plots throughout the book can be recreated using the ds_theme_set() function from dslabs package. This work is licensed under the Creative Commons Attribution 3.0 Unported (CC-BY) United States License. We make annonucements related to the book on Twitter. For updates follow @rafalab "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements This book is dedicated to all the people involved in building and maintaining R and the R packages we use in this book. A special thanks to the developers and maintainers of R base, the tidyverse and the caret package. A special thanks to my tidyverse guru David Robinson. Also, many thanks to Stephanie Hicks who twice served as a co-instructor in my data science class and Yihui Xie who patiently put up with my many questions about bookdown. Thanks also to Karl Broman, from whom I borrowed ideas for the Data Visualization and Productivity Tools parts, and to Hector Corrada-Bravo, for advice on how to best teach Machine Learning. Thanks to Peter Aldhous from whom I borrowed ideas for the principles of data visualization section and Jenny Bryan for writing Happy Git and GitHub for the useR, which influenced our Git chapters. Thanks to Alyssa Frazee for helping create the homework problem that became the Recommendation Systems chapter. Also, many thanks to Jeff Leek, Roger Peng and Brian Caffo whose class inspired the way this book is divided and to Garrett Grolemund and Hadley Wickham for making the bookdown code for their R for Data Science book open. Finally, thanks to Alex Nones for proofreading the manuscript during its various stages. This book was conceived during the teaching of several applied statistics courses, starting over fifteen years ago. The teaching assistants working with me throughout the years made important indirect contributions to this book. The latest iteration of this course is a HarvardX series coordinated by Heather Sternshein. We thank her for her contributions. We are also grateful to all the students whose questions and comments helped us improve the book. The courses were partially funded by NIH grant R25GM114818. We are very grateful to the National Institute of Health for its support. A special thanks goes to all those that edited the book via GitHub pull requests or made suggestions by creating an issue: Huang Qiang (nickyfoto), Marc-André Désautels (desautm), Michail Schwab (michaschwab), Alvaro Larreategui (alvarolarreategui), Jake VanCampen (jakevc), Guillermo Lengemann (omerta), Amy Gill (gillsignals), and Matthew Ploenzke (mPloenzke). "],
["introduction.html", "Chapter 1 Introduction 1.1 Case studies 1.2 Who will find this book useful? 1.3 What does this book cover? 1.4 What is not covered by this book?", " Chapter 1 Introduction The demand for skilled data science practitioners in industry, academia, and government is rapidly growing. This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with Git and GitHub, and reproducible document preparation with knitr and R markdown. The book is divided into eight parts: R Basics, Data Visualization, Probability, Statistical Inference and Modeling, Data Wrangling, Regression, Machine Learning, and Productivity Tools. Each part has about 12 chapters, many of which include between 3-16 exercises. 1.1 Case studies Throughout the series, we use motivating case studies. For each of the concepts covered, we start by asking specific questions and answer these through data analysis. We learn the concepts as a means to answer the questions. Examples of the case studies included in the book are: Case Study Concept US murder rates by state R basics Student heights Statistical summaries Trends in world health and economics Data Visualization The impact of vaccines on infectious disease rates Data Visualization The financial crisis of 2007-2008 Probability Election forecasting Statistical Inference Reported student heights Data Wrangling Money Ball: Building a baseball team Linear Regression MNIST: Image processing hand written digits Machine Learning Movie recommendation systems Machine Learning We use the R software environment for all our analysis. You will learn R, statistical concepts, and data analysis techniques simultaneously. By working through the examples, we gain experience in R and packages such as dplyr, ggplot2, and caret. In each case study, we try to realistically mimic a data scientist’s experience. 1.2 Who will find this book useful? This book is meant to be a textbook for a first course in Data Science. No previous knowledge of R or statistics is necessary, although we do assume a high level of quantitative ability and some experience with programming. If you possess these skills, read all the chapters and complete all the exercises, you will be well positioned to perform basic data analysis tasks and you will be prepared to learn the more advanced concepts and skills needed to become an expert. 1.3 What does this book cover? We start by going over the basics of R. You learn R throughout the book, but in the first chapter we go over the building blocks needed to keep learning during the rest of the book. Data Visualization is the topic of the following chapter. The growing availability of informative datasets and software tools has led to increased reliance on Data Visualizations in many fields. We demonstrate how to use ggplot2 to generate graphs and describe important Data Visualization principles. In the third part, we introduce Probability which is useful in many contexts, in particular those that depend on data affected by chance in some way. As a result, knowledge of probability is indispensable for data analysis. Probability is also necessary to understand the topic of the next part: Statistical Inference and Modeling. Here we describe the statistical theory used by election forecasting. The fifth part uses several examples to familiarize the reader with Data Wrangling. Among the specific skills we learn are data import, web scrapping, using regular expressions, and reshaping data. We do this using tidyverse tools. After describing these skills and concepts, we are ready to embark on more complex data analysis tasks. In Part 7 we delineate challenges that lead us to describe linear regression and, in Chapter 8, challenges that lead us to introduce Machine Learning. In the final chapter, we provide a brief introduction to the tools we use on a day-to-day basis in data science projects. These are RStudio, UNIX/Linux shell, Git and GitHub, and knitr and R Markdown. 1.4 What is not covered by this book? This book focuses on the data analysis aspects of data science. We therefore do not cover aspects related to data management or engineering. Although R programming is an essential part of the book, we do not teach more advanced topics such as data structures, optimization, and algorithm theory. Similarly, we do not cover topics such as web services, interactive graphics, parallel computing, and data streaming processing. "],
["case-study-us-gun-murders.html", "Chapter 2 Case study: US Gun Murders", " Chapter 2 Case study: US Gun Murders Imagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries have you worried. Charts like this may concern you even more: (Source: Ma’ayan Rosenzweigh/ABC News, Data from UNODC Homicide Statistics) Or even worse, this version from everytown.org: (Source everytown.org) But then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC). California, for example, has a larger population than Canada and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US during 2010 using R. Before we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills. "],
["getting-started.html", "Chapter 3 Getting Started 3.1 Why R? 3.2 Installing R 3.3 The R console 3.4 Scripts 3.5 Installing RStudio 3.6 The R ecosystem", " Chapter 3 Getting Started 3.1 Why R? R is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history here. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used to since you will be disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and, specifically, data visualization. Other attractive features of R are: R is free and open source. It runs on all major platforms: Windows, Mac Os, UNIX/Linux. Scripts and data objects can be shared seamlessly across platforms. There is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions. It is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. This gives R users early access to the latest methods and to tools which are developed for a wide variety of disciplines, including ecology, molecular biology, social sciences and geography, just to name a few examples. 3.2 Installing R In this chapter we provide a very brief description on how to install R and RStudio. However, note that this book includes an entire part dedicated to data science software tools titled Productivity Tools. Those wanting more details can skip ahead to that part. Here we briefly describe the basics of the installation process. You can download R freely from the Comprehensive R Archive Network (CRAN). It is relatively straightforward, but if you need further help you should conult the Productivity Tools part of this book. If you want to try out R without installing it, you can access a web based console such as R fiddle. 3.3 The R console Interactive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this: As a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71: 0.15 * 19.71 #&gt; [1] 2.96 Note that in this book, grey boxes are used to show R code typed into the R console. The symbol #&gt; is used to denote what the R console outputs. 3.4 Scripts One of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. We highly recommend working on an interactive integrated development environment (IDE) such as RStudio. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures. Most web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. 3.5 Installing RStudio Detailed instructions on how to install RStudio can be found in the Productivity Tools part of this book. Once you install RStudio, you can simply start RStudio rather than R since that program automatically starts R. But don’t be confused: R and RStudio are different pieces of software and you can’t run RStudio without first installing R. If you are going to follow along with RStudio as you read this book, you might consider reading the Productivity Tools chapter specifically dedicated to RStudio. Besides a script editor, RStudio includes many useful features and you want to familiarize yourself with these. And remember the R scripts used to generate this book can be found on GitHub. 3.6 The R ecosystem The functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, we instead make different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package which we use to share datasets and code related to this book, you would type: install.packages(&quot;dslabs&quot;) In RStudio, you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function: library(dslabs) As you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to install it first. "],
["the-very-basics.html", "Chapter 4 The very basics 4.1 Objects 4.2 The workspace 4.3 Functions 4.4 Other prebuilt objects 4.5 Variable names 4.6 Saving your workspace 4.7 Scripts 4.8 Comments 4.9 Exercises", " Chapter 4 The very basics Before we get started with the motivating dataset, we need to cover the very basics of R. 4.1 Objects Suppose a high school student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions: \\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\] which of course change depending on the values of \\(a\\), \\(b\\), and \\(c\\). One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define: a &lt;- 1 b &lt;- 1 c &lt;- -1 which stores the values for later use. We use &lt;- to assign values to the variables. We can also assign values using = instead of &lt;-, but we recommend against using = to avoid confusion. Copy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. To see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value: a #&gt; [1] 1 A more explicit way to ask R to show us the value stored in a is using print like this: print(a) #&gt; [1] 1 We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later. 4.2 The workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing: ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;murders&quot; In RStudio, the Environment tab shows the values: We should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found. Now since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula: (-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) #&gt; [1] 0.618 (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) #&gt; [1] -1.62 4.3 Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these. We already used the install.packages, library, and ls functions. We also used the function sqrt to solve the quadratic equation above. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use. In general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace. Unlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1: log(8) #&gt; [1] 2.08 log(a) #&gt; [1] 0 You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this: help(&quot;log&quot;) For most functions, we can also use this shorthand: ?log The help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default. If you want a quick look at the arguments without opening the help system, you can type: args(log) #&gt; function (x, base = exp(1)) #&gt; NULL You can change the default values by simply assigning another object: log(8, base = 2) #&gt; [1] 3 Note that we have not been specifying the argument x as such: log(x = 8, base = 2) #&gt; [1] 3 The above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base: log(8,2) #&gt; [1] 3 If using the arguments’ names, then we can include them in whatever order we want: log(base = 2, x = 8) #&gt; [1] 3 To specify arguments, we must use =, and cannot use &lt;-. There are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example: 2 ^ 3 #&gt; [1] 8 You can see the arithmetic operators by typing: help(&quot;+&quot;) or ?&quot;+&quot; and the relational operators by typing: help(&quot;&gt;&quot;) or ?&quot;&gt;&quot; 4.4 Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing: data() This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type: co2 R will show you Mauna Loa atmospheric CO2 concentration data. Other prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\): pi #&gt; [1] 3.14 Inf+1 #&gt; [1] Inf 4.5 Variable names We have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R is that they have to start with a letter, can’t contain spaces and should not be variables that are predefined in R. For example, don’t use install.packages by typing something like install.packages &lt;- 2. A nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this: solution_1 &lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a) solution_2 &lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a) ** For more advice, I highly recommend studying Hadley Wickham’s style guide. ** 4.6 Saving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the programs asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace. We actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image and load to learn more. 4.7 Scripts To solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above, but this time redefine the variables and recompute the solution: a &lt;- 3 b &lt;- 2 c &lt;- -1 (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a) By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how much easier it is to change the variables and receive an answer. 4.8 Comments If a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote a particular code. For example, in the script above we could add: ## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a &lt;- 3 b &lt;- 2 c &lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a) 4.9 Exercises What is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum? Now use the same formula to compute the sum of the integers from 1 through 1,000. Look at the result of typing the following code into R: n &lt;- 1000 x &lt;- seq(1, n) sum(x) Based on the result, what do you think the functions seq and sum do? You can use the help system: A. sum creates a list of numbers and seq adds them up. B. seq creates a list of numbers and sum adds them up. C. seq computes the difference between two arguments and sum computes the sum of 1 through 1000. D. sum always returns the same number. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want. A. log(10^x) B. log10(x^10) C. log(exp(x)) D. exp(log(x, base = 2)) "],
["data-types.html", "Chapter 5 Data types 5.1 Data frames 5.2 Examining an object 5.3 The accessor: $ 5.4 Vectors: numerics, characters, and logical 5.5 Factors 5.6 Lists 5.7 Matrices 5.8 Exercises", " Chapter 5 Data types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have: a &lt;- 2 class(a) #&gt; [1] &quot;numeric&quot; To work efficiently in R, it is important to learn the different types of variables and what we can do with these. 5.1 Data frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object. We stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function: library(dslabs) data(murders) To see that this is in fact a data frame, we type: class(murders) #&gt; [1] &quot;data.frame&quot; 5.2 Examining an object The function str is useful for finding out more about the structure of an object: str(murders) #&gt; &#39;data.frame&#39;: 51 obs. of 5 variables: #&gt; $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 2 ... #&gt; $ population: num 4779736 710231 6392017 2915918 37253956 ... #&gt; $ total : num 135 19 232 93 1257 ... This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head: head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 In this dataset, each state is considered an observation and five variables are reported for each state. Before we go any further in answering our original question about different states, let’s learn more about the components of this object. 5.3 The accessor: $ For our analysis we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way: murders$population #&gt; [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 #&gt; [8] 897934 601723 19687653 9920000 1360301 1567582 12830632 #&gt; [15] 6483802 3046355 2853118 4339367 4533372 1328361 5773552 #&gt; [22] 6547629 9883640 5303925 2967297 5988927 989415 1826341 #&gt; [29] 2700551 1316470 8791894 2059179 19378102 9535483 672591 #&gt; [36] 11536504 3751351 3831074 12702379 1052567 4625364 814180 #&gt; [43] 6346105 25145561 2763885 625741 8001024 6724540 1852994 #&gt; [50] 5686986 563626 But how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using: names(murders) #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders. Tip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio. 5.4 Vectors: numerics, characters, and logical The object murders$population is not one number, but several. We call these types of objects vectors. A single number is technically a vector, but, in general, we use vectors to refer to objects with several entries. The function length tells you how many entries are in the vector: pop &lt;- murders$population length(pop) #&gt; [1] 51 This particular vector is numeric since population sizes are numbers: class(pop) #&gt; [1] &quot;numeric&quot; In a numeric vector, every entry must be a number. To store character strings, vectors can also be of class character. For example, the state names are characters: class(murders$state) #&gt; [1] &quot;character&quot; As with numeric vectors, all entries in a character vector need to be a character. Another important type of vectors are logical vectors. These must be either TRUE or FALSE. z &lt;- 3 == 2 z #&gt; [1] FALSE class(z) #&gt; [1] &quot;logical&quot; Here the == is a relational operator asking if 3 is equal to 2. Remember that in R, if you just use one =, you actually assign. You can see the other relational operators by typing: ?Comparison In future sections, you will see how useful relational operators can be. Advanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L) 5.5 Factors In the murders dataset, we might expect the region to also be a character vector. However, it is not: class(murders$region) #&gt; [1] &quot;factor&quot; It is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function: levels(murders$region) #&gt; [1] &quot;Northeast&quot; &quot;South&quot; &quot;North Central&quot; &quot;West&quot; In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. Yet factors are also a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs. In general, we recommend avoiding factors as much as possible, although they are sometimes necessary to fit models containing categorical data. We will see several example of these faction in the Regression and Machine Learning parts. 5.6 Lists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you: record #&gt; $name #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; $student_id #&gt; [1] 1234 #&gt; #&gt; $grades #&gt; [1] 95 82 91 97 93 #&gt; #&gt; $final_grade #&gt; [1] &quot;A&quot; class(record) #&gt; [1] &quot;list&quot; We won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here. As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list. record$student_id #&gt; [1] 1234 We can also use double brackets like this: record[[&quot;student_id&quot;]] #&gt; [1] 1234 You should get used to the fact that in R, there are often several ways to do the same thing, such as accessing entries. 5.7 Matrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors and numbers in them. Yet matrices have a major advantage over data frames: we can perform a matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. We cover matrices in more detail in Chapter 61 but describe them briefly here since some of the functions we will learn return matrices. We can define a matrix using the matrix function. We need to specify the number of rows and columns. mat &lt;- matrix(1:12, 4, 3) mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 You can access specific entries in a matrix using [. If you want the second row, third column, you use: mat[2, 3] #&gt; [1] 10 If you want the entire second row, you leave the column spot empty: mat[2, ] #&gt; [1] 2 6 10 Notice that this returns a vector, not a matrix. Similarly, if you want the entire third column, you leave the row spot empty: mat[, 3] #&gt; [1] 9 10 11 12 This is also a vector, not a matrix. You can access more than one column or more than one row if you like. This will give you a new matrix. mat[, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 #&gt; [3,] 7 11 #&gt; [4,] 8 12 You can subset both rows and columns: mat[1:2, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 We can convert matrices into data frames using the function as.data.frame: as.data.frame(mat) #&gt; V1 V2 V3 #&gt; 1 1 5 9 #&gt; 2 2 6 10 #&gt; 3 3 7 11 #&gt; 4 4 8 12 You can also use the [ to access rows and columns of a data frame: data(&quot;murders&quot;) murders[25, 1] #&gt; [1] &quot;Mississippi&quot; murders[2:3, ] #&gt; state abb region population total #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 5.8 Exercises Load the US murders dataset. library(dslabs) data(murders) Use the function str to examine the structure of the murders object. We can see that this object is a data frame with 51 rows and five columns. Which of the following best describes the variables represented in this data frame? A. The 51 states. B. The murder rates for all 50 states and DC. C. The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010. D. str shows no relevant information. What are the column names used by the data frame for these five variables? Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object? Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same. We saw that the regions column stores a factor. You can corroborate this by typing: class(murders$region) With one line of code, use the function levels and length to determine the number of regions defined by this dataset. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region. "],
["vectors.html", "Chapter 6 Vectors 6.1 Creating vectors 6.2 Names 6.3 Sequences 6.4 Subsetting 6.5 Coercion 6.6 Not availables (NA) 6.7 Exercises", " Chapter 6 Vectors In R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class. 6.1 Creating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way: codes &lt;- c(380, 124, 818) codes #&gt; [1] 380 124 818 We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names. country &lt;- c(&quot;italy&quot;, &quot;canada&quot;, &quot;egypt&quot;) In R you can also use single quotes: country &lt;- c(&#39;italy&#39;, &#39;canada&#39;, &#39;egypt&#39;) But be careful not to confuse the single quote ’ with the back quote `. By now you should know that if you type: country &lt;- c(italy, canada, egypt) you receive an error because the variables italy, canada and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error. 6.2 Names Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two: codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 The object codes continues to be a numeric vector: class(codes) #&gt; [1] &quot;numeric&quot; but with names: names(codes) #&gt; [1] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; If the use of strings without quotes looks confusing, know that you can use the quotes as well: codes &lt;- c(&quot;italy&quot; = 380, &quot;canada&quot; = 124, &quot;egypt&quot; = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages. We can also assign names using the names functions: codes &lt;- c(380, 124, 818) country &lt;- c(&quot;italy&quot;,&quot;canada&quot;,&quot;egypt&quot;) names(codes) &lt;- country codes #&gt; italy canada egypt #&gt; 380 124 818 6.3 Sequences Another useful function for creating vectors generates sequences: seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by: seq(1, 10, 2) #&gt; [1] 1 3 5 7 9 If we want consecutive integers, we can use the following shorthand: 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 When we use these functions, R produces integers, not numerics, because they are typically used to index something: class(1:10) #&gt; [1] &quot;integer&quot; However, as soon as we create sequence including non-integer, the class changes: class(seq(1, 10, 0.5)) #&gt; [1] &quot;numeric&quot; 6.4 Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using: codes[2] #&gt; canada #&gt; 124 You can get more than one entry by using a multi-entry vector as an index: codes[c(1,3)] #&gt; italy egypt #&gt; 380 818 The sequences defined above are particularly useful if we want to access, say, the first two elements: codes[1:2] #&gt; italy canada #&gt; 380 124 If the elements have names, we can also access the entries using these names. Below are two examples. codes[&quot;canada&quot;] #&gt; canada #&gt; 124 codes[c(&quot;egypt&quot;,&quot;italy&quot;)] #&gt; egypt italy #&gt; 818 380 6.5 Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples. We said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error: x &lt;- c(1, &quot;canada&quot;, 3) But we don’t get one, not even a warning! What happened? Look at x and its class: x #&gt; [1] &quot;1&quot; &quot;canada&quot; &quot;3&quot; class(x) #&gt; [1] &quot;character&quot; R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings &quot;1&quot; and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R. R also offers functions to change from one type to another. For example, you can turn numbers into characters with: x &lt;- 1:5 y &lt;- as.character(x) y #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; You can turn it back with as.numeric: as.numeric(y) #&gt; [1] 1 2 3 4 5 This function is actually quite useful since datasets that include numbers as character strings are common. 6.6 Not availables (NA) When a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for “not available”. For example: x &lt;- c(&quot;1&quot;, &quot;b&quot;, &quot;3&quot;) as.numeric(x) #&gt; Warning: NAs introduced by coercion #&gt; [1] 1 NA 3 R does not have any guesses for what number you want when you type b, so it does not try. As a data scientist, you will encounter the NAs often as they are generally used for missing data, a common problem in real-world datasets. 6.7 Exercises Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp. Now create a vector with the city names and call the object city. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city. Use the [ and : operators to access the temperature of the first three cities on the list. Use the [ operator to access the temperature of Paris and San Juan. Use the : operator to create a sequence of numbers \\(12,13,14,\\dots,73\\). Create a vector containing all the positive odd numbers smaller than 100. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6+4/7, 6+8/7, etc.. How many numbers does the list have? Hint: use seq and length. What is the class of the following object a &lt;- seq(1, 10, 0.5)? What is the class of the following object a &lt;- seq(1, 10)? The class of class(a&lt;-1) is numeric not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer. Define the following vector: x &lt;- c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;) and coerce it to get integers. "],
["sorting.html", "Chapter 7 Sorting 7.1 sort 7.2 order 7.3 max and which.max 7.4 rank 7.5 Beware of recycling 7.6 Exercise", " Chapter 7 Sorting Now that we have mastered some basic R knowledge, let’s try to gain some insights into the safety of different states in the context of gun murders. 7.1 sort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing: library(dslabs) data(murders) sort(murders$total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 #&gt; [15] 32 36 38 53 63 65 67 84 93 93 97 97 99 111 #&gt; [29] 116 118 120 135 142 207 219 232 246 250 286 293 310 321 #&gt; [43] 351 364 376 413 457 517 669 805 1257 However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257. 7.2 order The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it: x &lt;- c(31, 4, 15, 92, 65) sort(x) #&gt; [1] 4 15 31 65 92 Rather than sort the input vector, the function order returns the index that sorts input vector: index &lt;- order(x) x[index] #&gt; [1] 4 15 31 65 92 This is the same output as that returned by sort(x). If we look at this index, we see why it works: x #&gt; [1] 31 4 15 92 65 order(x) #&gt; [1] 2 3 1 5 4 The second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on. How does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations respectively are matched by their order: murders$state[1:10] #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; #&gt; [4] &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; #&gt; [7] &quot;Connecticut&quot; &quot;Delaware&quot; &quot;District of Columbia&quot; #&gt; [10] &quot;Florida&quot; murders$abb[1:10] #&gt; [1] &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; &quot;CA&quot; &quot;CO&quot; &quot;CT&quot; &quot;DE&quot; &quot;DC&quot; &quot;FL&quot; This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector: ind &lt;- order(murders$total) murders$abb[ind] #&gt; [1] &quot;VT&quot; &quot;ND&quot; &quot;NH&quot; &quot;WY&quot; &quot;HI&quot; &quot;SD&quot; &quot;ME&quot; &quot;ID&quot; &quot;MT&quot; &quot;RI&quot; &quot;AK&quot; &quot;IA&quot; &quot;UT&quot; &quot;WV&quot; #&gt; [15] &quot;NE&quot; &quot;OR&quot; &quot;DE&quot; &quot;MN&quot; &quot;KS&quot; &quot;CO&quot; &quot;NM&quot; &quot;NV&quot; &quot;AR&quot; &quot;WA&quot; &quot;CT&quot; &quot;WI&quot; &quot;DC&quot; &quot;OK&quot; #&gt; [29] &quot;KY&quot; &quot;MA&quot; &quot;MS&quot; &quot;AL&quot; &quot;IN&quot; &quot;SC&quot; &quot;TN&quot; &quot;AZ&quot; &quot;NJ&quot; &quot;VA&quot; &quot;NC&quot; &quot;MD&quot; &quot;OH&quot; &quot;MO&quot; #&gt; [43] &quot;LA&quot; &quot;IL&quot; &quot;GA&quot; &quot;MI&quot; &quot;PA&quot; &quot;NY&quot; &quot;FL&quot; &quot;TX&quot; &quot;CA&quot; According to the above, California had the most murders. 7.3 max and which.max If we are only interested in the entry with the largest value, we can use max for the value: max(murders$total) #&gt; [1] 1257 and which.max for the index of the largest value: i_max &lt;- which.max(murders$total) murders$state[i_max] #&gt; [1] &quot;California&quot; For the minimum, we can use min and which.min in the same way. Does this mean California the most dangerous state? In an upcoming section, we argue that we should be considering rates not totals. Before doing that, we introduce one last order related function: rank 7.4 rank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example: x &lt;- c(31, 4, 15, 92, 65) rank(x) #&gt; [1] 3 1 2 5 4 To summarize, let’s look at the results of the three functions we have introduced: original sort order rank 31 4 2 3 4 15 3 1 15 31 1 2 92 65 5 5 65 92 4 4 7.5 Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens: x &lt;- c(1,2,3) y &lt;- c(10, 20, 30, 40, 50, 60, 70) x+y #&gt; Warning in x + y: longer object length is not a multiple of shorter object #&gt; length #&gt; [1] 11 22 33 41 52 63 71 We do get a warning but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output. 7.6 Exercise For these exercises we will use the US murders dataset. Make sure you load it prior to starting. library(dslabs) data(&quot;murders&quot;) Use the $ operator to access the population size data and store it the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this. Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population. You can create a data frame using the data.frame function. Here is a quick example: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Use the rank function to determine the population size rank (from smallest to biggest) of each state. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame. The na_example represents a series of counts. You can quickly examine the object using: data(&quot;na_example&quot;) str(na_example) #&gt; int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... However, when we compute the average with the function mean, we obtain an NA: mean(na_example) #&gt; [1] NA The is.na returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator. "],
["vector-arithmetics.html", "Chapter 8 Vector arithmetics 8.1 Rescaling a vector 8.2 Two vectors 8.3 Exercises", " Chapter 8 Vector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population: library(dslabs) data(&quot;murders&quot;) murders$state[which.max(murders$population)] #&gt; [1] &quot;California&quot; with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy. 8.1 Rescaling a vector In R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches: inches &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) and want to convert to centimeters. Notice what happens when we multiply inches by 2.54: inches * 2.54 #&gt; [1] 175 157 168 178 178 185 170 185 170 178 In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this: inches - 69 #&gt; [1] 0 -7 -3 1 1 4 -2 4 -2 1 8.2 Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows: \\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\] The same holds for other mathematical operations, such as -, * and /. This implies that to compute the murder rates we can simply type: murder_rate &lt;- murders$total / murders$population * 100000 Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate: murders$state[order(murder_rate)] #&gt; [1] &quot;Vermont&quot; &quot;New Hampshire&quot; &quot;Hawaii&quot; #&gt; [4] &quot;North Dakota&quot; &quot;Iowa&quot; &quot;Idaho&quot; #&gt; [7] &quot;Utah&quot; &quot;Maine&quot; &quot;Wyoming&quot; #&gt; [10] &quot;Oregon&quot; &quot;South Dakota&quot; &quot;Minnesota&quot; #&gt; [13] &quot;Montana&quot; &quot;Colorado&quot; &quot;Washington&quot; #&gt; [16] &quot;West Virginia&quot; &quot;Rhode Island&quot; &quot;Wisconsin&quot; #&gt; [19] &quot;Nebraska&quot; &quot;Massachusetts&quot; &quot;Indiana&quot; #&gt; [22] &quot;Kansas&quot; &quot;New York&quot; &quot;Kentucky&quot; #&gt; [25] &quot;Alaska&quot; &quot;Ohio&quot; &quot;Connecticut&quot; #&gt; [28] &quot;New Jersey&quot; &quot;Alabama&quot; &quot;Illinois&quot; #&gt; [31] &quot;Oklahoma&quot; &quot;North Carolina&quot; &quot;Nevada&quot; #&gt; [34] &quot;Virginia&quot; &quot;Arkansas&quot; &quot;Texas&quot; #&gt; [37] &quot;New Mexico&quot; &quot;California&quot; &quot;Florida&quot; #&gt; [40] &quot;Tennessee&quot; &quot;Pennsylvania&quot; &quot;Arizona&quot; #&gt; [43] &quot;Georgia&quot; &quot;Mississippi&quot; &quot;Michigan&quot; #&gt; [46] &quot;Delaware&quot; &quot;South Carolina&quot; &quot;Maryland&quot; #&gt; [49] &quot;Missouri&quot; &quot;Louisiana&quot; &quot;District of Columbia&quot; 8.3 Exercises Previously we created this data frame: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. What is the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\). Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average? "],
["indexing.html", "Chapter 9 Indexing 9.1 Subsetting with logicals 9.2 Logical operators 9.3 which 9.4 match 9.5 %in% 9.6 Exercises", " Chapter 9 Indexing R provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this: library(dslabs) data(&quot;murders&quot;) 9.1 Subsetting with logicals We have now calculated the murder rate using: murder_rate &lt;- murders$total / murders$population * 100000 Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above: ind &lt;- murder_rate &lt; 0.71 If we instead want to know if a value is less or equal, we can use: ind &lt;- murder_rate &lt;= 0.71 ind #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [12] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; [34] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [45] FALSE TRUE FALSE FALSE FALSE FALSE FALSE Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals. murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Iowa&quot; &quot;New Hampshire&quot; &quot;North Dakota&quot; #&gt; [5] &quot;Vermont&quot; In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using: sum(ind) #&gt; [1] 5 9.2 Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &amp;. This operation results in TRUE, only when both logicals are TRUE. To see this, consider this example: TRUE &amp; TRUE #&gt; [1] TRUE TRUE &amp; FALSE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE For our example, we can form two logicals: west &lt;- murders$region == &quot;West&quot; safe &lt;- murder_rate &lt;= 1 and we can use the &amp; to get a vector of logicals that tells us which states satisfy both conditions: ind &lt;- safe &amp; west murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Oregon&quot; &quot;Utah&quot; &quot;Wyoming&quot; 9.3 which Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type: ind &lt;- which(murders$state == &quot;California&quot;) murder_rate[ind] #&gt; [1] 3.37 9.4 match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector: ind &lt;- match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) ind #&gt; [1] 33 10 44 Now we can look at the murder rates: murder_rate[ind] #&gt; [1] 2.67 3.40 3.20 9.5 %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota and Washington are states. You can find out like this: c(&quot;Boston&quot;, &quot;Dakota&quot;, &quot;Washington&quot;) %in% murders$state #&gt; [1] FALSE FALSE TRUE Note that we will be using %in% often throughout the book. Advanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order): match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) #&gt; [1] 33 10 44 which(murders$state%in%c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;)) #&gt; [1] 10 33 44 9.6 Exercises Start by loading the library and data. library(dslabs) data(murders) Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use the logical operators to create a logical vector, name it low; that tells us which entries of murder_rate are lower than 1. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1. Use the results from the previous exercise to report the names of the states with murder rates lower than 1. Now extend the code from exercise 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &amp;. In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average? Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU ? Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index. "],
["basic-data-wrangling.html", "Chapter 10 Basic data wrangling 10.1 Adding a column with mutate 10.2 Subsetting with filter 10.3 Selecting columns with select 10.4 The pipe: %&gt;% 10.5 Creating a data frame 10.6 Exercises", " Chapter 10 Basic data wrangling Up to now we have been changing vectors by reordering them and subsetting them through indexing. However, once we start more advanced analyses, we will want to prepare data tables for data analysis. We refer to this task as data wrangling. For this purpose, we will introduce the dplyr package which provides intuitive functionality for working with tables. In the Data Wrangling Chapter, we will cover the dplyr package in much more depth. Once you install dplyr, you can load it using: library(dplyr) This package introduces functions that perform the most common operations in data wrangling and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select. We can also perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %&gt;%. Some details are included below. 10.1 Adding a column with mutate We want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable in the second using the convention name = values. So, to add murder rates, we use: library(dslabs) data(&quot;murders&quot;) murders &lt;- mutate(murders, rate = total / population * 100000) Notice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error? This is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable. We can see that the new column is added: head(murders) #&gt; state abb region population total rate #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; 6 Colorado CO West 5029196 65 1.29 Although we have over-written the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will over-write our mutated version. 10.2 Subsetting with filter Now suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this, we use the filter function which takes the data table as an argument and then the conditional statement as the next. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace. filter(murders, rate &lt;= 0.71) #&gt; state abb region population total rate #&gt; 1 Hawaii HI West 1360301 7 0.515 #&gt; 2 Iowa IA North Central 3046355 21 0.689 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Vermont VT Northeast 625741 2 0.320 10.3 Selecting columns with select Although our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below, we select three columns, assign this to a new object and then filter the new object: new_table &lt;- select(murders, state, region, rate) filter(new_table, rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 In the call to select, the first argument murders is an object, but state, region, and rate are variable names. 10.4 The pipe: %&gt;% We wrote the code above because we wanted to show the three variables for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do: \\[ \\mbox{original data } \\rightarrow \\mbox{ select } \\rightarrow \\mbox{ filter } \\] For such an operation, we can use the pipe %&gt;%. The code looks like this: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 This line of code is equivalent to the two lines of code above. What is going on here? In general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example: 16 %&gt;% sqrt() #&gt; [1] 4 We can continue to pipe values along: 16 %&gt;% sqrt() %&gt;% log2() #&gt; [1] 2 The above statement is equivalent to log2(sqrt(16)). Remember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined: 16 %&gt;% sqrt() %&gt;% log(base = 2) #&gt; [1] 2 Therefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) murders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function. 10.5 Creating a data frame It is sometimes useful for us to create our own data frames. You can do this by using the data.frame function: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) grades #&gt; names exam_1 exam_2 #&gt; 1 John 95 90 #&gt; 2 Juan 80 85 #&gt; 3 Jean 90 85 #&gt; 4 Yao 85 90 Warning: By default the function data.frame coerces characters into factors: class(grades$names) #&gt; [1] &quot;factor&quot; To avoid this, we use the rather cumbersome argument stringsAsFactors: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90), stringsAsFactors = FALSE) class(grades$names) #&gt; [1] &quot;character&quot; 10.6 Exercises Load the dplyr package and the murders dataset. library(dplyr) library(dslabs) data(murders) You can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted: murders &lt;- mutate(murders, population_in_millions = population / 10^6) We can write population rather than murders$population. The function mutate knows we are grabbing columns from murders. Use the function mutate to add a murders column named rate with the per 100,000 murder rate. Make sure you redefine murders as done in the example code above and remember the murder rate is defined by the total divided by the population size times 100,000. If rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders. With dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes: select(murders, state, population) %&gt;% head() Use select to show the state names and abbreviations in murders. Do not define a new object, just show the results. The dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this: filter(murders, state == &quot;New York&quot;) You can use other logical vectors to filter rows. Use filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column. We can remove rows using the != operator. For example, to remove Florida, we would do this: no_florida &lt;- filter(murders, state != &quot;Florida&quot;) Create a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this. We can also use the %in% to filter with dplyr. You can therefore see the data from New York and Texas like this: filter(murders, state %in% c(&quot;New York&quot;, &quot;Texas&quot;)) Create a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category? Suppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region. filter(murders, population &lt; 5000000 &amp; region == &quot;Northeast&quot;) Add a murder rate column and a rank column as done before. Create a table, call it my_states, that satisfies both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate and the rank. The pipe %&gt;% can be used to perform operations sequentially without having to define intermediate objects, after redefining murder to include rate and rank. murders &lt;- mutate(murders, rate = total / population * 100000, rank = rank(-rate)) In the solution to the previous exercise, we did the following: my_states &lt;- filter(murders, region %in% c(&quot;Northeast&quot;, &quot;West&quot;) &amp; rate &lt; 1) select(my_states, state, rate, rank) The pipe %&gt;% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this: mutate(murders, rate = total / population * 100000, rank = rank(-rate)) %&gt;% select(state, rate, rank) Notice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %&gt;%. Repeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %&gt;% to do this in just one line. Now we will make murders the original table one gets when loading using data(murders). Use just one line to create a new data frame, called my_states, that has a murder rate and a rank column, considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate, and rank columns. The line should also have four components separated by three %&gt;%. The original dataset murders. A call to mutate to add the murder rate and the rank. A call to filter to keep only the states from the Northeast or West and that have a murder rate below 1. A call to select that keeps only the columns with the state name, the murder rate and the rank. The line should look something like this: my_states &lt;- murders %&gt;% mutate SOMETHING %&gt;% filter SOMETHING %&gt;% select SOMETHING "],
["basic-plots.html", "Chapter 11 Basic plots 11.1 Scatterplots 11.2 Histograms 11.3 Boxplot 11.4 Exercises", " Chapter 11 Basic plots Exploratory data visualization is perhaps the strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than R, but it is nowhere near as flexible. D3.js may be more flexible and powerful than R, but it takes much longer to generate a plot. Before we focus on data visualization, we first we want to introduce some very basic plotting functions. 11.1 Scatterplots Earlier we inferred that states with larger populations are likely to have more murders. This can be confirmed with an exploratory visualization that plots these two quantities against each other: library(dslabs) data(&quot;murders&quot;) population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) We can clearly see a relationship. For a quick plot that avoids accessing variables twice, we can use the with function: with(murders, plot(population, total)) The function with lets us use the murders column names in the plot function. It also works with any data frames and any function. 11.2 Histograms We will describe histograms as they relate to distribution in the next part of the book. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing: murders &lt;- mutate(murders, rate = total / population * 100000) hist(murders$rate) We can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15: murders$state[which.max(murders$rate)] #&gt; [1] &quot;District of Columbia&quot; 11.3 Boxplot Boxplots will be described in more detail in the next chapter as well. Boxplots provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions: boxplot(rate~region, data = murders) We can see that the South has higher murder rates than the other three regions. 11.4 Exercises We made a plot of total murders versus population and noted a strong relationship. Not surprisingly, states with larger populations had more murders. library(dslabs) data(murders) population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) Keep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them. Create a histogram of the state populations. Generate boxplots of the state populations by region. "],
["importing-data.html", "Chapter 12 Importing data 12.1 Paths and the working directory 12.2 read.csv", " Chapter 12 Importing data In this chapter, we used a data set already stored in an R object. A data scientist will rarely have such luck and will have to import data into R from either a file, a database, or other source. We cover this in more detail in the Data Wrangling part of the book. But because it is so common to read data from a file, we will briefly describe the key approach and function, in case you want to use your new knowledge on one of your own datasets. Small datasets such as the one used in this chapter are commonly stored as Excel files. Although there are R packages designed to read Excel (xls) format, you generally want to avoid this format and save files as comma delimited (Comma-Separated Value/CSV) or tab delimited (Tab-Separated Value/TSV/TXT) files. These plain-text formats make it easier to share data since commercial software is not required for working with the data. 12.1 Paths and the working directory The first step is to find the file containing your data and know its path. When you are working in R, it is useful to know your working directory. This is the folder in which R will save or look for files by default. You can see your working directory by typing: getwd() You can also change your working directory using the function setwd or you can change it through RStudio by clicking on “Session”. The functions that read and write files (there are several in R) assume you mean to look for files or write files in the working directory. Our recommended approach for beginners will have you reading and writing to the working directory. However, you can also type the full path, which will work independently of the working directory. We have included the US murders data in CSV file as part of the dslabs package. We recommend placing your data in your working directory. Because knowing where packages store files is rather advanced, we provide the following code that finds the directory and copies the file: dir &lt;- system.file(package=&quot;dslabs&quot;) #extracts the location of package filename &lt;- file.path(dir,&quot;extdata/murders.csv&quot;) file.copy(filename, &quot;murders.csv&quot;) #&gt; [1] TRUE If a file is copied succesfully, the file.copy function returns TRUE. You should be able to see the file in your working directory and can check by using: list.files() 12.2 read.csv We are ready to read in the file. There are several functions for reading in tables. Here we introduce one included in base R: dat &lt;- read.csv(&quot;murders.csv&quot;) head(dat) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 We can see that we have read in the file. Warning: read.csv automatically converts characters to factors. Note, for example, that: class(dat$state) #&gt; [1] &quot;factor&quot; You can avoid this using: dat &lt;- read.csv(&quot;murders.csv&quot;, stringsAsFactors = FALSE) class(dat$state) #&gt; [1] &quot;character&quot; With this call, the region variable is no longer a factor, but we can easily change this with: dat &lt;- mutate(dat, region = as.factor(region)) Now that we are done with this example, we remove the murders.csv file from our working directory file.remove(&quot;murders.csv&quot;) #&gt; [1] TRUE If the file is removed successfully file.remove returns TRUE. "],
["programming-basics.html", "Chapter 13 Programming basics 13.1 Conditional expressions 13.2 Defining functions 13.3 Namespaces 13.4 For-loops 13.5 Vectorization and functionals 13.6 map", " Chapter 13 Programming basics We teach R because it greatly facilitates data analysis, the main topic of this book. By coding in R, we can efficiently perform exploratory data analysis, build data analysis pipelines and prepare data visualization to communicate results. However, R is not just a data analysis environment but a programming language. Advanced R programmers can develop complex packages and even improve R itself, but we do not cover advanced programming in this book. Nonetheless, in this section, we introduce three key programming concepts: conditional expressions, for-loops and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also note that there are several functions that are widely used to program in R but that we will not cover in this book. These include split, cut, do.call and Reduce. These are worth learning if you plan to become an expert R programmer. 13.1 Conditional expressions Conditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages. Here is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0: a &lt;- 0 if(a!=0){ print(1/a) } else{ print(&quot;No reciprocal for 0.&quot;) } #&gt; [1] &quot;No reciprocal for 0.&quot; Let’s look at one more example using the US murders data frame: library(dslabs) data(murders) murder_rate &lt;- murders$total/murders$population*100000 Here is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition. ind &lt;- which.min(murder_rate) if(murder_rate[ind] &lt; 0.5){ print(murders$state[ind]) } else{ print(&quot;No state has murder rate that low&quot;) } #&gt; [1] &quot;Vermont&quot; If we try it again with a rate of 0.25, we get a different answer: if(murder_rate[ind] &lt; 0.25){ print(murders$state[ind]) } else{ print(&quot;No state has a murder rate that low.&quot;) } #&gt; [1] &quot;No state has a murder rate that low.&quot; A related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example: a &lt;- 0 ifelse(a &gt; 0, 1/a, NA) #&gt; [1] NA The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE. a &lt;- c(0,1,2,-4,5) result &lt;- ifelse(a &gt; 0, 1/a, NA) This table helps us see what happened: a is_a_positive answer1 answer2 result 0 FALSE Inf NA NA 1 TRUE 1.00 NA 1.0 2 TRUE 0.50 NA 0.5 -4 FALSE -0.25 NA NA 5 TRUE 0.20 NA 0.2 Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros: data(na_example) no_nas &lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) #&gt; [1] 0 Two other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example: z &lt;- c(TRUE, TRUE, FALSE) any(z) #&gt; [1] TRUE all(z) #&gt; [1] FALSE 13.2 Defining functions As you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this: avg &lt;- function(x){ s &lt;- sum(x) n &lt;- length(x) s/n } Now avg is a function that computes the mean: x &lt;- 1:100 identical(mean(x), avg(x)) #&gt; [1] TRUE Notice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example: s &lt;- 3 avg(1:10) #&gt; [1] 5.5 s #&gt; [1] 3 Note how s is still 3 after we call avg. In general, functions are objects, so we assign them to variable names with &lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this: my_function &lt;- function(VARIABLE_NAME){ perform operations on VARIABLE_NAME and calculate VALUE VALUE } The functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this: avg &lt;- function(x, arithmetic = TRUE){ n &lt;- length(x) ifelse(arithmetic, sum(x)/n, prod(x)^(1/n)) } We will learn more about how to create functions through experience as we face more complex tasks. 13.3 Namespaces Once you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this becuase both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this becasue when we first load dplyr we see the following message: The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union So what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version? These function live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing: search() The first entry in this list is the global environment which includes all the objects you define. So what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific name space by using double colons (::) like this: stats::filter If we want to be absolutely sure we use the dplyr filter we can use dplyr::filter Also note that if we want to use a function in a package without loading the entire package, we can use the double colon as well. For more on this more advanced topic we recommend the R packages book. 13.4 For-loops The formula for the sum \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\): compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } Now if we can compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\), how do we do it? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). for-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop. Perhaps the simplest example of a for-loop is this useless piece of code: for(i in 1:5){ print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 And here is the for-loop we would write for our \\(S_n\\) example: m &lt;- 25 s_n &lt;- vector(length = m) # create an empty vector for(n in 1:m){ s_n[n] &lt;- compute_s_n(n) } In each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n. Now we can create a plot to search for a pattern: n &lt;- 1:m plot(n, s_n) If you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\), which we can confirm with a table: head(data.frame(s_n = s_n, formula = n*(n+1)/2)) #&gt; s_n formula #&gt; 1 1 1 #&gt; 2 3 3 #&gt; 3 6 6 #&gt; 4 10 10 #&gt; 5 15 15 #&gt; 6 21 21 We can also overlay the two results by using the function lines to draw a line over the previously plotted points: plot(n, s_n) lines(n, n*(n+1)/2) 13.5 Vectorization and functionals Although for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. We already saw examples in the Vector Arithmetic Section. A vectorized function is a function that will apply the same operation on each of the vectors. x &lt;- 1:10 sqrt(x) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 y &lt;- 1:10 x*y #&gt; [1] 1 4 9 16 25 36 49 64 81 100 To make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n: n &lt;- 1:25 compute_s_n(n) Functionals are functions that help us apply the same function to each entry in a vector, matrix, data frame or list. Here we cover the functional that operates on numeric, logical and character vectors: sapply. The function sapply permits us to perform element-wise operations on any function. Here is how it works: x &lt;- 1:10 sapply(x, sqrt) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 Each element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows: n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) plot(n, s_n) Other functionals are apply, lapply, tapply, mapply, vapply, and replicate. We moslty use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful. 13.6 map Another series of functionals are provided by the purrr package. These are similar to sapply, but, in some instances, they are somewhat easier to use for beginners. The general idea is the same. library(purrr) n &lt;- 1:25 s_n &lt;- map(n, compute_s_n) One advantage that becomes clear later is that with the purrr functionals you know what type of object you will get back. This is different with sapply which can return several different types. The map function always returns a list. class(s_n) #&gt; [1] &quot;list&quot; If we want a numeric vector, we can instead use: s_n &lt;- map_dbl(n, compute_s_n) plot(n, s_n) We will learn other advantages of map in later chapters. "],
["exercises-7.html", "Chapter 14 Exercises", " Chapter 14 Exercises What will this conditional expression return? x &lt;- c(1,2,-3,4) if(all(x&gt;0)){ print(&quot;All Postives&quot;) } else{ print(&quot;Not all positives&quot;) } Which of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE? A. all(x) B. any(x) C. any(!x) D. all(!x) The function nchar tells you how many characters long a character vector is. Write a line of code that assigns to the object new_names the state abbreviation when the state name is longer than 8 characters. Create a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000. Create a function altman_plot that takes two arguments, x and y, and plots the difference against the sum. After running the code below, what is the value of x? x &lt;- 3 my_func &lt;- function(y){ x &lt;- 5 y+5 } Write a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\). Define an empty numerical vector s_n of size 25 using s_n &lt;- vector(&quot;numeric&quot;, 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop. Repeat exercise 8, but this time use sapply. Repeat exercise 8, but this time use map_dbl. Plot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\). Confirm that the formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\). "],
["introduction-to-data-visualization.html", "Chapter 15 Introduction to data visualization", " Chapter 15 Introduction to data visualization Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table: library(dslabs) data(murders) head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 What do you learn from staring at this table? How quickly can you determine which states have the largest populations? Which states have the smallest? How large is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most human brains, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to all these questions are readily available from examining this plot: We are reminded of the saying “a picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting. A particularly effective example is a Wall Street Journal article showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced. (Source: Wall Street Journal) Another striking example comes from the New York Times and shows data on scores from the NYC Regents Exams. These scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic: (Source: New York Times) The most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up. This is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey, considered the father of EDA, once said, “The greatest value of a picture is when it forces us to notice what we never expected to see.” Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked. Data visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty and The Best Stats You’ve Ever Seen, Hans Rosling forces us to to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true. It is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important. We refer to this particular task as exploratory data analysis (EDA). In this chapter, we will learn the basics of data visualization and exploratory data analysis by using three motivating examples. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above 1) world health and economics and 2) infectious disease trends in the United States. Of course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more: ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130. NB Robbins (2004) Creating more effective graphs. Wiley. Nature Methods columns A Cairo (2013) The Functional Art: An Introduction to Information Graphics and Visualization. New Riders. N Yau (2013) Data Points: Visualization That Means Something. Wiley. We also do not cover interactive graphics, a topic that is too advanced for this book. Some useful resources for those interested in learning more can be found below: https://shiny.rstudio.com/ https://d3js.org/ "],
["distributions.html", "Chapter 16 Distributions 16.1 Variable types 16.2 Case study: Student heights 16.3 Distribution function 16.4 Cumulative distribution functions 16.5 Histograms 16.6 Smoothed density 16.7 Exercises 16.8 The normal distribution 16.9 Standardized units 16.10 Quantile-quantile QQ plots 16.11 Percentiles 16.12 Case study: Student heights (continued) 16.13 Boxplots 16.14 Case study: Student heights (continued) 16.15 Exercises", " Chapter 16 Distributions You may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. So, for example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list? Our first data visualization building block is learning to summarize lists of factors or numeric vectors. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information. 16.1 Variable types We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous. When each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered, for example spiciness (mild, medium, hot), even if they are not numbers per se. In statistics textbooks, they refer to these as ordinal data. Example of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches respectively. Counts, such as population sizes, are discrete because they have to be round numbers. Keep in mind that all discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data. 16.2 Case study: Student heights Here we introduce a new motivating problem. It is an artificial one, but it will help us illustrate the concepts needed to understand distributions. Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions. We collect the data and save it in a data frame: library(dslabs) data(heights) head(heights) #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. 16.3 Distribution function It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough. The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for most of you. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is: #&gt; #&gt; Female Male #&gt; 0.227 0.773 This two category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with the US state regions: This particular plot is simply showing us four numbers: one for each category. We usually use barplots to display a few numbers. Although this particular plot, a graphical representation of a frequency table, does not provide much more insight than a table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. Once the data is numerical, the task of displaying distributions is more challenging. 16.4 Cumulative distribution functions Numerical data, that are not categorical, also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters respectively. Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used: \\[ F(a) = \\mbox{Pr}(x \\leq a) \\] Here is a plot of \\(F\\) for the male height data: Similar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.164, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841, etc.. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousands word”, in this case, a picture is as informative as 812 numbers. A final note: because CDFs can be defined mathematically, as opposed to using data as we do here, the word empirical is added to distinguish and we use the term empirical CDF (ECDF) instead. 16.5 Histograms Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret. The simplest way to make a histograms is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\([49.5, 50.5], [51.5,52.5],(53.5,54.5],...,(82.5,83.5]\\) As you can see in the figure above, a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical. If we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts. So what information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers. 16.6 Smoothed density Smooth density plots are aesthetically more appealing than histograms. Here is what a smooth density plot looks like for our heights data: In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. To understand the smooth densities, we have to understand estimates, a topic we don’t cover until a later chapter. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool. The main new concept you must understand is that we assume that our list of observed values comes from a much larger list of unobserved values. In the case of heights, you can imagine that our list of 1050 students comes from a hypothetical list containing all the heights of all the students in all the world measured very precisely. Let’s say there are 1,000,000 of these. This list of values, like any list of values, has a distribution and this is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it. However, we make an assumption that helps us perhaps approximate it. Because we have 1,000,000 values, measured very precisely, we can make a histogram with very, very small bins. The assumption is that if we do this, consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps. Below we have a hypothetical histogram with bins of size 1: The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5 and 0.1: The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts: Now, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins. We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars: Remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density. Here are two examples using different degrees of smoothness on the same histogram: p1 &lt;- heights %&gt;% filter(sex==&quot;Male&quot;)%&gt;% ggplot(aes(height)) + geom_histogram(aes(y=..density..), binwidth = 1) + geom_density(col=&quot;#00BFC4&quot;, adjust = 0.5) p2 &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(aes(y=..density..), binwidth = 1) + geom_density(col=&quot;#00BFC4&quot;, adjust = 2) grid.arrange(p1,p2, ncol=2) We need to make this choice with care as the resulting visualizations can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71, than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, more like the example on the right than on the left. While the histogram is an assumption free summary, the smoothed density is based on some assumptions. Interpreting the y-axis Finally, we point out that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. But this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68: The proportion of this area is about 0.32, meaning that about that proportion is between 65 and 68 inches. By understanding this we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption and, therefore, with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data: Densities permit stratification As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities makes it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights: heights %&gt;% ggplot(aes(height, fill=sex)) + geom_density(alpha = 0.2) With the right argument, ggplot automatically shades the intersecting region with a different color. 16.7 Exercises In the murders dataset, the region is a categorical variable and the following is its distribution: To the closet 5%, what proportion of the states are in the North Central region? Which of the following is true: A. The graph above is a histogram. B. The graph above shows only four numbers with a bar plot. C. Categories are not numbers so it does not make sense to graph the distribution. D. The colors, not the height of the bars, describe the distribution. The plot below shows the eCDF for male heights: Based on the plot, what percentage of males are shorter than 75 inches? A. 100% B. 95% C. 80% D. 72 inches To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter? A. 61 inches B. 64 inches C. 69 inches D. 74 inches Here is an eCDF of the murder rates across states: Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people? A. 1 B. 5 C. 10 D. 50 Based on the eCDF above, which of the following statements are true: A. About half the states have murder rates above 7 per 100,000 and the other half below. B. Most states have murder rates below 2 per 100,000. C. All the states have murder rates above 2 per 100,000. D. With the exception of 4 states, the murder rates are below 5 per 100,000. Below is a histogram of male heights in our heights dataset: Based on this plot, how many males are between 63.5 and 65.5? A. 10 B. 24 C. 34 D. 100 About what percentage are shorter than 60 inches? A. 1% B. 10% C. 25% D. 50% Based on the density plot below, about what proportion of US states have populations larger than 10 million? A. 0.02 B. 0.15 C. 0.50 D. 0.55 Below are three density plots. Is it possible that they are from the same dataset? Which of the following statements is true: A. It is impossible that they are from the same dataset. B. They are from the same dataset, but different due to code errors. C. They are the same dataset, but the first and second undersmooth and the third oversmooths. D. They are the same dataset, but the first is not in the log scale, the second undersmooths and the third oversmooths. 16.8 The normal distribution Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution. The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these in a later chapter. Here we focus on how the normal distribution helps us summarize data. Rather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula: \\[\\mbox{Pr}(a &lt; x &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left\\{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2\\right\\} \\, dx\\] You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(\\mathrm{e}\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average, also called the mean, and the standard deviation (SD) of the distribution respectively. The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what it looks like when the average is 0 and the SD is 1: The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation, which we now define for an arbitrary list of numbers. For a list of numbers contained in a vector x, the average is defined as: average &lt;- sum(x) / length(x) and the SD is defined as: SD &lt;- sqrt( sum( (x-mu)^2) / length(x)) which can be interpreted as the average distance between values and their average. Let’s compute the values for the height for males which we will store in the object \\(x\\): index &lt;- heights$sex==&quot;Male&quot; x &lt;- heights$height[index] The pre-built functions mean and sd (note that sd divides by length(x)-1 not length(x)) can be used here: average &lt;- mean(x) SD &lt;- sd(x) c(average=average,SD=SD) #&gt; average SD #&gt; 69.31 3.61 Here is a plot of the smooth density and the normal distribution with mean average = 69.315 and SD = 3.611: It does appear to be quite a good approximation. We now will see how well this approximation works at predicting proportion of values within intervals. 16.9 Standardized units For data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value \\(x\\), we define it as \\(z = (x-\\mbox{average})/\\mbox{SD}\\). If you look back at the formula for the normal distribution, you see that what is being exponentiated is \\(- z^/2\\). The maximum of \\(\\exp{-z^2/2}\\) is when \\(z=0\\), which explains why the maximum of the distribution is at the mean. It also explains the symmetry since \\(- z^/2\\) is symmetric around 0. If we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z=2\\)), one of the smallest (\\(z=-2\\)) or an extremely rare occurrence (\\(z&gt;3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to data that is approximately normal. In R, we can obtain standard units using the function scale: z &lt;- scale(x) Now to see how many men are within 2 SDs from the average, we simply type: mean(abs(z) &lt; 2) #&gt; [1] 0.95 The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots. 16.10 Quantile-quantile QQ plots A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, the approach of the QQ-plot is as follows: Define a series of proportions \\(p=0.05,\\dots .95\\). For each \\(p\\), determine the value \\(q\\) so that the proportion of values in the data below \\(q\\) is \\(p\\). The \\(q\\)s are referred to as the quantiles. To give a quick example, for the male heights data, we have that: mean(x &lt;= 69.5) #&gt; [1] 0.515 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\). Now we define a series of \\(p\\): p &lt;- seq(0.05, 0.95, 0.05) If the quantiles for the data match the quantiles for the normal, then it must be because the data follows a normal distribution. To obtain the quantiles from the data, we can use the quantile function like this: observed_quantiles &lt;- quantile(x, p) To obtain the theoretical normal distribution quantiles, with the corresponding average and SD, we use the qnorm function: theoretical_quantiles &lt;- qnorm( p, mean = mean(x), sd = sd(x)) To see if they match or not, we plot them against each other and draw the identity line: plot(theoretical_quantiles, observed_quantiles) abline(0,1) Notice that this code becomes much cleaner if we use standard units: observed_quantiles &lt;- quantile(z, p) theoretical_quantiles &lt;- qnorm(p) plot(theoretical_quantiles, observed_quantiles) abline(0,1) 16.11 Percentiles Before we move on, let’s define some terms that are commonly used in exploratory data analysis. Percentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median. For the normal distribution the median and average are the same, but this is generally not the case. Another special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\). 16.12 Case study: Student heights (continued) Using the histogram, density plots and qq-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.315 inches and a SD of 3.611 inches. With this information, ET will have a good idea of what to expect when he meets our male students. 16.13 Boxplots To introduce boxplots we will go back to the US murder data. Suppose we want to summarize the murder rate distribution. Using the data visualization technique we have learned, we can quickly see that the normal approximation does not apply here: In this case, the histogram, or a smooth density plot, would serve as a relatively succinct summary. Now suppose those used to receiving just two numbers as summaries ask us for a more compact summary. Here Tukey offered some advice. Provide a five number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). Tukey further suggested that we ignore outliers when computing the range and instead plot these as independent points. We provide a detailed explanation of outliers later in the chapter. Finally, he suggested we plot these numbers as a “box” with “whiskers”&quot; like this: with the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the interquartile range. The two points are outliers according to Tukey’s definition. The median is shown with a horizontal line. Today, we call these boxplots. From just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions. Boxplots are even more useful when we want to quickly compare two or more distributions. For example, here are the heights for men and women: heights %&gt;% ggplot(aes(x=sex, y=height, fill=sex)) + geom_boxplot() The plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. 16.14 Case study: Student heights (continued) We have to give ET a full summary of our heights, but we have not yet summarized female heights. We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful: We see something we did not see for the males: the density plot has a second “bump”. Also, the qqplot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the qqplot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights. However, go back and read Tukey’s quote. We have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, FEMALE was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data. Regarding the five smallest values, note that these values are: heights %&gt;% filter(sex==&quot;Female&quot;) %&gt;% top_n(5, desc(height)) %&gt;% .$height #&gt; [1] 51 53 55 52 52 Because these are reported heights, a possibility is that the student meant to enter 5’1“, 5’2”, 5’3&quot; or 5’5“. 16.15 Exercises Define variables containing the heights of males and females like this: library(dslabs) data(heights) male &lt;- heights$height[heights$sex==&quot;Male&quot;] female &lt;- heights$height[heights$sex==&quot;Female&quot;] How many measurements do we have for each? Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, …, 90th percentiles for each sex. Then create a data frame with these two as columns. Study the following boxplots showing population sizes by country: Which continent has the country with the biggest population size? What continent has the largest median population size? What is median population size for Africa to the nearest million? What proportion of countries in Europe have populations below 14 million? A. 0.99 B. 0.75 C. 0.50 D. 0.25 If we use a log transformation, which continent shown above has the largest interquartile range? Load the height data set and create a vector x with just the male heights: library(dslabs) data(heights) x &lt;- heights$height[heights$sex==&quot;Male&quot;] What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions. Notice that the approximation calculated in question two is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation? Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven footers. Hint: use the pnorm function. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world? There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18 to 40 year old seven footers are in the NBA? Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are that tall. In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations: A. Practice and talent are what make a great basketball player, not height. B. The normal approximation is not appropriate for heights. C. As seen in question 3, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted. D. As seen in question 3, the normal approximation tends to overestimate the extreme values. It’s possible that there are less seven footers than we predicted. "],
["robust-summaries.html", "Chapter 17 Robust summaries 17.1 Outliers 17.2 Median 17.3 The inter quartile range (IQR) 17.4 Tukey’s definition of an outlier 17.5 Median absolute deviation Exercises 17.6 Case study: Self-reported student heights", " Chapter 17 Robust summaries 17.1 Outliers We previously described how boxplots show outliers, but we did not provide a precise definition. Here we discuss outliers, approaches that can help detect them, and summaries that take into account their presence. Outliers are very common in data science. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. An individual, for instance, may mistakenly enter their height in centimeters instead of inches. Now, how do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case. Suppose a colleague is charged with collecting demography data for a group of males. The data is stored in the object: #&gt; num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ... Our colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation: mean(outlier_example) #&gt; [1] 6.1 sd(outlier_example) #&gt; [1] 7.8 and writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! Using your data science skills, however, you notice something else that is unexpected: the standard deviation is over 7 inches. Adding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.489, 21.697 inches, which does not make sense. A quick plot reveals the problem: hist(outlier_example) There appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier: boxplot(outlier_example) 17.2 Median When we have an outlier like this, the average can become very large. Mathematically, we can make the average as large as we want by simply changing one number: with 500 data points, we can increase the average by any amount \\(\\Delta\\) by adding \\(\\Delta \\times\\) 500 to a single number. The median, defined as the value for which half the values are smaller and the other half are bigger, is robust to such outliers. No matter how large we make the largest point, the median remains the same. With this data the median is: median(outlier_example) #&gt; [1] 5.74 which is about 5 feet and 9 inches. The median is what boxplots display as a horizontal line. 17.3 The inter quartile range (IQR) The box in boxplots are defined by the first and third quartile. These are meant to provide an idea of the variability in the data: 50% of the data is within this range. The difference between the 3rd and 1st quartile (or 75th and 25th percentiles) is referred to as the inter quartile range (IQR). As is the case with the median, this quantity will be robust to outliers as large values do not affect it. We can do some math to see that for normal data the IQR / 1.349 approximates the standard deviation of the data had an outlier not been present. We can see that this works well in our example since we get a standard deviation estimate of: IQR(outlier_example) / 1.349 #&gt; [1] 0.245 which is about 3 inches. 17.4 Tukey’s definition of an outlier In R, points falling outside the whiskers of the boxplot are referred to as outliers. This definition of outlier was introduced by Tukey. The top whisker ends at the 75th percentile plus 1.5 \\(\\times\\) IQR. Similarly the bottom whisker ends at the 25th percentile minus 1.5\\(\\times\\) IQR. If we define the first and third quartiles as \\(Q_1\\) and \\(Q_3\\) respectively, then an outlier is anything outside the range: \\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)]\\]. When the data is normally distributed, the standard units of these values are: q3 &lt;- qnorm(0.75) q1 &lt;- qnorm(0.25) iqr &lt;- q3 - q1 r &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr) r #&gt; [1] -2.7 2.7 Using the pnorm function, we see that 99.3% of the data falls in this interval. Keep in mind that this is not such an extreme event: if we have 1000 data points that are randomly distributed, we expect to see about 7 outside of this range. But these would not be outliers since we expect to see them under the typical variation. If we want an outlier to be rarer, we can increase the 1.5 to a larger number. Tukey also used 3 and called these far out outliers. With a normal distribution, 100% of the data falls in this interval. This translates into about 1 in a million chance of being outside the range. In the geom_boxplot function, this can be controlled by the outlier.size argument, which defaults to 1.5. The 180 feet measurement is well beyond the range of the height data: max_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example) If we take this value out, we can see that the data is in fact normally distributed as expected: x &lt;- outlier_example[outlier_example &lt; max_height] qqnorm(x) qqline(x) 17.5 Median absolute deviation Another way to estimate the standard deviation in the presence of outliers is to use the median absolute deviation (MAD). To compute the MAD, we first compute the median, and then for each value we compute the distance between that value and the median. The MAD is defined as the median of these distances. For technical reasons not discussed here, this quantity needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. The mad function already incorporates this correction. For the height data, we get a MAD of: mad(outlier_example) #&gt; [1] 0.237 which is about 3 inches. Exercises We are going to use the HistData. If it is not installed you can install it like this: install.packages(&quot;HistData&quot;) Load the height data set and create a vector x with just the male heights used in Galton’s data on the heights of parents and their children from his historic research on heredity. library(HistData) data(Galton) x &lt;- Galton$child Compute the average and median of these data. Compute the median and median absolute deviation of these data. Now suppose Galton made a mistake when entering the first value and forgot to use the decimal point. You can imitate this error by typing: x_with_error &lt;- x x_with_error[1] &lt;- x_with_error[1]*10 How many inches does the average grow after this mistake? How many inches does the SD grow after this mistake? How many inches does the median grow after this mistake? How many inches does the MAD grow after this mistake? How could you use exploratory data analysis to detect that an error was made? A. Since it is only one value out of many, we will not be able to detect this. B. We would see an obvious shift in the distribution. C. A boxplot, histogram, or qq-plot would reveal a clear outlier. D. A scatter plot would show high levels of measurement error. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000. 17.6 Case study: Self-reported student heights The heights we have been looking at are not the original hieghts reported by students. The original reported heights are also included in the dslabs package and can be loaded like this: data(&quot;reported_heights&quot;) head(reported_heights) #&gt; time_stamp sex height #&gt; 1 2014-09-02 13:40:36 Male 75 #&gt; 2 2014-09-02 13:46:59 Male 70 #&gt; 3 2014-09-02 13:59:20 Male 68 #&gt; 4 2014-09-02 14:51:53 Male 74 #&gt; 5 2014-09-02 15:16:15 Male 61 #&gt; 6 2014-09-02 15:16:16 Female 65 Height is a character vector so we create a new column with the numeric version: reported_heights &lt;- reported_heights %&gt;% mutate(original_heights = height, height = as.numeric(height)) #&gt; Warning in evalq(as.numeric(height), &lt;environment&gt;): NAs introduced by #&gt; coercion Note that we get a warning about NAs. This is because some of the self reported heights were not numbers. We can see why we get these: reported_heights %&gt;% filter(is.na(height)) %&gt;% head() #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-02 15:16:28 Male NA 5&#39; 4&quot; #&gt; 2 2014-09-02 15:16:37 Female NA 165cm #&gt; 3 2014-09-02 15:16:52 Male NA 5&#39;7 #&gt; 4 2014-09-02 15:16:56 Male NA &gt;9000 #&gt; 5 2014-09-02 15:16:56 Male NA 5&#39;7&quot; #&gt; 6 2014-09-02 15:17:09 Female NA 5&#39;3&quot; Some students self reported their heights using feet and inches rather than just inches. Others used centimeters and others were just trolling. For now we will remove these entries: reported_heights &lt;- filter(reported_heights, !is.na(height)) If we compute the average and standard deviation, we notice that we obtain strange results. The average and standard deviation are different from the median and MAD: reported_heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), sd = sd(height), median = median(height), MAD = mad(height)) #&gt; # A tibble: 2 x 5 #&gt; sex average sd median MAD #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 63.4 27.9 64.2 4.05 #&gt; 2 Male 103. 530. 70 4.45 This suggests that we have outliers, which is confirmed by simply creating a boxplot: reported_heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot() We can see some rather extreme values. To see what these values are, we can quickly look at the largest values using the arrange function: reported_heights %&gt;% arrange(desc(height)) %&gt;% top_n(10, height) #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-03 23:55:37 Male 11111 11111 #&gt; 2 2016-04-10 22:45:49 Male 10000 10000 #&gt; 3 2015-08-10 03:10:01 Male 684 684 #&gt; 4 2015-02-27 18:05:06 Male 612 612 #&gt; 5 2014-09-02 15:16:41 Male 511 511 #&gt; 6 2014-09-07 20:53:43 Male 300 300 #&gt; 7 2014-11-28 12:18:40 Male 214 214 #&gt; 8 2017-04-03 16:16:57 Male 210 210 #&gt; 9 2015-11-24 10:39:45 Male 192 192 #&gt; 10 2014-12-26 10:00:12 Male 190 190 #&gt; 11 2016-11-06 10:21:02 Female 190 190 The first seven entries look like strange errors. However, the next few look like they were entered as centimeters instead of inches. Since 184 cm is equivalent to six feet tall, we suspect that 184 was actually meant to be 72 inches. We can review all the nonsensical answers by looking at the data considered to be far out by Tukey: max_height &lt;- quantile(reported_heights$height, .75) + 3*IQR(reported_heights$height) min_height &lt;- quantile(reported_heights$height, .25) - 3*IQR(reported_heights$height) c(min_height, max_height) #&gt; 25% 75% #&gt; 44 93 reported_heights %&gt;% filter(!between(height, min_height, max_height)) %&gt;% select(original_heights) %&gt;% head(n=10) #&gt; original_heights #&gt; 1 6 #&gt; 2 5.3 #&gt; 3 511 #&gt; 4 6 #&gt; 5 2 #&gt; 6 5.25 #&gt; 7 5.5 #&gt; 8 11111 #&gt; 9 6 #&gt; 10 6.5 Examining these heights carefully, we see two common mistakes: entries in centimeters, which turn out to be too large, and entries of the form x.y with x and y representing feet and inches respectively. Some of the even smaller values, such as 1.6, could be entries in meters. In the data wrangling chapter, we will learn techniques for correcting these values and converting them into inches. Here we were able to detect this problem using careful data exploration to uncover issues with the data: the first step in the great majority of data science projects. "],
["summarizing-data-with-dplyr.html", "Chapter 18 Summarizing data with dplyr 18.1 Summarize 18.2 The dot operator 18.3 Group then summarize 18.4 Sorting data frames Exercises", " Chapter 18 Summarizing data with dplyr An important part of exploratory data analysis is summarizing data. We learned about the average and standard deviation as a two summary statistic that provides all the necessary information to summarize data that is normally distributed. We also learned that better summaries can be achieved by splitting data into groups before using the normal approximation. For example, in our heights dataset, we described the height of men and women separately. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using what we call the dot placeholder. Finally, we also learn to use arrange, which helps us examine data after sorting. 18.1 Summarize The summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights: library(dslabs) data(heights) that computes the average and standard deviation for males: s &lt;- heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) s #&gt; average standard_deviation #&gt; 1 69.3 3.61 This takes our original data table as input, filters it to keep only males and then produces a new, summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same. Because the resulting table, stored in s, is a data frame, we can access the components with the accessor $, which in this case will be a numeric: s$average #&gt; [1] 69.3 s$standard_deviation #&gt; [1] 3.61 As with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), it is accessing the column with the name, and then computing the average of the respective numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, min and max like this: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(median = median(height), minimum = min(height), maximum = max(height)) #&gt; median minimum maximum #&gt; 1 69 50 82.7 We can obtain these three values with just one line using the quantiles function; e.g. quantile(x, c(0,0.5,1)) returns the min, median, and max of the vector x. However, if we attempt to use a function that returns two or more values: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) we will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In a later chapter, we will learn how to deal with functions that return more than one value. For another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column: data(murders) murders &lt;- murders %&gt;% mutate(murder_rate = total/population*100000) Remember that the US murder is not the average of the state murder rates: summarize(murders, mean(murder_rate)) #&gt; mean(murder_rate) #&gt; 1 2.78 This is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total US murders divided by the total US population. So the correct computation is: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 100000) us_murder_rate #&gt; rate #&gt; 1 3.03 This computation counts larger states proportionally to their size which results in a larger value. 18.2 The dot operator The us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame: class(us_murder_rate) #&gt; [1] &quot;data.frame&quot; since, as most __dplyr functions, summarize always returns a data frame. This might be problematic if we want to use the result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data piped via %&gt;%: when a data object is piped it can be accessed using the dot .. To understand what we mean take a look at this line of code: us_murder_rate %&gt;% .$rate #&gt; [1] 3.03 This returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate. To understand this line, you just need to think of . as a placeholder for the data that is being passed through the pipe. Because this data object is a data frame, we can access its columns with the $. To get a number from the original data table with one line of code we can type: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 100000) %&gt;% .$rate us_murder_rate #&gt; [1] 3.03 which is now a numeric: class(us_murder_rate) #&gt; [1] &quot;numeric&quot; We will see other instances in which using the . is useful. For now, we will only use it to produce numeric vectors from pipelines constructed with dplyr. 18.3 Group then summarize A common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this. If we type this: heights %&gt;% group_by(sex) #&gt; # A tibble: 1,050 x 2 #&gt; # Groups: sex [2] #&gt; sex height #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; # ... with 1,044 more rows the result does not look very different from heights, except we see this Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens: heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) #&gt; # A tibble: 2 x 3 #&gt; sex average standard_deviation #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.9 3.76 #&gt; 2 Male 69.3 3.61 The summarize function applies the summarization to each group separately. For another example, let’s compute the median murder rate in the four regions of the country: murders %&gt;% group_by(region) %&gt;% summarize(median_rate = median(murder_rate)) #&gt; # A tibble: 4 x 2 #&gt; region median_rate #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.80 #&gt; 2 South 3.40 #&gt; 3 North Central 1.97 #&gt; 4 West 1.29 18.4 Sorting data frames When examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size when we type: murders %&gt;% arrange(population) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Wyoming WY West 563626 5 0.887 #&gt; 2 District of Columbia DC South 601723 99 16.453 #&gt; 3 Vermont VT Northeast 625741 2 0.320 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Alaska AK West 710231 19 2.675 #&gt; 6 South Dakota SD North Central 814180 8 0.983 We get to decide which column to sort by. To see the states by population, from smallest to largest, we arrange by murder_rate instead: murders %&gt;% arrange(murder_rate) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Hawaii HI West 1360301 7 0.515 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Iowa IA North Central 3046355 21 0.689 #&gt; 6 Idaho ID West 1567582 12 0.766 Note that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type: murders %&gt;% arrange(desc(murder_rate)) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Missouri MO North Central 5988927 321 5.36 #&gt; 4 Maryland MD South 5773552 293 5.07 #&gt; 5 South Carolina SC South 4625364 207 4.48 #&gt; 6 Delaware DE South 897934 38 4.23 Nested sorting If we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region then, within region, we order by murder rate: murders %&gt;% arrange(region, murder_rate) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Maine ME Northeast 1328361 11 0.828 #&gt; 4 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 5 Massachusetts MA Northeast 6547629 118 1.802 #&gt; 6 New York NY Northeast 19378102 517 2.668 The top \\(n\\) In the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. Here are the first 10 rows: murders %&gt;% top_n(10, murder_rate) #&gt; state abb region population total murder_rate #&gt; 1 Arizona AZ West 6392017 232 3.63 #&gt; 2 Delaware DE South 897934 38 4.23 #&gt; 3 District of Columbia DC South 601723 99 16.45 #&gt; 4 Georgia GA South 9920000 376 3.79 #&gt; 5 Louisiana LA South 4533372 351 7.74 #&gt; 6 Maryland MD South 5773552 293 5.07 #&gt; 7 Michigan MI North Central 9883640 413 4.18 #&gt; 8 Mississippi MS South 2967297 120 4.04 #&gt; 9 Missouri MO North Central 5988927 321 5.36 #&gt; 10 South Carolina SC South 4625364 207 4.48 top_n picks the highest n based on the column given as a second argument. However, the rows are not sorted. If the second argument is left blank, then it returns the first n columns. This means that to see the 10 states with the highest murder rates we can type: murders %&gt;% arrange(desc(murder_rate)) %&gt;% top_n(10) #&gt; Selecting by murder_rate #&gt; state abb region population total murder_rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Missouri MO North Central 5988927 321 5.36 #&gt; 4 Maryland MD South 5773552 293 5.07 #&gt; 5 South Carolina SC South 4625364 207 4.48 #&gt; 6 Delaware DE South 897934 38 4.23 #&gt; 7 Michigan MI North Central 9883640 413 4.18 #&gt; 8 Mississippi MS South 2967297 120 4.04 #&gt; 9 Georgia GA South 9920000 376 3.79 #&gt; 10 Arizona AZ West 6392017 232 3.63 Exercises For these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package which can install using: install.packages(&quot;NHANES&quot;) Once you install it, you can load the data this way: library(NHANES) data(NHANES) The NHANES data has many missing values. Remember that the main summarization function in R will return NA if any of the entries of the input vector is an NA. Here is an example: library(dslabs) data(na_example) mean(na_example) #&gt; [1] NA sd(na_example) #&gt; [1] NA To ignore the NAs we can use the na.rm argument: mean(na_example, na.rm=TRUE) #&gt; [1] 2.3 sd(na_example, na.rm=TRUE) #&gt; [1] 1.22 Let’s now explore the NHANES data. We will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-29 year old females. Note that the category is coded with 20-29, with a space in front! The AgeDecade is a categorical variable with these ages. What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref. Hint: Use filter and summarize and use the na.rm=TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter. Using only one line of code, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then the dot. Now report the min and max values for the same group. Compute the average and standard deviation for females, but for each age group separately. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age, filter by Gender and then use group_by. Now do the same for males. We can actually combine both these summaries into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender). For males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure. "],
["ggplot2.html", "Chapter 19 ggplot2 19.1 The cheat sheet 19.2 The components of a graph 19.3 ggplot objects: a blank slate 19.4 Geometries 19.5 Aesthetic mappings 19.6 Layers 19.7 Tinkering with arguments 19.8 Global versus local aesthetic mappings 19.9 Scales 19.10 Labels and titles 19.11 Categories as colors 19.12 Annotation and shapes 19.13 Adjustments 19.14 Add-on packages 19.15 Putting it all together 19.16 Other geometries 19.17 Grids of plots 19.18 Quick plots with qplot Exercises", " Chapter 19 ggplot2 We have now described several data visualization techniques and are ready to learn how to create them in R. Throughout the book, we will be using the ggplot2 package. We can load it, along with dplyr, as part of the tidyverse: Many other approaches are available for creating plots in R. In fact, the plotting capabilities that come with a basic installation of R are already quite powerful. We have seen examples of these already with the functions plot, hist and boxplot. There are also other packages for creating graphics such as grid and lattice. We chose to use ggplot2 in this book because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember. One reason ggplot2 is generally more intuitive for beginners is that it uses a grammar of graphics, the gg in ggplot2. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of ggplot2 building blocks and its grammar, you will be able to create hundreds of different plots. Another reason ggplot2 makes it easier for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code. One limitation is that ggplot2 is designed to work exclusively with data tables in which rows are observations and columns are variables. However, a substantial percentage of datasets that beginners work with are, or can be converted into, this format. An advantage of this approach is that, assuming that our data follows this format, it simplifies the code and learning the grammar. 19.1 The cheat sheet To use ggplot2 you will have to learn several functions and arguments. These are hard to memorize so we highly recommend you have the a ggplot2 sheet cheat handy. You can get a copy here: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf or simply perform an internet search for “ggplot2 cheat sheet”. 19.2 The components of a graph We construct a graph that summarizes the US murders dataset: We can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color which depicts how most southern states have murder rates above the average. This data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part. The first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are: Data: The US murders data table is being summarized. We refer to this as the data component. Geometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histograms, smooth densities, qqplots, and boxplots. Aesthetic mapping: The x-axis values are used to display population size, the y-axis values are used to display the total number of murders, text is used to identify the states, and colors are used to denote the four different regions. These are the aesthetic mappings component. How we define the mapping depends on what geometry we are using. We also note that: The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales. We refer to this as the scale component. There are labels, a title, a legend, and we use the style of The Economist magazine. We will now construct the plot piece by piece. We start by loading the dataset: library(dslabs) data(murders) 19.3 ggplot objects: a blank slate The first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object: ggplot(data = murders) We can also pipe the data. So this line of code is equivalent to the one above: murders %&gt;% ggplot() It renders a plot, in this case a blank slate, since no geometry has been defined. The only style choice we see is a grey background. What has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can define an object, for example, like this: p &lt;- ggplot(data = murders) class(p) #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; To render the plot associated with this object, we simply print the object p. The following two lines of code produce the same plot we see above: print(p) p 19.4 Geometries In ggplot we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the the symbol +. In general, a line of code will look like this: DATA %&gt;% ggplot() + LAYER 1 + LAYER 2 + … + LAYER N Usually, the first added layer defines the geometry. We want to make a scatterplot. So what geometry do we use? Taking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is geom_point. (Source: RStudio) (Source: RStudio) Geometry function names follow this pattern: geom and the name of the geometry connected by an underscore. For geom_point to know what to do, we need to provide data and a mapping. We have already connected the object p with the murders data table and, if we add as a layer geom_point, we will default to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file geom_point help file: Aesthetics geom_point understands the following aesthetics (required aesthetics are in bold): x y alpha colour and, as expected, we see that at least two arguments are required x and y. 19.5 Aesthetic mappings aes will be one of the functions you will most use. This function connects data with what we see on the graph. We refer to this connect as the aesthetic mappings. The outcome of this function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions: murders %&gt;% ggplot() + geom_point(aes(x = population/10^6, y = total)) We can drop the x = and y = if we wanted to since these are the first and second expected arguments, as seen in the help page. We can also add a layer to the p object that has defined above as p &lt;- ggplot(data = murders): p + geom_point(aes(population/10^6, total)) The scale and labels are defined by default when adding this layer. We also use the variable names from the object component: population and total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error. 19.6 Layers A second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text respectively. Because each state (each point) has a label, we need an aesthetic mapping to make the connection. By reading the help file, we learn that we supply the mapping between point and label through the label argument of aes. So the code looks like this: p + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb)) We have successfully added a second layer to the plot. As an example of the unique behavior of aes mentioned above, note that this call: p_test &lt;- p + geom_text(aes(population/10^6, total, label = abb)) is fine, whereas this call: p_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) will give you an error since abb is not found once it is outside of the aes function. The layer geom_text does not know where to find abb since it is not a global variable. 19.7 Tinkering with arguments Each geometry function has many arguments other than aes and data. They tend to be specific to the function. For example, in the plot we wish to make, the points are larger than the default ones. In the help file we see that size is an aesthetic and we can change it like this: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb)) size is not a mapping, it affects all the points so we do not need to include it inside aes. Now that the points are larger, it is hard to see the labels. If we read the help file for geom_text, we see the nudge_x argument, which moves the text slightly to the right: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb), nudge_x = 1) This is preferred as it makes it easier to read the text. 19.8 Global versus local aesthetic mappings In the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings: args(ggplot) #&gt; function (data = NULL, mapping = aes(), ..., environment = parent.frame()) #&gt; NULL If we define a mapping in ggplot, then all the geometries that are added as layers will default to this mapping. We redefine p: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) and then we can simply use code as follows: p + geom_point(size = 3) + geom_text(nudge_x = 1.5) We keep the size and nudge_x argument in geom_point and geom_text respectively because we want to only increase the size of points and only nudge the labels. Also, note that the geom_point function does not need a label argument and therefore ignores it. If necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example: p + geom_point(size = 3) + geom_text(aes(x = 10, y = 800, label = &quot;Hello there!&quot;)) Clearly, the second call to geom_text does not use the population and total. 19.9 Scales First, our desired scales are in log-scale. This is not the default so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous lets us control the behavior of scales. We use them like this: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) Because we are in the log-scale now, the nudge must be made smaller. This particular transformation is so common that ggplot2 provides specialized functions: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() 19.10 Labels and titles Similarly, the cheat sheet quickly reveals that to change labels and add a title, we use the following functions: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) We are almost there! All we have left to do is add color, a legend and optional changes to the style. 19.11 Categories as colors We can change the color of the points using the col argument in the geom_point function. To facilitate exposition, we will redefine p to be everything except the points layer: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) and then test out what happens by adding different calls to geom_point. We can make all the points blue by adding the color argument: p + geom_point(size = 3, color =&quot;blue&quot;) This, of course, is not what we want. We want to assign color depending on the geographical region. A nice default behavior of ggplot2 is that if we assign a categorical variable to color, it automatically assigns a different color to each category. It also adds a legend! To map each point to a color, we need to use aes since this is a mapping. We use the following code: p + geom_point(aes(col=region), size = 3) The x and y mappings are inherited from those already defined in p. So we do not redefine them. We also move aes to the first argument since that is where the mappings are expected in this call. Here we see yet another useful default behavior: ggplot2 has automatically added a legend that maps color to region. 19.12 Annotation and shapes We often want to add shapes or annotation to figures that are not derived directly from the aesthetic mapping; examples include labels, boxes, shaded areas and lines. Here we want to add a line that represents the average murder rate for the entire country. Once we determine the per million rate to be \\(r\\), this line is defined by the formula: \\(y = r x\\), with \\(y\\) and \\(x\\) our axes: total murders and population in millions respectively. In the log-scale this line turns into: \\(\\log(y) = \\log(r) + \\log(x)\\). So in our plot it’s a line with slope 1 and intercept \\(\\log(r)\\). To compute this value, we use our dplyr skills: r &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 10^6) %&gt;% .$rate To add a line we use the geom_abline function. ggplot2 uses ab in the name to remind us we are supplying the intercept (a) and slope (b). The default line has slope 1 and intercept 0 so we only have to define the intercept: p + geom_point(aes(col=region), size = 3) + geom_abline(intercept = log10(r)) Here geom_abline does not use any information from the data object. We can change the line type and color of the lines using arguments. Also, we draw it first so it doesn’t go over our points. p &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) Note that we redefined p. 19.13 Adjustments The default plots created by ggplot2 are already very useful. However, we frequently need to make minor tweaks to the default behavior. Although it is not always obvious how to make these even with the cheat sheet, ggplot2 is very flexible. For example, we can make changes to the legend via the scale_color_discrete function. In our plot the word region is capitalized and we can change it like this: p &lt;- p + scale_color_discrete(name = &quot;Region&quot;) 19.14 Add-on packages The power of ggplot2 is augmented further due to the availability of add-on packages. The remaining changes needed to put the finishing touches on our plot require the ggthemes and ggrepel packages. The style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package. In fact, for most of the plots in this book, we use a function in the dslabs package that automatically sets a default theme: ds_theme_set() Many other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this: library(ggthemes) p + theme_economist() You can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead. The final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The add-on package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel. 19.15 Putting it all together Now that we are done testing, we can write one piece of code that produces our desired plot from scratch. library(ggthemes) library(ggrepel) ### First define the slope of the line r &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 10^6) %&gt;% .$rate ## Now make the plot murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name = &quot;Region&quot;) + theme_economist() 19.16 Other geometries Now let’s try to make the summary plots we have described in this chapter. Histogram Let’s start with the histogram. First, we need to use dplyr to filter the data: heights %&gt;% filter(sex==&quot;Male&quot;) Once we have a dataset, the next step is deciding what geometry we need. If you guessed geom_histogram, you guessed correctly. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. The code looks like this: p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(x = height)) p + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As before, we can drop the x =. This call gives us a message: stat_bin() using bins = 30. Pick better value with binwidth. We previously used a bin size of 1 inch, so the code looks like this: p + geom_histogram(binwidth = 1) Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title: Density To create a smooth density, we need a different geometry: we used geom_density instead. p + geom_density() To fill in with color, we can use the fill argument. p + geom_density(fill=&quot;blue&quot;) QQ-plots For qq-plots we use the geom_qq geometry. From the help file, we learn that we need to specify the sample (we will learn about samples in a later chapter). p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) p + geom_qq() By default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, again from the help file, we use the dparams arguments. params &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% summarize(mean = mean(height), sd = sd(height)) p + geom_qq(dparams = params) Adding an identity line is as simple as assigning another layer. For straight lines, we use the geom_abline function. To help you remember the name of this function, remember that the ab in front of line serves to remind us that we need to supply an intercept (a) and slope (b) to draw the line \\(y=a+bx\\). The default is the identity a=0 and b=1. p + geom_qq(dparams = params) + geom_abline() Another option here is to scale the data first and then make a qqplot against the standard normal: heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() 19.17 Grids of plots There are often reasons to graph plots next to each other. The gridExtra package permits us to do that: p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(x = height)) p1 &lt;- p + geom_histogram(binwidth = 1, fill = &quot;blue&quot;, col=&quot;black&quot;) p2 &lt;- p + geom_histogram(binwidth = 2, fill = &quot;blue&quot;, col=&quot;black&quot;) p3 &lt;- p + geom_histogram(binwidth = 3, fill = &quot;blue&quot;, col=&quot;black&quot;) To print them all side-by-side, we can use the function grid.arrange in the gridExtra package: library(gridExtra) grid.arrange(p1,p2,p3, ncol = 3) 19.18 Quick plots with qplot library(tidyverse) library(dslabs) data(murders) We have learned the powerful approach to generating visualization with ggplot. However, there are instances in which all we want is to make a quick plot of, for example, a histogram of the values in a vector, a scatter plot of the values in two vectors, or a boxplot using categorical and numeric vectors. We demonstrated how to generate these plots with hist, plot, and boxplot. However, if we want to keep consistent with the ggplot style, we can use the function qplot. So if we have values in a vector, say: x &lt;- murders$population and we want to make a histogram with ggplot, we would have to type something like: data.frame(x = x) %&gt;% ggplot(aes(x)) + geom_histogram(bins=15) Using R-base it is much quicker: hist(x) However, using qplot we can generate a plot using the ggplot style just as quickly: qplot(x, bins=15) Looking at the help file for the qplot function, we see several ways in which we can improve the look of the plot: qplot(x, bins=15, color = I(&quot;black&quot;), xlab = &quot;Population&quot;) The reason we use I(&quot;black&quot;) is because we want qplot to treat &quot;black&quot; as a character rather than convert it to a factor, which is the default behavior within aes, which is internally called here. In general, the function I is used in R to say “keep it as it is!”. One convenient feature of qplot is that it guesses what plot we want. For example, if we call it with two variables we get a scatterplot y &lt;- murders$total qplot(x, y) and if the first argument is a factor we get a points plot like this: f &lt;- murders$region qplot(f, y) We can also explicitly ask for a geometry using the geom argument: qplot(f, y, geom = &quot;boxplot&quot;) We can also explicitly tell qplot what dataset to use: qplot(population, total, data = murders) Exercises Start by loading the tidyverse or ggplot2 library as well as the murders and heights data. library(tidyverse) library(dslabs) data(heights) data(murders) With ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this p &lt;- ggplot(data = murders) Because data is the first argument we don’t need to spell it out p &lt;- ggplot(murders) and we can also use the pipe: p &lt;- murders %&gt;% ggplot() What is class of the object p? Remember that to print an object you can use the command print or simply type the object. For example x &lt;- 2 x print(x) Print the object p defined in exercise one and describe what you see. A. Nothing happens B. A blank slate plot C. A scatter plot D. A histogram Using the pipe %&gt;%, create an object p but this time associated with the heights dataset instead of the murders dataset. What is the class of the object p you have just created? Now we are going to add a layers and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders. A. state and abb B. total_murers and population_size total and population murders and size To create the scatter plot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables respectively. So the code looks like this: murders %&gt;% ggplot(aes(x = , y = )) + geom_point() except we have to define the two variables x and y. Fill this out with the correct variable names. Note that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this: murders %&gt;% ggplot(aes(population, total)) + geom_point() Remake the plot but now with total in the x-axis and population in the y-axis. If instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code murders %&gt;% ggplot(aes(population, total)) + geom_label() will give us the error message: Error: geom_label requires the following missing aesthetics: label Why is this? A. We need to map a character to each point through the label argument in aes B. We need to let geom_label know what character to use in the plot C. The geom_label geometry does not require x-axis and y-axis values. D. geom_label is not a ggplot2 command Rewrite the code above to abbreviation as the label through aes Now let’s change the color of the labels through blue. How will we do this? A. Adding a column called blue to murders B. Because each label needs a different color we map the colors through aes C. Use the color argument in ggplot D. Because we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label Rewrite the code above to make the labels blue Now suppose we want to use color to represent the different regions. In this case which of the following is most appropriate: A. Adding a column called color to murders with the color we want to use. B. Because each label needs a different color we map the colors through the color argument of aes . C. Use the color argument in ggplot D. Because we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label Rewrite the code above to make the labels’ color be determined by the state’s region. Now we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by define an object p holding the plot we have made up to now p &lt;- murders %&gt;% ggplot(aes(population, total, label = abb, color = region)) + geom_label() To change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot Repeat the previous exercise but now change both axes to be in the log scale Now edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function. Now we are going to use the geom_histogram function to make a histogram of the heights in the height data frame. When reading the documentation for this function we see that it requires just one mapping, the values to be used for the histogram. Make a histogram of all the plots. What is the variable containing the heights? A. sex B. heights C. height D. heights$height Now create a ggplot object using the pipe to assign the heights data to a ggplot object an assign height to the x values through the aes function Now we are ready to add a layer to actually make the histogram. Use the object p you have just created and the geom_histogram function to make the histogram Note that when we run the code in the previous exercise we get the warning: stat_bin() using bins = 30. Pick better value with binwidth.` Use the binwidth argument to change the histogram made in the previous exercise to use bins of size 1 inch. Now instead of histogram we are going to make a smooth density plot. In this case we will not make a object. instead we will render the plot with one line of code. Change the code previous used heights %&gt;% ggplot(aes(height)) + geom_histogram() to make a smooth density instead of a histogram. Now we are going to make a density plot for males and females separately. We can do this using any of the following arguments group. We do this through the aesthetic mapping as each point need to be assigned the calculation of a density. We can also assign groups through the color argument. This has the added benefit that it uses color to distinguish the groups. Change the code above to use color We can also assign groups through the fill argument. This has the added benefit that it uses color to distinguish the groups. Like this: heights %&gt;% ggplot(aes(height, fill = sex)) + geom_density() However, here the second density is drawn over the other. We can change this by using alpha blending. Set the alpha parameter to 0.2 in the geom_density function to make this change. "],
["case-study-trends-in-world-health-and-economics.html", "Chapter 20 Case study: Trends in world health and economics 20.1 Example 1: Life expectancy and fertility rates 20.2 Faceting 20.3 Fixed scales for better comparisons 20.4 Time series plots 20.5 Labels for legends 20.6 Example 2: Income distribution 20.7 Transformations 20.8 Modes 20.9 Stratify and boxplot 20.10 Comparing distributions 20.11 Ecological fallacy", " Chapter 20 Case study: Trends in world health and economics In this section, we will demonstrate how relatively simple ggplot2 code can create insightful and aesthetically pleasing plots that help us better understand trends in world health and economics. We later augment the code somewhat to perfect the plots and describe some general data visualization principles. 20.1 Example 1: Life expectancy and fertility rates Hans Rosling was the co-founder of the Gapminder Foundation, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies and other unfortunate events. As stated in the Gapminder Foundation’s website: Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!” Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: New Insights on Poverty and The Best Stats You’ve Ever Seen. Specifically, in this section, we use data to attempt to answer the following two questions: Is it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia and Latin America? Has income inequality across countries worsened during the last 40 years? To answer these questions, we will be using the gapminder dataset provided in dslabs. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this: library(dslabs) data(gapminder) head(gapminder) #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 1960 115.4 62.9 6.19 #&gt; 2 Algeria 1960 148.2 47.5 7.65 #&gt; 3 Angola 1960 208.0 36.0 7.32 #&gt; 4 Antigua and Barbuda 1960 NA 63.0 4.43 #&gt; 5 Argentina 1960 59.9 65.4 3.11 #&gt; 6 Armenia 1960 NA 66.9 4.55 #&gt; population gdp continent region #&gt; 1 1636054 NA Europe Southern Europe #&gt; 2 11124892 1.38e+10 Africa Northern Africa #&gt; 3 5270844 NA Africa Middle Africa #&gt; 4 54681 NA Americas Caribbean #&gt; 5 20619075 1.08e+11 Americas South America #&gt; 6 1867396 NA Asia Western Asia Hans Rosling’s quiz As done in the New Insights on Poverty video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar? Sri Lanka or Turkey Poland or South Korea Malaysia or Russia Pakistan or Vietnam Thailand or South Africa When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand and South Africa, have similarly high mortality rates. To answer these questions with data, we can use dplyr. For example, for the first comparison we see that: gapminder %&gt;% filter(year == 2015 &amp; country %in% c(&quot;Sri Lanka&quot;,&quot;Turkey&quot;)) %&gt;% select(country, infant_mortality) #&gt; country infant_mortality #&gt; 1 Sri Lanka 8.4 #&gt; 2 Turkey 11.6 Turkey has the higher infant mortality rate. We can use this code on all comparisons and find the following: country infant_mortality country1 infant_mortality1 Sri Lanka 8.4 Turkey 11.6 Poland 4.5 South Korea 2.9 Malaysia 6.0 Russia 8.2 Pakistan 65.8 Vietnam 17.3 Thailand 10.5 South Africa 33.6 We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5. The average score was worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. A scatterplot The reason for this stems from the preconceived notion that the world is divided into two groups: the western world (Western Europe and North America), characterized by long life spans and small families, versus the developing world (Africa, Asia, and Latin America) characterized by short life spans and large families. But, does the data support this dichotomous view? The necessary data to answer this question is also available in our gapminder table. Using our newly learned data visualization skills, we will be able to tackle this challenge. In order to analyze this world view, our first plot is a scatterplot of life expectancy versus fertility rates (average number of children per woman). We start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds. filter(gapminder, year==1962) %&gt;% ggplot( aes(fertility, life_expectancy)) + geom_point() Most points fall into two distinct categories: Life expectancy around 70 years and 3 or less children per family. Life expectancy lower then 65 years and more than 5 children per family. To confirm that indeed these countries are from the regions we expect, we can use color to represent continent. filter(gapminder, year==1962) %&gt;% ggplot( aes(fertility, life_expectancy, color = continent)) + geom_point() So in 1962, “the west versus developing world” view was grounded in some reality. But is this still the case 50 years later? 20.2 Faceting We could easily plot the 2012 data in the same way we did for 1962. But to compare, side by side plots are preferable. In ggplot2, we can achieve this by faceting variables: we stratify the data by some variable and make the same plot for each strata. To achieve faceting, we add a layer with the function facet_grid, which automatically separates the plots. This function lets you facet by up to two variables using columns to represent one variable and rows to represent the other. The function expects the row and column variables to be separated by a ~. Here is an example of a scatterplot with facet_grid added as the last layer: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(continent~year) We see a plot for each continent/year pair. However, this is just an example and more than what we want, which is simply to compare 1962 and 2012. In this case, there is just one variable and we use . to let facet know that we are not using one of the variables: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid( . ~ year) This plot clearly shows that the majority of countries have moved from the developing world cluster to the western world one. In 2012, the western versus developing world view no longer makes sense. This is particularly clear when comparing Europe to Asia, the latter which includes several countries that have made great improvements. facet_wrap To explore how this transformation happened through the years, we can make the plot for several years. For example, we can add 1970, 1980, 1990, and 2000. If we do this, we will not want all the plots on the same row, the default behavior of facet_grid, since they will become too thin to show the data. Instead, we will want to use multiple rows and columns. The function facet_wrap permits us to do this by automatically wrapping the series of plots so that each display has viewable dimensions: years &lt;- c(1962, 1980, 1990, 2000, 2012) continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder %&gt;% filter(year %in% years &amp; continent %in% continents) %&gt;% ggplot( aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(~year) This plot clearly shows how most Asian countries have improved at a much faster rate than European ones. 20.3 Fixed scales for better comparisons The default choice of the range of the axes is an important one. When not using facet, this range is determined by the data shown in the plot. When using facet, this range is determined by the data shown in all plots and therefore kept fixed across plots. This makes comparisons across plots much easier. For example, in the above plot, we can see that life expectancy has increased and the fertility has decreased across most countries. We see this because the cloud of points moves. This is not the case if we adjust the scales: In the plot above, we have to pay special attention to the range to notice that the plot on the right has a larger life expectancy. 20.4 Time series plots The visualizations above effectively illustrates that data no longer supports the western versus developing world view. Once we see these plots, new questions emerge. For example, which countries are improving more and which ones less? Was the improvement constant during the last 50 years or was it more accelerated during certain periods? For a closer look that may help answer these questions, we introduce time series plots. Time series plots have time in the x-axis and an outcome or measurement of interest on the y-axis. For example, here is a trend plot of United States fertility rates: gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year,fertility)) + geom_point() We see that the trend is not linear at all. Instead there is sharp drop during the 60s and 70s to below 2. Then the trend comes back to 2 and stabilizes during the 90s. When the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single country. To do this, we use the geom_line function instead of geom_point. gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year,fertility)) + geom_line() This is particularly helpful when we look at two countries. If we subset the data to include two countries, one from Europe and one from Asia, then adapt the code above: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility)) + geom_line() Unfortunately, this is not the plot that we want. Rather than a line for each country, the points for both countries are joined. This is actually expected since we have not told ggplot anything about wanting two separate lines. To let ggplot know that there are two curves that need to be made separately, we assign each point to a group, one for each country: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility, group = country)) + geom_line() But which line goes with which country? We can assign colors to make this distinction. A useful side-effect of using the color argument to assign different colors to the different countries is that the data is automatically grouped: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility, col = country)) + geom_line() #&gt; Warning: Removed 2 rows containing missing values (geom_path). The plot clearly shows how South Korea’s fertility rate dropped drastically during the 60s and 70s, and by 1990 had a similar rate to that of Germany. 20.5 Labels for legends For trend plots we recommend labeling the lines rather than using legends since the viewer can quickly see which line is which country. This suggestion actually applies to most plots: labeling is usually preferred over legends. We demonstrate how we can do this using the life expectancy data. We define a data table with the label locations and then use a second mapping just for these labels: labels &lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72)) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year, life_expectancy, col = country)) + geom_line() + geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = &quot;none&quot;) The plot clearly shows how an improvement in life expectancy followed the drops in fertility rates. In 1960, Germans lived 15 years longer than South Koreans, although by 2010 the gap is completely closed. It exemplifies the improvement that many non-western countries have achieved in the last 40 years. 20.6 Example 2: Income distribution Another commonly held notion is that wealth distribution across the world has become worse during the last decades. When general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes. By using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case. We will also learn how transformations can sometimes help provide more informative summaries and plots. 20.7 Transformations The gapminder data table includes a column with the countries gross domestic product (GDP). GDP measures the market value of goods and services produced by a country in a year. The GDP per person is often used as a rough summary of a country’s wealth. Here we divide this quantity by 365 to obtain the more interpretable measure dollars per day. Using current US dollars as a unit, a person surviving on an income of less than $2 a day is defined to be living in absolute poverty. We add this variable to the data table: gapminder &lt;- gapminder %&gt;% mutate(dollars_per_day = gdp/population/365) The GDP values are adjusted for inflation and represent current US dollars, so these values are meant to be comparable across the years. Of course, these are country averages and within each country there is much variability. All the graphs and insights described below relate to country averages and not to individuals. Country income distribution Here is a histogram of per day incomes from 1970: past_year &lt;- 1970 gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) We use the color = &quot;black&quot; argument to draw a boundary and clearly distinguish the bins. In this plot, we see that for the majority of countries, averages are below $10 a day. However, the majority of the x-axis is dedicated to the 35 countries with averages above $10. So the plot is not very informative about countries with values below $10 a day. It might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day. These changes are multiplicative and log transformations convert multiplicative changes into additive ones: when using base 2, a doubling of a value turns into an increase by 1. Here is the distribution if we apply a log base 2 transform: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(log2(dollars_per_day))) + geom_histogram(binwidth = 1, color = &quot;black&quot;) In a way this provides a close up of the mid to lower income countries. Which base? In the case above, we used base 2 in the log transformations. Other common choices are base \\(\\mathrm{e}\\) (the natural log) and base 10. In general, we do not recommend using the natural log for data exploration and visualization. This is because while \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^1, 10^2, \\dots\\) are easy to compute in our heads, the same is not true for \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\). In the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret. The range of the values being plotted is 0.327, 48.885. In base 10, this turns into a range that includes very few integers: just 0 and 1. With base two, our range includes -2, -1, 0, 1, 2, 3, 4 and 5. It is easier to compute \\(2^x\\) and \\(10^x\\) when \\(x\\) is an integer and between -10 and 10, so we prefer to have smaller integers in the scale. Another consequence of a limited range is that choosing the binwidth is more challenging. With log base 2, we know that a binwidth of 1 will translate to a bin with range \\(x\\) to \\(2x\\). For an example in which base 10 makes more sense, consider population sizes. A log base 10 is preferable since the range for these is: filter(gapminder, year == past_year) %&gt;% summarize(min = min(population), max = max(population)) #&gt; min max #&gt; 1 46075 8.09e+08 Here is the histogram of the transformed values: gapminder %&gt;% filter(year == past_year) %&gt;% ggplot(aes(log10(population))) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;) In the above, we quickly see that country populations range between ten thousand and ten billion. Transform the values or the scale? There are two ways we can use log transformations in plots. We can log the values before plotting them or use log scales in the axes. Both approaches are useful and have different strengths. If we log the data, we can more easily interpret intermediate values in the scale. For example, if we see: —-1—-x—-2——–3—- for log transformed data, we know that the value of \\(x\\) is about 1.5. If the scales are logged: —-1—-x—-10——100— then, to determine x, we need to compute \\(10^{1.5}\\), which is not easy to do in our heads. The advantage of using logged scales is that we see the original values on the axes. However, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret. For example, we would see “32 dollars a day” instead of “5 log base 2 dollars a day”. As we learned earlier, if we want to scale the axis with logs, we can use the scale_x_ccontinuous function. So instead of logging the values first, we apply this layer: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) Note that the log base 10 transformation has its own function: scale_x_log10(), but currently base 2 does not, although we could easily define our own. There are other transformations available through the trans argument. As we learn later on, the square root (sqrt) transformation, for example, is useful when considering counts. The logistic transformation (logit) is useful when plotting proportions between 0 and 1. The reverse transformation is useful when we want smaller values to be on the right or on top. 20.8 Modes In statistics these bumps are sometimes referred to as modes. The mode of a distribution is the value with the highest frequency. The mode of the normal distribution is the average. When a distribution, like the one above, doesn’t monotonically decrease from the mode, we call the locations where it goes up and down again local modes and say that the distribution has multiple modes. The histogram above suggests that the 1970 country income distribution has two modes: one at about 2 dollars per day (1 in the log 2 scale) and another at about 32 dollars per day (5 in the log 2 scale). This bimodality is consistent with a dichotomous world made up of countries with average incomes less than $8 (3 in the log 2 scale) a day and countries above that. 20.9 Stratify and boxplot The histogram showed us that the income distribution values show a dichotomy. However, the histogram does not show us if the two groups of countries are west versus the developing world. To see distributions by geographical region, we first stratify the data into regions and then examine the distribution for each. Because of the number of regions: n_distinct(gapminder$region) #&gt; [1] 22 looking at histograms or smooth densities for each will not be useful. Instead, we can stack boxplots next to each other: p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(region, dollars_per_day)) p + geom_boxplot() Now we can’t read the region names because the default gpplot2 behavior is to write the labels horizontally and here we run out of room. We can easily fix this by rotating the labels. Consulting the cheat sheet we find we can rotate the names by changing the theme through element_text. The hjust=1 justifies the text so that it is next to the axis. p + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) We can already see that there is indeed a “west versus the rest” dichotomy. Do not order alphabetically There are a few more adjustments we can make to this plot that help uncover this reality. First, it helps to order the regions in the boxplots from poor to rich rather than alphabetically. This can be achieved using the reorder function. This function lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. Remember that many graphing functions coerce character vectors into a factor. The default behavior results in alphabetically ordered levels: fac &lt;- factor(c(&quot;Asia&quot;, &quot;Asia&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;)) levels(fac) #&gt; [1] &quot;Asia&quot; &quot;West&quot; value &lt;- c(10, 11, 12, 6, 4) fac &lt;- reorder(fac, value, FUN = mean) levels(fac) #&gt; [1] &quot;West&quot; &quot;Asia&quot; Second, we can use color to distinguish the different continents, a visual cue that helps find specific regions. Here is the code: p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% ggplot(aes(region, dollars_per_day, fill = continent)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;&quot;) p This plot shows two clear groups, with the rich group composed of North America, Northern and Western Europe, New Zealand and Australia. As with the histogram, if we remake the plot using a log scale: p + scale_y_continuous(trans = &quot;log2&quot;) we are better able to see differences within the developing world. Show the data In many cases, we do not show the data because it adds clutter to the plot and obfuscates the message. In the example above, we don’t have that many points so adding them actually lets us see all the data. We can add this layer using geom_point(): p + scale_y_continuous(trans = &quot;log2&quot;) + geom_point(show.legend = FALSE) 20.10 Comparing distributions The exploratory data analysis above has revealed two characteristics about average income distribution in 1970. Using a histogram, we found a bimodal distribution with the modes relating to poor and rich countries. Then by stratifying by region and examining boxplots, we found that the rich countries were mostly in Europe and Northern America, along with Australia and New Zealand. We define a vector with these regions: west &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) Now we want to focus on comparing the differences in distributions across time. We start by confirming that the bimodality observed in 1970 is explained by a “west versus developing world” dichotomy. We do this by creating histograms for the previously identified groups. We create the two groups with an ifelse inside a mutate and then we use facet_grid to make a histogram for each group: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(group = ifelse(region%in%west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ group) Now we are ready to see if the separation is worse today than it was 40 years ago. We do this by faceting by both region and year: past_year &lt;- 1970 present_year &lt;- 2010 gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; !is.na(gdp)) %&gt;% mutate(group = ifelse(region%in%west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) Before we interpret the findings of this plot, we notice that there are more countries represented in the 2010 histograms than in 1970: the total counts are larger. One reason for this is that several countries were founded after 1970. For example, the Soviet Union divided into several countries, including Russia and Ukraine, during the 1990s. Another reason is that data was available for more countries in 2010. We remake the plots using only countries with data available for both years. In the data wrangling chapter, we will learn tidyverse tools that permit us to write efficient code for this, but here we can use simple code using the intersect function: country_list_1 &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% .$country country_list_2 &lt;- gapminder %&gt;% filter(year == present_year &amp; !is.na(dollars_per_day)) %&gt;% .$country country_list &lt;- intersect(country_list_1, country_list_2) These 108 account for 86 % of the world population, so this subset should be representative. Let’s remake the plot, but only for this subset by simply adding country %in% country_list to the filter function: We now see that while the rich countries have become a bit richer, percentage-wise, the poor countries appear to have improved more. In particular, we see that the proportion of developing countries earning more than $16 a day increases substantially. To see which specific regions improved the most, we can remake the boxplots we made above but now adding the year 2010: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) and then using facet to compare the two years: p + geom_boxplot(aes(region, dollars_per_day, fill = continent)) + facet_grid(year~.) Here, we pause to introduce another powerful ggplot2 feature. Because we want to compare each region before and after, it would be convenient to have the 1970 boxplot next to the 2010 boxplot for each region. In general, comparisons are easier when data are plotted next to each other. So instead of faceting, we keep the data from each year together and ask to color (or fill) them depending on the year. Note that groups are automatically separated by year and each pair of boxplots drawn next to each other. Finally, because year is a number, we turn it into a factor since ggplot2 automatically assigns a color to each category of a factor: p + geom_boxplot(aes(region, dollars_per_day, fill = factor(year))) Finally, we point out that if what we are most interested is in comparing before and after values, it might make more sense to plot the ratios, or difference in the log scale. We are still not ready to learn to code this, but here is what the plot would look like: Density plots We have used data exploration to discover that the income gap between rich and poor countries has narrowed considerably during the last 40 years. We used a series of histograms and boxplots to see this. Here we suggest a succinct way to convey this message with just one plot. We will use smooth density plots. Let’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing: gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day)) + geom_density(fill = &quot;grey&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year~.) In the 1970 plot, we see two clear modes: poor and rich countries. In 2010, it appears that some of the poor countries have shifted towards the right, closing the gap. The next message we need to convey is that the reason for this change in distribution is that poor countries became richer, rather than some rich countries becoming poorer. To do this, we need to assign a color to the groups we identified during data exploration. However, we first need to learn how to make these smooth densities in a way that preserves information on the number of countries in each group. To understand why we need this, note the discrepancy in the size of each group: group n Developing 87 West 21 However, when we overlay two densities, the default is to have the area represented by each distribution add up to 1, regardless of the size of each group: gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% mutate(group = ifelse(region %in% west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density(alpha = 0.2) + facet_grid(year ~ .) which makes it appear as if there are the same number of countries in each group. To change this, we will need to learn to access computed variables with geom_density function. Accessing computed variables To have the areas of these densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. From the geom_density help file, we see that the functions compute a variable called count that does exactly this. We want this variable to be on the y-axis rather than the density. In ggplot2, we access these variables by surrounding the name with two dots ... So we will use the following mapping: aes(x = dollars_per_day, y = ..count..) We can now create the desired plot by simply changing the mapping in the previous code chunk: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% mutate(group = ifelse(region %in% west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density(alpha = 0.2) + facet_grid(year ~ .) If we want the densities to be smoother, we use the bw argument. We tried a few and decided on 0.75: p + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .) This plot now shows what is happening very clearly. The developing world distribution is changing. A third mode appears consisting of the countries that most narrowed the gap. 20.10.1 ‘case_when’ We can actually make this figure somewhat more informative. From the exploratory data analysis, we noticed that many of the countries that most improved were from Asia. We can easily alter the plot to show key regions separately. We introduce the case_when function which is useful for defining groups. It currently does not have a data argument (this might change) so we need to access the components of our data table using the dot placeholder: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( .$region %in% west ~ &quot;West&quot;, .$region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, .$region %in% c(&quot;Caribbean&quot;, &quot;Central America&quot;, &quot;South America&quot;) ~ &quot;Latin America&quot;, .$continent == &quot;Africa&quot; &amp; .$region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan Africa&quot;, TRUE ~ &quot;Others&quot;)) We turn this group variable into a factor to control the order of the levels: gapminder &lt;- gapminder %&gt;% mutate(group = factor(group, levels = c(&quot;Others&quot;, &quot;Latin America&quot;, &quot;East Asia&quot;, &quot;Sub-Saharan Africa&quot;, &quot;West&quot;))) We pick this particular order for a reason that becomes clear later. Now we can now easily plot the densities for each region. We use color and size to clearly see the tops: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group, color = group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density(alpha = 0.2, bw = 0.75, size = 2) + facet_grid(year ~ .) The plot is cluttered and somewhat hard to read. A clearer picture is sometimes achieved by stacking the densities on top of each other: p + geom_density(alpha = 0.2, bw = 0.75, position = &quot;stack&quot;) + facet_grid(year ~ .) Here we can clearly see how the distributions for East Asia, Latin America and others shift markedly to the right. While Sub-Saharan Africa remains stagnant. Notice that we order the levels of the group so that the West’s density are plotted first, then Sub-Saharan Africa. Having the two extremes plotted first allows us to see the remaining bimodality better. Weighted densities As a final point, we note that these distributions weigh every country the same. So if most of the population is improving, but living in a very large country, such as China, we might not appreciate this. We can actually weight the smooth densities using the weight mapping argument. The plot then looks like this: This particular figure shows very clearly how the income distribution gap is closing with most of the poor remaining in Sub-Saharan Africa. 20.11 Ecological fallacy Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income. We start by comparing these quantities across regions, but before doing this, we define a few more regions: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( .$region %in% west ~ &quot;The West&quot;, .$region %in% &quot;Northern Africa&quot; ~ &quot;Northern Africa&quot;, .$region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, .$region == &quot;Southern Asia&quot;~ &quot;Southern Asia&quot;, .$region %in% c(&quot;Central America&quot;, &quot;South America&quot;, &quot;Caribbean&quot;) ~ &quot;Latin America&quot;, .$continent == &quot;Africa&quot; &amp; .$region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan Africa&quot;, .$region %in% c(&quot;Melanesia&quot;, &quot;Micronesia&quot;, &quot;Polynesia&quot;) ~ &quot;Pacific Islands&quot;)) We then compute these quantities for each region: surv_income &lt;- gapminder %&gt;% filter(year %in% present_year &amp; !is.na(gdp) &amp; !is.na(infant_mortality) &amp; !is.na(group)) %&gt;% group_by(group) %&gt;% summarize(income = sum(gdp)/sum(population)/365, infant_survival_rate = 1 - sum(infant_mortality/1000*population)/sum(population)) surv_income %&gt;% arrange(income) #&gt; # A tibble: 7 x 3 #&gt; group income infant_survival_rate #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sub-Saharan Africa 1.76 0.936 #&gt; 2 Southern Asia 2.07 0.952 #&gt; 3 Pacific Islands 2.70 0.956 #&gt; 4 Northern Africa 4.94 0.970 #&gt; 5 Latin America 13.2 0.983 #&gt; 6 East Asia 13.4 0.985 #&gt; # ... with 1 more row This shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%! The relationship between these two variables is almost perfectly linear: In this plot, we introduce the use of the limit argument which lets us change the range of the axes. We are making the range larger than the data requires because we will later compare this plot to one with more variability and we want the ranges to be the same. We also introduce the breaks argument, which lets us set the location of the axes labels. Finally, we introduce a new transformation, the logistic transformation. Logistic transformation The logistic or logit transformation for a proportion or rate \\(p\\) is defined as: \\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\] When \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\) is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments respectively. This scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases. Show the data Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that all survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on? Jumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story: Specifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries. "],
["data-visualization-principles.html", "Chapter 21 Data visualization principles 21.1 Encoding data using visual cues 21.2 Know when to include 0 21.3 Do not distort quantities 21.4 Order by a meaningful value 21.5 Show the data 21.6 Ease comparisons: use common axes 21.7 Ease comparisons: align plots vertically to see horizontal changes and horizontally to see vertical changes 21.8 Consider transformations 21.9 Ease comparisons: visual cues to be compared should be adjacent 21.10 Ease comparison: use color 21.11 Think of the color blind 21.12 Use scatterplots to examine the relationship between two variables 21.13 Encoding a third variable 21.14 Avoid pseudo three dimensional plots 21.15 Avoid gratuitous three dimensional plots 21.16 Avoid too many significant digits 21.17 Know your audience 21.18 Exercises", " Chapter 21 Data visualization principles We have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman titled “Creating Effective Figures and Tables” and includes some of the figures which were made with code that Karl makes available on his GitHub repository, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t. The principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distribution for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience. We will be using these libraries: library(gridExtra) 21.1 Encoding data using visual cues We start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue. To illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing four quantities – the four percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart: Here we are representing quantities with both areas and angles since both the angle and area of each pie slice is proportional to the quantity it represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when only area is the only available visual cue. The donut chart is an example of a plot that uses only area: To see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that: Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data. In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy. Browser 2000 2015 Opera 3 2 Safari 21 22 Firefox 23 21 Chrome 26 29 IE 28 27 The preferred way to plot these quantities is to use length and position as visual cues since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures. Notice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis. If for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area: In general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once. 21.2 Know when to include 0 When using barplots it is dishonest not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous via Media Matters via Fox News): (Source: Fox News, via Media Matters. ) From the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly: Another example, described in detail, can be found in the Flowing Data blog: (Source: Fox News, via Flowing Data This plot makes a 13% increase look like a five fold change. Here is the appropriate plot: When using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within the group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012: p1 &lt;- gapminder %&gt;% filter(year == 2012) %&gt;% ggplot(aes(continent, life_expectancy)) + geom_point() p2 &lt;- p1 + scale_y_continuous(limits = c(0, 84)) grid.arrange(p2, p1, ncol = 2) Note that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability. 21.3 Do not distort quantities During President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations: (Source: The 2011 State of the Union Address: Enhanced Version via Peter Aldhous ) Judging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area: Not surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length: 21.4 Order by a meaningful value When one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots where ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015. We previously learned how to use the reorder function, which helps us achieve this goal. To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate: data(murders) p1 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + xlab(&quot;&quot;) p2 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;% mutate(state = reorder(state, murder_rate)) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + xlab(&quot;&quot;) grid.arrange(p1, p2, ncol = 2) The reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other: The first orders the regions alphabetically, while the second orders them by the group’s median. 21.5 Show the data We have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups. To motivate our first principle, ‘show the data’, we go back to our artificial example of describing heights to ET, an extraterrestrial. This time let’s assume ET is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, shows the average and standard errors (standard errors are defined in a later chapter, but do not confuse them with the standard deviation of the data). The plot looks like this: The average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0, does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution. This brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points: heights %&gt;% ggplot(aes(sex, height)) + geom_point() For example, this plot gives us an idea of the range of the data. However, this plot has limitations as well since we can’t really see all the 238 and 812 points plotted for females and males respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points. The first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the height of the points do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending: heights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2) Now we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer. 21.6 Ease comparisons: use common axes Since there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group: However, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across to plots. Below we see how the comparison becomes easier: 21.7 Ease comparisons: align plots vertically to see horizontal changes and horizontally to see vertical changes In these histograms, the visual cue related to decreases or increases in height are shifts to the left or right respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed: p2 &lt;- heights %&gt;% ggplot(aes(height, ..density..)) + geom_histogram(binwidth = 1, color=&quot;black&quot;) + facet_grid(sex~.) p2 This plot makes it much easier to notice that men are, on average, taller. If instead of histograms, we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points: p3 &lt;- heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot(coef=3) + geom_jitter(width = 0.1, alpha = 0.2) + ylab(&quot;Height in inches&quot;) p3 Now contrast and compare these three plots, based on exactly the same data: grid.arrange(p1, p2, p3, ncol = 3) Notice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions. 21.8 Consider transformations We have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation. The combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015: From this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China: Using a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis: With the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia. Other transformations you should consider are the logistic transformation, useful to better see fold changes in odds, and the square root transformation, useful for count data. 21.9 Ease comparisons: visual cues to be compared should be adjacent When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below. A difference is that here we look at continents instead of regions, but this is not relevant to the point we are making. For each continent, we want to compare the distributions from 1970 to 2010. The default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging. But it is much easier to make the comparison when the boxplots are next to each other: 21.10 Ease comparison: use color The comparison becomes even easier to make if we use color to denote the two things we want to compare: 21.11 Think of the color blind About 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does it make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: color_blind_friendly_cols &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) p1 &lt;- data.frame(x=1:8, y=1:8, col = as.character(1:8)) %&gt;% ggplot(aes(x, y, color = col)) + geom_point(size=5) p1 + scale_color_manual(values=color_blind_friendly_cols) There are several resources that can help you select colors, for example this one. 21.12 Use scatterplots to examine the relationship between two variables In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. 21.12.1 Slope charts One exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart. There is no geometry for slope charts in ggplot2, but we can construct one using geom_lines. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries: west &lt;- c(&quot;Western Europe&quot;,&quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;,&quot;Australia and New Zealand&quot;) dat &lt;- gapminder %&gt;% filter(year%in% c(2010, 2015) &amp; region %in% west &amp; !is.na(life_expectancy) &amp; population &gt; 10^7) dat %&gt;% mutate(location = ifelse(year == 2010, 1, 2), location = ifelse(year == 2015 &amp; country %in% c(&quot;United Kingdom&quot;,&quot;Portugal&quot;), location+0.22, location), hjust = ifelse(year == 2010, 1, 0)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(year, life_expectancy, group = country)) + geom_line(aes(color = country), show.legend = FALSE) + geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) + xlab(&quot;&quot;) + ylab(&quot;Life Expectancy&quot;) An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot: In the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines. 21.12.2 Bland-Altman plot Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also know as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average: library(ggrepel) dat %&gt;% mutate(year = paste0(&quot;life_expectancy_&quot;, year)) %&gt;% select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;% mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2, difference = life_expectancy_2015 - life_expectancy_2010) %&gt;% ggplot(aes(average, difference, label = country)) + geom_point() + geom_text_repel() + geom_abline(lty = 2) + xlab(&quot;Average of 2010 and 2015&quot;) + ylab(&quot;Difference between 2015 and 2010&quot;) Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis. 21.13 Encoding a third variable An earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population. We encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside. For continuous variables, we can use color, intensity or size. We now show an example of how we do this with a case study. When selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer library(RColorBrewer) display.brewer.all(type=&quot;seq&quot;) Diverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns: library(RColorBrewer) display.brewer.all(type=&quot;div&quot;) 21.14 Avoid pseudo three dimensional plots The figure below, taken from the scientific literature, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable. (Source: Karl Broman) Humans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D: Notice how much easier it is to determine the survival values. 21.15 Avoid gratuitous three dimensional plots Pseudo 3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples: (Source: Karl Broman) (Source: Karl Broman) 21.16 Avoid too many significant digits By default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added the visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates for California across the five decades: state year Measles Pertussis Polio California 1940 37.8826320 18.3397861 18.3397861 California 1950 13.9124205 4.7467350 4.7467350 California 1960 14.1386471 0.0000000 0.0000000 California 1970 0.9767889 0.0000000 0.0000000 California 1980 0.3743467 0.0515466 0.0515466 We are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing: state year Measles Pertussis Polio California 1940 37.9 18.3 18.3 California 1950 13.9 4.7 4.7 California 1960 14.1 0.0 0.0 California 1970 1.0 0.0 0.0 California 1980 0.4 0.1 0.1 Useful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3). Another principle, related to displaying tables, is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one: state disease 1940 1950 1960 1970 1980 California Measles 37.9 13.9 14.1 1 0.4 California Pertussis 18.3 4.7 0.0 0 0.1 California Polio 18.3 4.7 0.0 0 0.1 21.17 Know your audience Graphs can be used for our 1) own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot. As a simple example, consider that for your own exploration, it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis will be much easier to digest. 21.18 Exercises For these exercises, we will be using the vaccines data in the dslabs package: library(dslabs) data(us_contagious_diseases) Pie charts are appropriate: A. When we want to display percentages. B. When ggplot2 is not available. C. When I am in a bakery. D. Never. Barplots and tables are always better. What is the problem with the plot below: A. The values are wrong. The final vote was 306 to 232. B. The axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more. C. The colors should be the same. D. Percentages should be shown as a pie chart. Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states. Which plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why? A. They provide the same information so they are both equally as good. B. The plot on the right is better because it orders the states alphabetically. C. The plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates. D. Both plots should be a pie chart. To make the plot on the left, we have to reorder the levels of the states’ variables. dat &lt;- us_contagious_diseases %&gt;% filter(year == 1967 &amp; disease==&quot;Measles&quot; &amp; !is.na(population)) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) Note what happens when we make a barplot: dat %&gt;% ggplot(aes(state, rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() Define these objects: state &lt;- dat$state rate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting Redefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels. Now with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and reorder the state variable so that the levels are reordered by this variable. Then make a barplot using the code above, but for this new dat. Say we are interested in comparing gun homicide rates across regions of the US. We see this plot: library(dslabs) data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) %&gt;% group_by(region) %&gt;% summarize(avg = mean(rate)) %&gt;% mutate(region = factor(region)) %&gt;% ggplot(aes(region, avg)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;Murder Rate Average&quot;) and decide to move to a state in the western region. What is the main problem with this interpretation? A. The categories are ordered alphabetically. B. The graph does not show standard errors. C. It does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West. D. The Northeast has the lowest average. Make a boxplot of the murder rates defined as data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) by region, showing all the points and ordering the regions by their median rate. The plots below show three continuous variables. The line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two dimensional points. Why is this happening? A. Humans are not good at reading pseudo 3D plots. B. There must be an error in the code C. The colors confuse us. D. Scatterplots should not be used to compare two variables when we have access to 3. Reproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks. Now reproduce the time series plot we previously made, but this time following the instructions of the previous question. For the state of California, make a time series plots showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease. Now do the same for the rates for the US. Hint: compute the US rate by using summarize, the total divided by total population. "],
["case-study-the-impact-of-vaccines-on-battling-infectious-diseases.html", "Chapter 22 Case study: The impact of vaccines on battling infectious diseases Exercises", " Chapter 22 Case study: The impact of vaccines on battling infectious diseases Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today, despite all the scientific evidence for their importance, vaccination programs have become somewhat controversial. The controversy started with a paper published in 1988 and lead by Andrew Wakefield claiming there was a link between the administration of the measles, mumps and rubella (MMR) vaccine, and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Center for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines. Effective communication of data is a strong antidote to misinformation and fear mongering. Earlier we used an example provided by a Wall Street Journal article showing data related to the impact of vaccines on battling infectious diseases. Here, we reconstruct that example. The data used for these plots were collected, organized and distributed by the Tycho Project. They include weekly reported counts data for seven diseases from 1928 to 2011, from all fifty states. We include the yearly totals in the dslabs package: data(us_contagious_diseases) str(us_contagious_diseases) #&gt; &#39;data.frame&#39;: 18870 obs. of 6 variables: #&gt; $ disease : Factor w/ 7 levels &quot;Hepatitis A&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ state : Factor w/ 51 levels &quot;Alabama&quot;,&quot;Alaska&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ year : num 1966 1967 1968 1969 1970 ... #&gt; $ weeks_reporting: int 50 49 52 49 51 51 45 45 45 46 ... #&gt; $ count : num 321 291 314 380 413 378 342 467 244 286 ... #&gt; $ population : num 3345787 3364130 3386068 3412450 3444165 ... We create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 50s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. So we have to adjust for that when computing the rate. the_disease &lt;- &quot;Measles&quot; dat &lt;- us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&gt;% mutate(state = reorder(state, rate)) We can now easily plot disease rates per year. Here are the measles data from California: dat %&gt;% filter(state == &quot;California&quot; &amp; !is.na(rate)) %&gt;% ggplot(aes(year, rate)) + geom_line() + ylab(&quot;Cases per 10,000&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 the yellow book. p. 250. ISBN 9780199948505]. Now can we show data for all states in one plot? We have three variables to show: year, state and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved. In our example, we want to use a sequential palette since there is no meaningful center, just low and high rates. We use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Once a disease was pretty much eradicated, some states stopped reporting cases all together. dat %&gt;% ggplot(aes(year, state, fill = rate)) + geom_tile(color = &quot;grey50&quot;) + scale_x_continuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank()) + ggtitle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity which we earlier explained makes it harder to know exactly how high it is going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this: avg &lt;- us_contagious_diseases %&gt;% filter(disease==the_disease) %&gt;% group_by(year) %&gt;% summarize(us_rate = sum(count, na.rm=TRUE)/sum(population, na.rm=TRUE)*10000) Now to make the plot we simply use the geom_line geometry: dat %&gt;% ggplot() + geom_line(aes(year, rate, group = state), color = &quot;grey50&quot;, show.legend = FALSE, alpha = 0.2, size = 1) + geom_line(mapping = aes(year, us_rate), data = avg, size = 1, color = &quot;black&quot;) + scale_y_continuous(trans = &quot;sqrt&quot;, breaks = c(5, 25, 125, 300)) + ggtitle(&quot;Cases per 10,000 by state&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + geom_text(data = data.frame(x = 1955, y = 50), mapping = aes(x, y, label=&quot;US average&quot;), color=&quot;black&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) #&gt; Warning: Removed 180 rows containing missing values (geom_path). In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors. Exercises Reproduce the image plot we previously made but for Smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks. Now reproduce the time series plot we previously made, but this time following the instructions of the previous question. For the state of California, make a time series plots showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease. Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population. Probability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. Knowledge of probability is therefore indispensable for data science. In games of chance, probability has a very intuitive definition. For instance, we know what it means that the chance of a pair of dice coming up seven is 1 in 6. However, this is not the case in other contexts. Today probability is being used much more broadly with the word probability commonly used in everyday language. Google’s auto-complete of “What are the chances of” give us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. The word probability is also used by election forecasters. In 2008, Nate Silver gave Obama a 94% chance of winning. In 2012 it was a 90% chance. Obama won both elections. In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. She lost. But 71% is still more than 50%. Was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere? To answer these questions, we will need to learn, among other things, some probability theory. We cover election forecasting in the next chapter since we also need to understand the concepts of statistical inference, which we explain in that chapter. But keep in mind that statistical inference builds upon probability theory. The motivation for this chapter are the circumstances surrounding the financial crisis of 2007-2008. This financial crisis was in part caused by underestimating the risk of certain securities sold by financial institutions. Specifically, the risk of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These MBS and CDO were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely. To begin to comprehend this very complicated event, we need to understand the basics of probability. We will introduce important concepts such as random variables, independence, Monte Carlo simulations, expected values, standard errors, and the Central Limit Theorem. Before using probability concepts to understand our motivating example, we will use several examples related to games of chance since these are simple and illustrative. "],
["discrete-probability.html", "Chapter 23 Discrete probability 23.1 Relative frequency 23.2 Notation 23.3 Monte Carlo simulations 23.4 Setting the random seed 23.5 Probability distributions 23.6 Independence 23.7 Conditional probabilities 23.8 Multiplication rule 23.9 Combinations and permutations 23.10 Birthday problem 23.11 How many Monte Carlo experiments are enough 23.12 Addition rule 23.13 Monty Hall problem Exercises", " Chapter 23 Discrete probability We start by covering some basic principles related to categorical data. The subset of probability is referred to as discrete probability. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples. 23.1 Relative frequency The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions. For example, if I have 2 red beads and 3 blue beads inside an urn and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue. A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment over and over, independently, and under the same conditions. 23.2 Notation We use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”. In data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\). We will see more of these examples later. Here, we focus on categorical data. 23.3 Monte Carlo simulations Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random. An example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn: beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) beads #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; and then use sample to pick a bead at random: sample(beads, 1) #&gt; [1] &quot;blue&quot; This line of code produces one random outcome. We want to repeat this experiment “over and over”, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent. This is an example of a Monte Carlo simulation. Much of what mathematical and theoretical statisticians study, which we do not cover in this book, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this section, we provide a practical approach to deciding what is “large enough”. To perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event \\(B =\\) 10,000 times: B &lt;- 10000 events &lt;- replicate(B, sample(beads, 1)) We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution: tab &lt;- table(events) tab #&gt; events #&gt; blue red #&gt; 5954 4046 and prop.table gives us the proportions: prop.table(tab) #&gt; events #&gt; blue red #&gt; 0.595 0.405 The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that \\(B\\) gets larger as the estimates get closer to 3/5=.6 and 2/5=.4. Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R. 23.4 Setting the random seed Before we continue, we will briefly explain the following important line of code: set.seed(1) Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the book may show a different result than what you obtain when you try to code as shown in the book. This is actually fine since the results are random and change from time to time. However, if you want to to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1. ?set.seed In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be. With and without replacement The function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads: sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error: sample(beads, 6) Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE' However, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this changing the replace argument, which defaults to FALSE, to replace=TRUE: events &lt;- sample(beads, B, replace = TRUE) prop.table(table(events)) #&gt; events #&gt; blue red #&gt; 0.597 0.403 Not surprisingly, we get results very similar to those previously obtained with replicate. 23.5 Probability distributions Defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution. If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided and 2% Green Party, these proportions define the probability for each group. The probability distribution is: \\[ \\mbox{Pr}(\\mbox{picking a Republican})=0.44\\\\ \\mbox{Pr}(\\mbox{picking a Democrat})=0.44\\\\ \\mbox{Pr}(\\mbox{picking an undecided})=0.10\\\\ \\mbox{Pr}(\\mbox{picking a Green})=0.02\\\\ \\] 23.6 Independence We say two events are independent if the outcome of one does not affect the other. The classic example are coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws. Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent. The first outcome affected the next one. To see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement: x &lt;- sample(beads, 5) If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes: x[2:5] #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; would you still guess blue? Of course not. Now you know that the probability of red is 1. The events are not independent so the probabilities changes. 23.7 Conditional probabilities When events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation: \\[ \\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51 \\] We use the \\(\\mid\\) as shorthand for “given that” or “conditional on”. When two events, say \\(A\\) and \\(B\\), are independent, we have: \\[ \\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A) \\] This is the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence. 23.8 Multiplication rule If we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule: \\[ \\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A) \\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose). So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card given that the first was an Ace: \\(1/13 \\times 12/52 \\approx 0.02\\) The multiplicative rule also applies to more than two events. We can use induction to expand for more events: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B) \\] Multiplication rule under indepedence When we have independent events, then the multiplication rule becomes simpler: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C) \\] But we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence. As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both. But to multiply like this we need to assume independence! The conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: 0.09. The multiplication rule also gives us a general formula for computing conditional probabilities: \\[ \\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games. 23.9 Combinations and permutations In our very first example we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue. For more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in Poker? In a Discrete Probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers. First let’s construct a deck of cards. For this, we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this: number &lt;- &quot;Three&quot; suit &lt;- &quot;Hearts&quot; paste(number, suit) #&gt; [1] &quot;Three Hearts&quot; paste also works on pairs of vectors performing the operation element-wise: paste(letters[1:5], as.character(1:5)) #&gt; [1] &quot;a 1&quot; &quot;b 2&quot; &quot;c 3&quot; &quot;d 4&quot; &quot;e 5&quot; The function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey and plaid shirts, all your combinations are: expand.grid(pants = c(&quot;blue&quot;, &quot;black&quot;), shirt = c(&quot;white&quot;, &quot;grey&quot;, &quot;plaid&quot;)) #&gt; pants shirt #&gt; 1 blue white #&gt; 2 black white #&gt; 3 blue grey #&gt; 4 black grey #&gt; 5 blue plaid #&gt; 6 black plaid So here is how we generate a deck of cards: suits &lt;- c(&quot;Diamonds&quot;, &quot;Clubs&quot;, &quot;Hearts&quot;, &quot;Spades&quot;) numbers &lt;- c(&quot;Ace&quot;, &quot;Deuce&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) deck &lt;- expand.grid(number=numbers, suit=suits) deck &lt;- paste(deck$number, deck$suit) With the deck constructed, we can now double check that the probability of a King in the first card is 1/13. We simply compute the proportion of possible outcomes that satisfy our condition: kings &lt;- paste(&quot;King&quot;, suits) mean(deck %in% kings) #&gt; [1] 0.0769 which is 1/13. Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes. To do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different combinations we can get when we select r items. So here are all the ways we can choose two numbers from a list consisting of 1,2,3: library(gtools) permutations(3, 2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2) and (3,3) do not appear because once we pick a number, it can’t appear again. Optionally, we can add a vector. So if you want to see five random seven digit phone numbers out of all possible phone numbers, you can type: all_phone_numbers &lt;- permutations(10, 7, v = 0:9) n &lt;- nrow(all_phone_numbers) index &lt;- sample(n, 5) all_phone_numbers[index,] Instead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9. To compute all possible ways we can choose two cards when the order matters, we type: hands &lt;- permutations(52, 2, v = deck) This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this: first_card &lt;- hands[,1] second_card &lt;- hands[,2] Now the cases for which the first hand was a King can be computed like this: kings &lt;- paste(&quot;King&quot;, suits) sum(first_card %in% kings) #&gt; [1] 204 To get the conditional probability, we compute what fraction of these have a King in the second card: sum(first_card %in% kings &amp; second_card %in% kings) / sum(first_card %in% kings) #&gt; [1] 0.0588 which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to: mean(first_card %in% kings &amp; second_card %in% kings) / mean(first_card %in% kings) #&gt; [1] 0.0588 which uses mean instead of sum and is an R version of: \\[ \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter. Below are the differences: permutations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 combinations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 3 In the second line, the outcome does not include (2,1) because the (1,2) already was enumerated. The same applies to (3,1) and (3,2). So to compute the probability of a Natural 21 in Blackjack, we can do this: aces &lt;- paste(&quot;Ace&quot;, suits) facecard &lt;- c(&quot;King&quot;, &quot;Queen&quot;, &quot;Jack&quot;, &quot;Ten&quot;) facecard &lt;- expand.grid( number=facecard, suit=suits) facecard &lt;- paste( facecard$number, facecard$suit) hands &lt;- combinations(52, 2, v = deck) mean(hands[,1] %in% aces &amp; hands[,2] %in% facecard) #&gt; [1] 0.0483 In the last line, we assume the Ace comes first. This is only because we know the way combination generates enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer: mean((hands[,1] %in% aces &amp; hands[,2] %in% facecard) | (hands[,2] %in% aces &amp; hands[,1] %in% facecard)) #&gt; [1] 0.0483 Monte Carlo example Instead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements: hand &lt;- sample(deck, 2) hand #&gt; [1] &quot;Eight Spades&quot; &quot;Ten Spades&quot; And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities: (hands[1] %in% aces &amp; hands[2] %in% facecard) | (hands[2] %in% aces &amp; hands[1] %in% facecard) #&gt; [1] FALSE If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21. Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment. blackjack &lt;- function(){ hand &lt;- sample(deck, 2) (hand[1] %in% aces &amp; hand[2] %in% facecard) | (hand[2] %in% aces &amp; hand[1] %in% facecard) } Here we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise: blackjack() #&gt; [1] FALSE Now we can play this game, say, 10,000 times: B &lt;- 10000 results &lt;- replicate(B, blackjack()) mean(results) #&gt; [1] 0.0488 23.10 Birthday problem Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much. First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this: n &lt;- 50 bdays &lt;- sample(1:365, n, replace = TRUE) To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated which returns TRUE whenever an element of a vector is a duplicate. Here is an example: duplicated(c(1,2,3,1,4,3,5)) #&gt; [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE The second time 1 and 3 appear, we get a TRUE. So to check if two birthdays were the same, we simply use the any and duplicated functions like this: any(duplicated(bdays)) #&gt; [1] TRUE In this case, we see that it did happen. At least two people had the same birthday. To estimate the probability, we repeat this experiment by sampling 50 birthdays, over and over: same_birthday &lt;- function(n){ bdays &lt;- sample(1:365, n, replace=TRUE) any(duplicated(bdays)) } B &lt;- 10000 results &lt;- replicate(B, same_birthday(50)) mean(results) #&gt; [1] 0.97 Where you expecting the probability to be this high? People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group is close to 365. At this stage, we run out of days and the probability is one. Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%? Let’s create a look-up table. We can quickly create a function to compute this for any group size: compute_prob &lt;- function(n, B=10000){ results &lt;- replicate(B, same_birthday(n)) mean(results) } Using the function sapply, we can perform element-wise operations on any function: n &lt;- seq(1,60) prob &lt;- sapply(n, compute_prob) We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\): prob &lt;- sapply(n, compute_prob) qplot(n, prob) Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments. To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule. Let’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is: \\[ 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365} \\] We can write a function that does this for any number: exact_prob &lt;- function(n){ prob_unique &lt;- seq(365,365-n+1)/365 1 - prod( prob_unique) } eprob &lt;- sapply(n, exact_prob) qplot(n, prob) + geom_line(aes(n, eprob), col = &quot;red&quot;) This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities. 23.11 How many Monte Carlo experiments are enough In the examples above, we used $B=$10,000 Monte Carlo experiments. It turns out that this provided very accurate estimates. But in more complex calculations, 10,000 may not nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training. One practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 22 people. B &lt;- 10^seq(1, 5, len = 100) compute_prob &lt;- function(B, n=25){ same_day &lt;- replicate(B, same_birthday(n)) mean(same_day) } prob &lt;- sapply(B, compute_prob) qplot(log10(B), prob, geom = &quot;line&quot;) In this plot, we can see that the values start to stabilize; that is, they vary less than .01, around 1000. Note that the exact probability, which we know in this case, is 0.569. 23.12 Addition rule Another way to compute the probability of a Natural 21 is to notice that it is the probability of an Ace followed by a face card or a face card followed by an Ace. Here we use the addition rule: \\[ \\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B) \\] This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice. In the case of a Natural 21, the intersection is empty since both hands can’t happen simultaneously. The probability of an Ace followed by a face card is \\(1/13 \\times 16/51\\) and the probability of a face card followed by an Ace is \\(16/52 \\times 4/51\\). These two are actually the same, which makes sense due to symmetry. In any case, we get the same result using the addition rule: 1/13*16/51 + 16/52*4/51 + 0 #&gt; [1] 0.0483 23.13 Monty Hall problem In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. If the contestant did not pick the prize door on his or her first try, Monty Hall would open one of the two remaining doors and show the contestant there was no prize. Then he would ask “Do you want to switch doors?” What would you do? We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed explanation here or read one here. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes. Let’s start with the stick strategy: B &lt;- 10000 stick &lt;- replicate(B, { doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;,&quot;goat&quot;,&quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1) stick &lt;- my_pick stick == prize_door }) mean(stick) #&gt; [1] 0.336 As we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation. From this we should realize that the chance is 1 in 3, what we began with. Now let’s repeat the exercise, but consider the switch strategy: switch &lt;- replicate(B, { doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;,&quot;goat&quot;,&quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)], 1) stick &lt;- my_pick switch &lt;- doors[!doors%in%c(my_pick, show)] switch == prize_door }) mean(switch) #&gt; [1] 0.672 The Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3. Exercises One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan? What is the probability that the ball will not be cyan? Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? Two events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent? A. You don’t replace the draw. B. You replace the draw. C. Neither D. Both Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow? If you roll a 6-sided die six times, what is the probability of not seeing a 6? Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game? Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: Use the following code to generate the results of the first four games: celtic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4)) The Celtics must win one of these 4 games. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series? Confirm the results of the previous question with a Monte Carlo simulation. Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation: prob_win &lt;- function(p){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=4 }) mean(result) } Use the function sapply to compute the probability, call it Pr, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result `plot(p, prob). Repeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N &lt;- seq(1, 25, 2). Hint: use this function: prob_win &lt;- function(N, p=0.75){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=(N+1)/2 }) mean(result) } "],
["continuous-probability.html", "Chapter 24 Continuous probability 24.1 Theoretical distribution 24.2 Approximations 24.3 The probability density 24.4 Monte Carlo simulations 24.5 Other continuous distributions Exercise", " Chapter 24 Continuous probability Earlier, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size \\(n\\) with extremely high precision, since no two people are exactly the same height, we need to assign the proportion \\(1/n\\) to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height. Just as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution functions (CDF). We previously described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector \\(x\\) to contain the male heights: library(tidyverse) library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% .$height We defined the empirical distribution function as: F &lt;- function(a) mean(x&lt;=a) which, for any value a, gives the proportion of values in the list x that are smaller or equal than a. Keep in mind that we have not yet introduced probability. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing: 1 - F(70) #&gt; [1] 0.377 Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is: F(b)-F(a) Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights x. 24.1 Theoretical distribution In the data visualization chapter, we introduced the normal distribution as a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average avg and standard deviation s, if its probability distribution is defined by: F(a) = rnorm(a, avg, s) This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire data set to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation: avg &lt;- mean(x) s &lt;- sd(x) 1 - pnorm(70.5, avg, s) #&gt; [1] 0.371 24.2 Approximations The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution: While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787 which is 177 centimeters. The probability assigned to this height is 0.001 or 1 in 812. The probability for 70 inches is much higher 0.106, but does it really make sense to think of the probability of being exactly 70 inches as being the same as 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch. With continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We, thus, could ask what is the probability that someone is between 69.5 and 70.5. In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. So, for example, the normal distribution is useful for approximating the proportion of students reporting between 69.5 and 70.5: mean(x &lt;= 68.5) - mean(x &lt;= 67.5) #&gt; [1] 0.115 mean(x &lt;= 69.5) - mean(x &lt;= 68.5) #&gt; [1] 0.119 mean(x &lt;= 70.5) - mean(x &lt;= 69.5) #&gt; [1] 0.122 Note how close we get with the normal approximation: pnorm(68.5, avg, s) - pnorm(67.5, avg, s) #&gt; [1] 0.103 pnorm(69.5, avg, s) - pnorm(68.5, avg, s) #&gt; [1] 0.11 pnorm(70.5, avg, s) - pnorm(69.5, avg, s) #&gt; [1] 0.108 However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks when we try to estimate: mean(x &lt;= 70.9) - mean(x&lt;=70.1) #&gt; [1] 0.0222 with pnorm(70.9, avg, s) - pnorm(70.1, avg, s) #&gt; [1] 0.0836 In general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool. 24.3 The probability density For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1,2,3,4,5 or 6. The probability of 4 is defined as: \\[ \\mbox{Pr}(X=4) = 1/6 \\] The CFD can then easily be defined: \\[ F(4) = \\mbox{Pr}(X\\leq 4) = \\mbox{Pr}(X = 4) + \\mbox{Pr}(X = 3) + \\mbox{Pr}(X = 2) + \\mbox{Pr}(X = 1) \\] Although for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that: \\[ F(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx \\] For those that know Calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know Calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability of \\(X\\leq a\\). For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use: 1 - pnorm(76, avg, s) #&gt; [1] 0.0321 which mathematically is the grey area below: The curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm. Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available. 24.4 Monte Carlo simulations R provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights: n &lt;- length(x) avg &lt;- mean(x) s &lt;- sd(x) simulated_heights &lt;- rnorm(n, avg, s) Not surprisingly, the distribution looks normal: This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations. If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer? The following Monte Carlo simulation helps us answer that question: B &lt;- 10000 tallest &lt;- replicate(B, { simulated_data &lt;- rnorm(800, avg, s) max(simulated_data) }) A seven footer is quite rare: mean(tallest &gt;= 7*12) #&gt; [1] 0.0193 Here is the resulting distribution: Note that it does not look normal. 24.5 Other continuous distributions The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, chi-squared, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm and rnorm for the normal distribution. The functions qnorm gives us the quantiles. We can therefore draw a distribution like this: x &lt;- seq(-4, 4, length.out = 100) data.frame(x, f = dnorm(x)) %&gt;% ggplot(aes(x, f)) + geom_line() For the student-t, the shorthand t is used so the functions are dt for the density, qt for the quantiles, pt for the cumulative distribution function, and rt for Monte Carlo simulation. Exercise Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter? Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller? Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random what is the probability that she is between 61 and 67 inches. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now? Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average. To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - \\mu)/\\sigma\\) standard deviations \\(\\sigma\\) away from the average \\(\\mu\\). The probability is: \\[ \\mbox{Pr}(X \\leq a) \\] Now we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\): \\[ \\mbox{Pr}\\left(\\frac{X-\\mu}{\\sigma} \\leq \\frac{a-\\mu}{\\sigma} \\right) \\] The quantity on the right is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\): \\[ \\mbox{Pr}\\left(Z \\leq \\frac{a-\\mu}{\\sigma} \\right) \\] So, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - \\mu)/\\sigma\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation: A. mean(X&lt;=a) B. pnorm((a - mu)/sigma) C. pnorm((a - mu)/sigma, mu, sigma) D. pnorm(a) Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm. The distribution of IQ scores is approximately normally distributed. The expected value is 100 and the standard deviation is 15. Suppose you want to know the distribution of the smartest person in your school district if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating one million IQ scores and keeping the highest. Make a histogram. "],
["random-variables.html", "Chapter 25 Random variables 25.1 Sampling models 25.2 The probability distribution of a random variable 25.3 Distributions versus probability distributions 25.4 Notation for random variables 25.5 Central Limit Theorem 25.6 The expected value and standard error 25.7 Central Limit Theorem approximation 25.8 Statistical properties of averages 25.9 Law of large numbers 25.10 How large is large in CLT? 25.11 Population SD versus the sample SD Exercises", " Chapter 25 Random variables Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1, if a bead is blue and red otherwise. beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) X &lt;- ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) Here X is a random variable: every time we select a new bead the outcome changes randomly. See below: ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 Sometimes it’s 1 and sometimes it’s 0. In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data scientist. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables. We start with games of chance. 25.1 Sampling models Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from a urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing that outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real world situations in which sampling models are used to answer specific questions. We will therefore start with such examples. Suppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels. We are going to define a random variable S that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn: color &lt;- rep(c(&quot;Black&quot;,&quot;Red&quot;,&quot;Green&quot;), c(18,18,2)) The 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -$1. Otherwise, the casino wins a dollar and we draw a $1. To construct our random variable S, we can use this code: n &lt;- 10000 X &lt;- sample(ifelse( color==&quot;Red&quot;, -1, 1), n, replace = TRUE) X[1:10] #&gt; [1] 1 -1 -1 -1 -1 1 1 1 -1 1 Because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) We call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings S is simply the sum of these 1,000 independent draws: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) S &lt;- sum(X) S #&gt; [1] 664 25.2 The probability distribution of a random variable If you run the code above, you see that \\(S\\) changes every time. This is, of course, because S is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that S is in the interval \\(S&lt;0\\). Note that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S&lt;0\\). We call this \\(F\\) the random variable’s distribution function. We can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically B=10,000 times: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) So now we can ask the following: in our simulations, how often did we get sums less than or equal to a? mean(S &lt;= a) This will be a very good approximation of \\(F(a)\\). In fact, we can visualize the distribution by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\): Now we can easily answer the casino’s question: how likely is it that we will lose money? mean(S&lt;0) #&gt; [1] 0.046 We can see it is quite low. In the histogram above, we see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to perfect. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these: mean(S) #&gt; [1] 52.5 sd(S) #&gt; [1] 31.7 If we add a normal density with this average and standard deviation to the histogram above, we see that it matches very well: This average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section. It turns out that statistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that \\((S+n)/2\\) follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of \\(S\\). We did this for illustrative purposes. We can use the function dbinom and pbinom to compute the probabilities exactly. For example, to know compute \\(\\mbox{Pr}(S &lt; 0)\\) we note that: \\[\\mbox{Pr}(S &lt; 0) = \\mbox{Pr}((S+n)/2 &lt; (0+n)/2)\\] and we can use the pbinom to compute \\[\\mbox{Pr}(S \\leq 0)\\] pbinom(n/2, size = n, prob = 9/19) #&gt; [1] 0.955 Because this is a discrete probability function, to get \\(\\mbox{Pr}(S &lt; 0)\\) rather than \\(\\mbox{Pr}(S \\leq 0)\\), we write: pbinom(n/2-1, size = n, prob = 9/19) #&gt; [1] 0.949 For the details of the binomial distribution, you can consult any basic probability book or even Wikipedia. Here we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT). 25.3 Distributions versus probability distributions Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization chapter, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that answers the question: what proportion of the list is less than or equal to \\(a\\)?. Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x: avg &lt;- sum(x)/length(x) s &lt;- sqrt(sum((x - avg)^2) / length(x)) A random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers. However, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable. Another way to think about it, that does not involve an urn, is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable. 25.4 Notation for random variables In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the value of \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened. 25.5 Central Limit Theorem The Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history. Previously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error. 25.6 The expected value and standard error We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work. The first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this: \\[\\mbox{E}[X]\\] to denote the expected value of the random variable \\(X\\). A random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take. Theoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. So in the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus: \\[ \\mbox{E}[X] = (20 + -18)/38 \\] which is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this: B &lt;- 10^6 X &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19)) mean(X) #&gt; [1] 0.0515 In general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is: \\[ap + b(1-p).\\] To see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s, \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\). Now the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is: \\[ \\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn} \\] So if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use: \\[\\mbox{SE}[X]\\] to denote the standard error of a random variable. If our draws are independent, then the standard error of the sum is given by the equation: \\[ \\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn} \\] Using the definition of standard deviation, we can derive, with a bit of math, that if a jar contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\) respectively, the standard deviation is: \\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\] So in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or: 2 * sqrt(90)/19 #&gt; [1] 0.999 The standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1. Using the formula above, the sum of 1,000 people playing has standard error of about $32: n &lt;- 1000 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 As a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help. Advanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT which can be generally applied to sums of random variables in a way that the binomial distribution can’t. 25.7 Central Limit Theorem approximation The Central Limit Theorem (CLT) tells us that the sum S is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are: n * (20-18)/38 #&gt; [1] 52.6 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 The theoretical values above match those obtained with the Monte Carlo simulation: mean(S) #&gt; [1] 52.5 sd(S) #&gt; [1] 31.7 Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation: mu &lt;- n * (20-18)/38 se &lt;- sqrt(n) * 2 * sqrt(90)/19 pnorm(0, mu, se) #&gt; [1] 0.0478 which is also in very good agreement with our Monte Carlo result: mean(S &lt; 0) #&gt; [1] 0.046 25.8 Statistical properties of averages There are two useful mathematical results that we used above and often employ when working with data. We list them below. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n] = \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n] \\] If the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n]= n\\mu \\] which is another way of writing the result we show above for the sum of draws. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols: \\[ \\mbox{E}[aX] = a\\times\\mbox{E}[X] \\] To see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again: \\[ \\mbox{E}[(X_1+X_2+\\dots+X_n) / n]= \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu \\] The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form: \\[ \\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2 } \\] The square of the standard error is referred to as the variance in statistical textbooks. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[ \\mbox{SE}[aX] = a \\times \\mbox{SE}[X] \\] To see why this is intuitive, again think of units. A consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\): \\[ \\begin{aligned} \\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &amp;= \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\ &amp;= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\ &amp;= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\ &amp;= \\sqrt{n\\sigma^2}/n\\\\ &amp;= \\sigma / \\sqrt{n} \\end{aligned} \\] If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\). Why we use \\(\\mu\\) and \\(\\sigma\\) Statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error. 25.9 Law of large numbers An important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages. 25.9.1 Misinterpreting law of averages The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses. Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row. 25.10 How large is large in CLT? The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need larger sample sizes. By way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate. You can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia. 25.11 Population SD versus the sample SD The standard deviation of a list x (we use heights as an example) is defined as the square root of the average of the squared differences: library(dslabs) x &lt;- heights$height m &lt;- mean(x) s &lt;- sqrt(mean((x-m)^2)) Using mathematical notation we write: \\[ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\] However, be aware that the sd function returns a slightly different result: identical(s,sd(x)) #&gt; [1] FALSE s-sd(x) #&gt; [1] -0.00194 This is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide by the \\(N-1\\). \\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i \\\\ s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2} \\] You can see that this is the case by typing: N &lt;- length(x) s-sd(x)*sqrt((N-1)/N) #&gt; [1] 0 For all the theory discussed here, you need to compute the actual standard deviation as defined: sqrt(mean((x-m)^2)) So be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N}\\) is close to 1. Exercises In American Roulette you can also bet on green. There are 18 reds, 18 blacks and 2 greens (0 and 00). What are the chances the green comes out? The payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings. Hint: see the example below for how it should look like when betting on red. X &lt;- sample(c(1,-1), 1, prob = c(9/19, 10/19)) Compute the expected value of \\(X\\). Compute the standard error of \\(X\\). Now create a random variable \\(S\\) that is the sum of your winnings after betting on green 1000 times. Hint: change the argument size and replace in your answer to question 2. Start your code by setting the seed to 1 with set.seed(1). What is the expected value of \\(S\\)? What is the standard error of \\(S\\)? What is the probability that you end up winning money? Hint: use the CLT. Create a Monte Carlo simulation that generates 10,000 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1). Now check your answer to 8 using the Monte Carlo result. The Monte Carlo result and the CLT approximation are close, but not that close. What could account for this? A. 10,000 simulations is not enough. If we do more, they match. B. The CLT does not work as well when the probability of success is small. In this case, it was 1/19. If we make the number of roulette plays bigger, they will match better. C. The difference is within rounding error. D. The CLT only works for the averages. Now create a random variable \\(Y\\) that is your average winnings per bet after playing off your winnings after betting on green 10,000 times. What is the expected value of \\(Y\\)? What is the standard error of \\(S\\)? What is the probability that you end up winnings per game that are positive? Hint: use the CLT. Create a Monte Carlo simulation that generates 2,500 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1) Now check your answer to 8 using the Monte Carlo result. The Monte Carlo result and the CLT approximation are now much closer. What could account for this? A. We are now computing averages instead of sums. B. 2,500 Monte Carlo simulations is not better than 10,000. C. The CLT does works better when the sample size is larger. We increased from 1,000 to 10,000. D. It is not closer. The difference is within rounding error. "],
["case-study-the-big-short.html", "Chapter 26 Case study: The Big Short 26.1 Interest rates explained with chance model 26.2 The Big Short Exercises", " Chapter 26 Case study: The Big Short 26.1 Interest rates explained with chance model More complex versions of the sampling models we have discussed are also used by banks to decide interest rates. Suppose you run a small bank that has a history of identifying potential homeowners that can be trusted to make payments. In fact, historically, in a given year, only 2% of your customers default, meaning that they don’t pay back the money that you lent them. However, you are aware that if you simply loan money to everybody without interest, you will end up losing money due to this 2%. Although you know 2% of your clients will probably default, you don’t know which ones. Yet by charging everybody just a bit extra in interest, you can make up the losses incurred due to that 2% and also cover your operating costs. You can also make a profit, but if you set the interest rates too high, your clients will go to another bank. We use all these facts and some probability theory to decide what interest rate you should charge. Suppose your bank will give out 1,000 loans for $180,000 this year. Also, after adding up all costs, suppose your bank loses $200,000 per foreclosure. For simplicity, we assume this includes all operational costs. A sampling model for this scenario can be coded like this: n &lt;- 1000 loss_per_foreclosure &lt;- -200000 p &lt;- 0.02 defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) #&gt; [1] -3e+06 Note that the total loss defined by the final sum is a random variable. Every time you run the above code, you get a different answer. We can easily construct a Monte Carlo simulation to get an idea of the distribution of this random variable. B &lt;- 10000 losses &lt;- replicate(B, { defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) }) Here is the distribution of this random variable: We don’t really need a Monte Carlo simulation though. Using what we have learned, the CLT tells us that because our losses are a sum of independent draws, its distribution is approximately normal with expected value and standard errors given by: n*(p*loss_per_foreclosure + (1-p)*0) #&gt; [1] -4e+06 sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p)) #&gt; [1] 885438 We can now set an interest rate to guarantee that, on average, we break even. Basically, we need to add a quantity \\(x\\) to each loan, which in this case are represented by draws, so that the expected value is 0. If we define \\(l\\) to be the loss per foreclosure, we need: \\[ lp + x(1-p) = 0 \\] which implies \\(x\\) is - loss_per_foreclosure*p/(1-p) #&gt; [1] 4082 or an interest rate of 0.023. However, we still have a problem. Although this interest rate guarantees that on average we break even, there is a 50% chance that we lose money. If our bank loses money, we have to close it down. We therefore need to pick an interest rate that makes it unlikely for this to happen. At the same time, if the interest rate is too high, our clients will go to another bank so we must be willing to take some risks. So let’s say that we want our chances of losing money to be 1 in 100, what does the \\(x\\) quantity need to be now? This one is a bit harder. We want the sum \\(S\\) to have: \\[\\mbox{Pr}(S&lt;0) = 0.01\\] We know that \\(S\\) is approximately normal. The expected value of \\(S\\) is\\(\\{ lp + x(1-p)\\}n\\) with \\(n\\) the number of draws, which in this case represents loans. The standard error is \\(|x-l| \\sqrt{np(1-p)}\\). Because \\(x\\) is positive and \\(l\\) negative \\(|x-l|=x-l\\). Note that these are just the formulas from above, but using more compact symbols. Now we are going to use a mathematical “trick” that is very common in statistics. We are going add and subtract the same quantities to both sides of the event \\(S&lt;0\\) so that the probability does not change and we end up with a standard normal random variable on the left, which will then permit us to write down an equation with only \\(x\\) as an unknown. This “trick” is as follows: If \\[\\mbox{Pr}(S&lt;0) = 0.01\\] then \\[ \\mbox{Pr}\\left(\\frac{S - \\mbox{E}[S]}{\\mbox{SE}[S]} &lt; \\frac{ - \\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] And remember \\(\\mbox{E}[S]\\) and \\(\\mbox{SE}[S]\\) are the expected value and standard error of \\(S\\) respectively. All we did above was add and divide by the same quantity on both sides. We did this because now the term on the left is a standard normal random variable, which we will rename \\(Z\\). Now we fill in the blanks with the actual formula for expected value and standard error: \\[ \\mbox{Pr}\\left(Z &lt; \\frac{- \\{ lp + x(1-p)\\}n}{(x-l) \\sqrt{np(1-p)}}\\right) = 0.01 \\] It may look complicated, but remember that \\(l\\), \\(p\\) and \\(n\\) are all known amounts so eventually we will turn them into numbers. Now because the term on the left side is a normal random with expected value 0 and standard error 1, it means that the quantity on the left must be equal to: qnorm(0.01) #&gt; [1] -2.33 for the equation to hold true. Remember that \\(z=\\)qnorm(0.01) gives us the value of \\(z\\) for which: \\[ \\mbox{Pr}(Z \\leq z) = 0.01 \\] So this means that right side of the complicated equation must be \\(z\\)=qnorm(0.01). \\[ \\frac{- \\{ lp + x(1-p)\\}N}{(x-l) \\sqrt{Np(1-p)}} = z \\] The trick works because we end up with an expression containing \\(x\\) that we know has to be equal to a known quantity \\(z\\). Solving for \\(x\\) is now simply algebra: \\[ x = - l \\frac{ np - z \\sqrt{np(1-p)}}{N(1-p) + z \\sqrt{np(1-p)}}\\] which is: l &lt;- loss_per_foreclosure z &lt;- qnorm(0.01) x &lt;- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p))) x #&gt; [1] 6249 Our interest rate now goes up to 0.035. This is still a very competitive interest rate. By choosing this interest rate, we now have an expected profit per loan of: loss_per_foreclosure*p + x*(1-p) #&gt; [1] 2124 which is a total expected profit of about: n*(loss_per_foreclosure*p + x*(1-p)) #&gt; [1] 2124198 dollars! We can run a Monte Carlo simulation to double check our theoretical approximations: B &lt;- 100000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 2122436 mean(profit&lt;0) #&gt; [1] 0.0128 26.2 The Big Short One of your employees points out that since the bank is making 2124 dollars per loan, the bank should give out more loans! Why just n? You explain that finding those n clients was hard. You need a group that is predictable and that keeps the chances of defaults low. He then points out that even if the probability of default is higher, as long as our expected value is positive, you can minimize your chances of losses by increasing \\(n\\) and relying on the law of large numbers. He claims that even if the default rate is twice as high, say 4%, if we set the rate just a bit higher than: p &lt;- 0.04 r &lt;- (- loss_per_foreclosure*p/(1-p)) / 180000 r #&gt; [1] 0.0463 At 5%, we are guaranteed a positive expected value of: r &lt;- 0.05 x &lt;- r*180000 loss_per_foreclosure*p + x * (1-p) #&gt; [1] 640 and can minimize our chances of losing money by simply increasing \\(n\\) since: \\[ \\mbox{Pr}(S &lt; 0) = \\mbox{Pr}\\left(Z &lt; - \\frac{\\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] with \\(Z\\) a standard normal random variable as above. If we define \\(\\mu\\) and \\(\\sigma\\) to be the expected value and standard deviation of the urn respectively (that is of a single loan), using the formulas above we have: \\(\\mbox{E}[S]= n\\mu\\) and \\(\\mbox{SE}[S]= \\sqrt{n}\\sigma\\). So if we define \\(z\\)=qnorm(0.01), we have: \\[ - \\frac{n\\mu}{\\sqrt{n}\\sigma} = - \\frac{\\sqrt{n}\\mu}{\\sigma} = z \\] which implies that if we let: \\[ n \\geq z^2 \\sigma^2 / \\mu^2 \\] we are guaranteed to have a probability of less than 0.01. The implication is that, as long as \\(\\mu\\) is positive, we can find an \\(n\\) that minimizes the probability of a loss. This is a form of the law of large numbers: when \\(n\\) is large, our average earnings per loan converges to the expected earning \\(\\mu\\). With \\(x\\) fixed, now we can ask what \\(n\\) do we need for the probability to be 0.01? In our example, if we give out: z &lt;- qnorm(0.01) n &lt;- ceiling((z^2*(x-l)^2*p*(1-p))/(l*p + x*(1-p))^2) n #&gt; [1] 22163 loans, the probability of losing is about 0.01 and we are expected to earn a total of: n*(loss_per_foreclosure*p + x * (1-p)) #&gt; [1] 14184320 dollars! We can confirm this with a Monte Carlo simulation: p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit&lt;0) #&gt; [1] 0.0107 This seems like a no brainier. As a result, your colleague decides to leave your bank and start his own high risk mortgage company. A few months later, your colleague’s bank has gone bankrupt. A book is written and eventually a movie is made relating the mistake your friend, and many others, made. What happened? Your colleague’s scheme was mainly based on this mathematical formula: \\[ \\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] = \\sigma / \\sqrt{n} \\] By making \\(n\\) large, we minimize the standard error of our per-loan profit. However, for this rule to hold, the \\(X\\)s must be independent draws: one person defaulting must be independent of others defaulting. Note that in the case of averaging the same event over and over, an extreme example of events that are not independent, we get a standard error that is \\(\\sqrt{n}\\) times bigger: \\[ \\mbox{SE}[(X_1+X_1+\\dots+X_1) / n] = \\mbox{SE}[n X_1 / n] = \\sigma &gt; \\sigma / \\sqrt{n} \\] To construct a more realistic simulation than the original one your colleague ran, let’s assume there is a global event that affects everybody with high risk mortgages and changes their probability. We will assume that with 50-50 chance, all the probabilities go up or down slightly to somewhere between 0.03 and 0.05. But it happens to everybody at once, not just one person. These draws are no longer independent. p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { new_p &lt;- 0.04 + sample(seq(-0.01, 0.01, length = 100), 1) draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-new_p, new_p), replace = TRUE) sum(draws) }) Note that our expected profit is still large: mean(profit) #&gt; [1] 14188864 However, the probability of the bank having negative earning shoots up to: mean(profit&lt;0) #&gt; [1] 0.346 Even scarier is that the probability of losing more than 10 million dollars is: mean(profit &lt; -10000000) #&gt; [1] 0.238 To understand how this happens look at the distribution: data.frame(profit_in_millions=profit/10^6) %&gt;% ggplot(aes(profit_in_millions)) + geom_histogram(color=&quot;black&quot;, binwidth = 5) The theory completely breaks down and the random variable has much more variability than expected. The financial meltdown of 2007 was due, among other things, to financial “experts” assuming independence when there was none. Exercises Create a random variable \\(S\\) with the earnings of your bank if you give out 10,000 loans, the default rate is 0.3, and you lose $200,000 in each foreclosure. Hint: use the code we showed in the previous section, but change the parameters. Run a Monte Carlo simulation with 10,000 outcomes for \\(S\\). Make a histogram of the results. What is the expected value of \\(S\\)? What is the standard error of \\(S\\)? Suppose we give out loans for $180,000. What should the interest rate be so that our expected value is 0? (Harder) What should the interest rate be so that the chance of losing money is 1 in 20? In math notation, what should the interest rate be so that \\(\\mbox{Pr}(S&lt;0) = 0.05\\) ? If the bank wants to minimize the probabilities of losing money, which of the following does not make interest rates go up? A. A smaller pool of loans. B. A larger probability of default. C. A smaller required probability of losing money. D. The number of Monte Carlo simulations. Anybody that thinks that this race is anything but a tossup right now is such an ideologue … they’re jokes. To which Nate Silver responded via Twitter: If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal? How was Mr. Silver so confident? We will demonstrate how poll aggregators, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. The two main statistical tools used by the aggregators are the topic of this chapter: inference and modeling. To begin to understand how election forecasting works, we need to understand the basic data point they use: poll results. Opinion polling has been conducted since the 19th century. The general goal of these is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter. "],
["polls.html", "Chapter 27 Polls 27.1 Opinion polls 27.2 The sampling model for polls", " Chapter 27 Polls 27.1 Opinion polls Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know which geographical locations to focus their ‘get out the vote’ efforts. Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results. Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and, therefore, are often made public. We will eventually be looking at such data. Real Clear Politics is an example of a news aggregator that organizes and publishes poll results. For example, here are examples of polls reporting estimates of the popular vote for the 2016 presidential election: (Source: Real Clear Politics) Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC. Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error. In this section, we will show how the probability concepts we learned in the previous chapter can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modelling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election. We start by connecting probability theory to the task of using polls to learn about a population. 27.2 The sampling model for polls To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. We will use an urn instead of voters and, rather than competing with other pollsters for media attention, we will have a competition with a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar): Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it cost you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay me $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner. The dslabs package includes a function that shows a random draw from this urn: library(tidyverse) library(dslabs) ds_theme_set() take_poll(25) Think about how you would construct your interval based on the data shown above. We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors; that is, that there are just two parties. "],
["populations-samples-parameters-and-estimates.html", "Chapter 28 Populations, samples, parameters and estimates 28.1 The sample average 28.2 Parameters 28.3 Polling versus forecasting 28.4 Properties of our estimate: expected value and standard error Exercises", " Chapter 28 Populations, samples, parameters and estimates We want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\). In statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample. Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue? We want to construct an estimate of \\(p\\) using only the information we observe. An estimate can be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times: we get a different answer each time since the sample proportion is a random variable. Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better. 28.1 The sample average Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\). We start by defining the random variable \\(X=1\\) if we pick a blue bead at random and \\(0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because if we know the distribution of the sum \\(N \\bar{X}\\), we know the distribution of the average \\(\\bar{X}\\) because \\(N\\) is a non-random constant. For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads. Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\). 28.2 Parameters Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter. The ideas presented here on how we estimate parameters and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample. 28.3 Polling versus forecasting Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in our a later section. 28.4 Properties of our estimate: expected value and standard error To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply. Using what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation: \\[ \\mbox{E}(\\bar{X}) = p \\] We can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error for the average: \\[ \\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N} \\] This result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\). If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small? One problem is that we do not know \\(p\\), so we can’t compute the standard error. For illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\): From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is: sqrt(p*(1-p))/sqrt(1000) #&gt; [1] 0.0158 or 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\). Exercises Suppose you poll a population in which a proportion \\(p\\) of voters are Democrats and \\(1-p\\) are Republicans. Your sample size is \\(N=25\\). Consider the random variable \\(S\\) which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of \\(p\\). What is the standard error of \\(S\\) ? Hint: it’s a function of \\(p\\). Consider the random variable \\(S/N\\). This is equivalent to the sample average, which we have been denoting as \\(\\bar{X}\\). What is the expected value of the \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\). What is the standard error of \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\). Write a line of code that gives you the standard error se for the problem above for p &lt;- seq(0, 1, length = 100). Make a plot of se versus p. Copy the code above and put it inside a for-loop to make the plot for \\(N=25\\), \\(N=100\\) and \\(N=1000\\). If we are interested in the difference in proportions, \\(p - (1-p)\\), our estimate is \\(d = \\bar{X} - (1-\\bar{X})\\). Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of \\(d\\). What is the standard error of \\(d\\)? If the actual \\(p=.45\\), it means the Republicans are winning by a relatively large margin since \\(d= -.1\\), which is a 10% margin of victory. What is the standard error of \\(2\\hat{X}-1\\) in this case? Given the answer to 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)? A. The expected value of our estimate \\(2\\bar{X}-1\\) is \\(d\\), so our prediction will be right on. B. Our standard error is larger than the difference, so the chances of \\(2\\bar{X}-1\\) being positive and throwing us off were not that small. We should pick a larger sample size. C. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference. D. Because we don’t know \\(p\\), we have no way knowing that making \\(N\\) larger would actually improve our standard error. "],
["central-limit-theorem-in-practice.html", "Chapter 29 Central Limit Theorem in practice 29.1 A Monte Carlo simulation 29.2 The spread 29.3 Bias: why not run a very large poll? Exercises", " Chapter 29 Central Limit Theorem in practice The CLT tells us that the distribution function for a sum of draws is approximately normal. We also learned that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal. In summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). Now how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking if: \\[ \\mbox{Pr}(| \\bar{X} - p| \\leq .01) \\] which is the same as: \\[ \\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01) \\] Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get: \\[ \\mbox{Pr}\\left(Z \\leq \\,.01 / \\mbox{SE}(\\bar{X}) \\right) - \\mbox{Pr}\\left(Z \\leq - \\,.01 / \\mbox{SE}(\\bar{X}) \\right) \\] One problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore: \\[ \\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N} \\] In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\). Now we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is: X_hat &lt;- 0.48 se &lt;- sqrt(X_hat*(1-X_hat)/25) se #&gt; [1] 0.0999 And now we can answer the question of the probability of being close to \\(p\\). The answer is: pnorm(0.01/se) - pnorm(-0.01/se) #&gt; [1] 0.0797 Therefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election. Earlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is: 2*se #&gt; [1] 0.2 Why do we multiply by 2? Because if you ask what is the probability that we are within two standard errors from \\(p\\), we get: \\[ \\mbox{Pr}\\left(Z \\leq \\, 2\\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) - \\mbox{Pr}\\left(Z \\leq - 2 \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) \\] which is: \\[ \\mbox{Pr}\\left(Z \\leq 2 \\right) - \\mbox{Pr}\\left(Z \\leq - 2\\right) \\] which we know is about 95%: pnorm(2)-pnorm(-2) #&gt; [1] 0.954 Hence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(2\\times \\hat{SE}(\\bar{X})\\), in our case 0, to \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. In summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes. From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.011. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition. 29.1 A Monte Carlo simulation Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this: B &lt;- 10000 N &lt;- 1000 Xhat &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) mean(X) }) The problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results. One thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll: p &lt;- 0.45 N &lt;- 1000 X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) Xhat &lt;- mean(X) In this particular sample, our estimate is Xhat. We can use that code to do a Monte Carlo simulation: B &lt;- 10000 Xhat &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) mean(X) }) To review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.016. The simulation confirms this: mean(Xhat) #&gt; [1] 0.45 sd(Xhat) #&gt; [1] 0.0158 A histogram and qq-plot confirm that the normal approximation is accurate as well: Of course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N. 29.2 The spread The competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread if \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error. For our 25 sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\). 29.3 Bias: why not run a very large poll? For realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%. Here are the calculations: N &lt;- 100000 p &lt;- seq(0.35, 0.65, length = 100) SE &lt;- sapply(p, function(x) 2*sqrt(x*(1-x)/N)) data.frame(p=p, SE = SE) %&gt;% ggplot(aes(p, SE)) + geom_line() One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter. Exercises Write an urn model function that takes the proportion of Democrats \\(p\\) and the sample size \\(N\\) as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample. Now assume p &lt;- 0.45 and that your sample size is \\(N=100\\). Take a sample of \\(N = 100\\) observations, repeat that 10,000 times and save the vector of mean(X)- p into an object called errors. Hint: use the function you wrote for exercise 1 to write this in one line of code. Vector errors contains, for each simulated sample, the difference between the actual \\(p\\) and our estimate \\(\\bar{X}\\). We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions: mean(errors) hist(errors) The error \\(\\bar{X}-p\\) is a random variable. In practice, the error is not observed because we do not know \\(p\\). Here we observe it because we constructed the simulation. What we can do in practice is describe the size of the error. What is the average size of the error if we define the size by taking the absolute value \\(\\mid \\bar{X} - p \\mid\\) ? The standard error is related to the value we just computed. It is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so in that sense the typical error is 0. For mathematical reasons related to the central limit theorem, we actually use the standard deviation of errors rather than the average of the absolute values. As we have discussed, the standard error is the squared root of the average squared distance \\((\\bar{X} - p)^2\\). What is this standard deviation, defined as the squared root of the distance squared? The theory we just learned tells us what this standard deviation is going to be because it is the standard error of \\(\\bar{X}\\). Generate a sample and create an estimate of the standard error of \\(\\bar{X}\\). In practice, we don’t know \\(p\\), so we construct an estimate of the theoretical prediction based by plugging in \\(\\bar{X}\\) for \\(p\\). Compute this estimate. Set the seed at 1 with set.seed(1). Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict \\(p\\) with \\(\\hat{X}\\). Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for \\(p=0.5\\). Create a plot of the largest standard error for \\(N\\) ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%? A. 100 B. 500 C. 2,500 D. 4,000 For \\(N=100\\), the central limit theorem tells us that the distribution of \\(\\hat{X}\\) is: A. practically equal to \\(p\\). B. approximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). C. approximately normal with expected value \\(\\bar{X}\\) and standard error \\(\\sqrt{\\bar{X}(1-\\bar{X})/N}\\). D. not a random variable. Based on the answer from exercise 8, errors \\(\\bar{X} - p\\) are: A. practically equal to 0. B. approximately normal with expected value \\(0\\) and standard error \\(\\sqrt{p(1-p)/N}\\). C. approximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). D. not a random variable. To corroborate your answer to exercise 9, make a qq-plot of the errors you generated in exercise 2 to see if they follow a normal distribution. If \\(p=0.45\\) and \\(N=100\\) as in exercise 2, use the CLT to estimate the probability that \\(\\bar{X}&gt;0.5\\). You can assume you know \\(p=0.45\\) for this calculation. Assume you are in a practical situation and you don’t know \\(p\\). Take a sample of size \\(N=100\\) and obtain a sample average of \\(\\bar{X} = 0.51\\). What is the CLT approximation for the probability that your error is equal or larger than 0.01? "],
["confidence-intervals.html", "Chapter 30 Confidence intervals 30.1 A Monte Carlo simulation 30.2 The correct language 30.3 Power 30.4 p-values Exercises", " Chapter 30 Confidence intervals Confidence intervals are a very useful concept widely employed by data scientists. A version of these that are commonly seen come from the ggplot geometry geom_smooth. Here is an example using a temperature dataset available in R: We will later learn how the curve is formed, but for now consider the shaded area around the curve. This is created using the concept of confidence intervals. In our earlier competition, you were asked to give an interval. If the interval you submitted includes the \\(p\\), you get half the money you spent on your “poll” back and pass to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval \\([0,1]\\) is guaranteed to include \\(p\\). However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious. On the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between. We can use the statistical theory we have learned to compute the probability of any given interval including \\(p\\). If we are asked to create an interval with, say, a 95% chance of including \\(p\\), we can do that as well. These are called 95% confidence intervals. When a pollster reports an estimate and a margin of error, they are, in a way, reporting a 95% confidence interval. Let’s show how this works mathematically. We want to know the probability that the interval \\([\\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X}), \\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X})]\\) contains the true proportion \\(p\\). First, consider that the start and end of these intervals are random variables: every time we take a sample, they change. To illustrate this, run the Monte Carlo simulation above twice. We use the same parameters as above: p &lt;- 0.45 N &lt;- 1000 And notice that the interval here: X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) c(X_hat - 2*SE_hat, X_hat + 2*SE_hat) #&gt; [1] 0.418 0.480 is different from this one: X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) c(X_hat - 2*SE_hat, X_hat + 2*SE_hat) #&gt; [1] 0.421 0.483 Keep sampling and creating intervals and you will see the random variation. To determine the probability that the interval includes \\(p\\), we need to compute this: \\[ \\mbox{Pr}\\left(\\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X}) \\leq p \\leq \\bar{X} + 2\\hat{\\mbox{SE}}(\\bar{X})\\right) \\] By subtracting and dividing the same quantities in all parts of the equation, we get that the above is equivalent to: \\[ \\mbox{Pr}\\left(-2 \\leq \\frac{\\bar{X}- p}{\\hat{\\mbox{SE}}(\\bar{X})} \\leq 2\\right) \\] The term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with \\(Z\\), so we have: \\[ \\mbox{Pr}\\left(-2 \\leq Z \\leq 2\\right) \\] which we can quickly compute using : pnorm(2) - pnorm(-2) #&gt; [1] 0.954 proving that we have a 95% probability. If we want to have a larger probability, say 99%, we need to multiply by whatever z satisfies the following: \\[ \\mbox{Pr}\\left(-z \\leq Z \\leq z\\right) = 0.99 \\] Using: z &lt;- qnorm(0.995) z #&gt; [1] 2.58 will achieve this because by definition pnorm(qnorm(0.995) is 0.995 and by symmetry pnorm(1-qnorm(0.995)) is 1 - 0.995. As a consequence, we have that: pnorm(z)-pnorm(-z) #&gt; [1] 0.99 is 0.995 - 0.005 = 0.99. We can use this approach for any percentile \\(q\\): we use \\(1 - (1 - q)/2\\). Why this number? Because \\(1 - (1 - q)/2 + (1 - q)/2 = q\\). To get exactly 0.95 confidence interval, we actually use a slightly smaller number than 2: qnorm(0.975) #&gt; [1] 1.96 30.1 A Monte Carlo simulation We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time. B &lt;- 10000 inside &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat) }) mean(inside) #&gt; [1] 0.955 The following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate: 30.2 The correct language When using the theory we described above, it is important to remember that it is the intervals that are random, not \\(p\\). In the plot above, we can see the random intervals moving around and \\(p\\), represented with the vertical line, staying in the same place. The proportion of blue in the urn \\(p\\) is not. So the 95% relates to the probability that this random interval falls on top of \\(p\\). Saying the \\(p\\) has a 95% of being between this and that is technically an incorrect statement because \\(p\\) is not random. 30.3 Power Pollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread: N &lt;- 25 X_hat &lt;- 0.48 (2*X_hat - 1) + c(-2,2)*2*sqrt(X_hat*(1-X_hat)/sqrt(N)) #&gt; [1] -0.934 0.854 includes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”. A problem with our poll results is that given the sample size and the value of \\(p\\), we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0. This does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0. By increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread. 30.4 p-values p-values are ubiquitous in the scientific literature. They are related to confidence intervals so we introduce the concept here. Let’s consider the blue and red beads. Suppose that rather than wanting an estimate of the spread or the proportion of blue, I am interested only in the question: are there more blue beads or red beads? I want to know if the spread \\(2p-1 &gt; 0\\). Say we take a random sample of \\(N=100\\) and we observe \\(52\\) blue beads, which gives us \\(2\\bar{X}-1=0.04\\). This seems to be pointing to the existence of more blue than red beads since 0.04 is larger than 0. However, as data scientists we need to be skeptical. We know there is chance involved in this process and we could get a 52 even when the actual spread is 0. We call this a null hypothesis. The null hypothesis is the skeptic’s hypothesis: the spread is \\(2p-1=0\\). We have observed a random variable \\(2*\\bar{X}-1 = 0.04\\) and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? So we write: \\[\\mbox{Pr}(\\mid \\bar{X} - 0.5 \\mid &gt; 0.02 ) \\] assuming the \\(2p-1=0\\) or \\(p=0.5\\). Under the null hypothesis we know that: \\[ \\sqrt{N}\\frac{\\bar{X} - 0.5}{\\sqrt{0.5(1-0.5)}} \\] is standard normal. We therefore can compute the probability above, which is the p-value. \\[\\mbox{Pr}\\left(\\sqrt{N}\\frac{\\mid \\bar{X} - 0.5\\mid}{\\sqrt{0.5(1-0.5)}} &gt; \\sqrt{N} \\frac{0.02}{ \\sqrt{0.5(1-0.5)}}\\right)\\] N=100 z &lt;- sqrt(N)*0.02/0.5 1 - (pnorm(z) - pnorm(-z)) #&gt; [1] 0.689 In this case, there is actually a large chance of seeing 52 or larger under the null hypothesis. Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05. To learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values since it gives us an idea of the size of the estimate. The p-value simply reports a probability and says nothing about the significance of the finding in the context of the problem. Exercises For these exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package. library(dslabs) data(&quot;polls_us_election_2016&quot;) Specifically, we will use all the national polls that ended within one week before the election. library(tidyverse) polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-31&quot; &amp; state == &quot;U.S.&quot;) For the first poll, you can obtain the samples size and estimated Clinton percentage with: N &lt;- polls$samplesize[1] X_hat &lt;- polls$rawpoll_clinton[1]/100 Assume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\). Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, X_hat,lower, upper variables. Hint: define temporary columns X_hat and se_hat. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not. For the table you just created, what proportion of confidence intervals included \\(p\\)? If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)? A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates \\(d\\), which in this election was \\(0.482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(d = 2p - 1\\), define: polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-31&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100) and re-do exercise 1, but for the difference. Now repeat exercise 3, but for the difference. Now repeat exercise 4, but for the difference. Although the proportion of confidence intervals goes up substantially, it is still lower that 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual \\(d=0.021\\). Stratify by pollster. Re-do the plot that you made for exercise 9, but only for pollsters that took five or more polls. "],
["statistical-models.html", "Chapter 31 Statistical models 31.1 Poll aggregators 31.2 Poll data 31.3 Pollster bias 31.4 Data driven model Exercises", " Chapter 31 Statistical models “All models are wrong, but some are useful” -George E. P. Box 31.1 Poll aggregators As we described earlier, in the 2012 Nate Silver was giving a Obama a 90% chance of winning. How was Mr. Silver so confident? We will demonstrate that Mr. Silver saw what others did not by using a Monte Carlo simulation. We generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls: d &lt;- 0.039 Ns &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516) p &lt;- (d + 1)/2 confidence_intervals &lt;- sapply(Ns, function(N) { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) 2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat)-1 }) Let’s save the results from this simulation in a data frame: polls &lt;- data.frame(poll=1:ncol(confidence_intervals), t(confidence_intervals), sample_size=Ns) %&gt;% setNames(c(&quot;poll&quot;, &quot;estimate&quot;, &quot;low&quot;, &quot;high&quot;, &quot;sample_size&quot;)) Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney: Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing. Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction. Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with: sum(polls$sample_size) #&gt; [1] 11269 participants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way: d_hat &lt;- polls %&gt;% summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&gt;% .$avg Once we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.018. Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote. Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits. Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. (Source: New York Times) For example, the Princeton Election Consortium gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton. By understanding statistical models and how these forecasters use them, we will start to understand how this happened. Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her a 81.4% chance. (Source: FiveThirtyEight) We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. 31.2 Poll data We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of of the dslabs package: data(polls_us_election_2016) names(polls_us_election_2016) #&gt; [1] &quot;state&quot; &quot;startdate&quot; &quot;enddate&quot; #&gt; [4] &quot;pollster&quot; &quot;grade&quot; &quot;samplesize&quot; #&gt; [7] &quot;population&quot; &quot;rawpoll_clinton&quot; &quot;rawpoll_trump&quot; #&gt; [10] &quot;rawpoll_johnson&quot; &quot;rawpoll_mcmullin&quot; &quot;adjpoll_clinton&quot; #&gt; [13] &quot;adjpoll_trump&quot; &quot;adjpoll_johnson&quot; &quot;adjpoll_mcmullin&quot; The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those: polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) We add a spread estimate: polls &lt;- polls %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) For this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference). We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is: d_hat &lt;- polls %&gt;% summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&gt;% .$d_hat and the standard error is: p_hat &lt;- (d_hat+1)/2 moe &lt;- 1.96 * 2 * sqrt(p_hat*(1-p_hat)/sum(polls$samplesize)) moe #&gt; [1] 0.00662 So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened? A histogram of the reported spreads shows a problem: polls %&gt;% ggplot(aes(spread)) + geom_histogram(color=&quot;black&quot;, binwidth = .01) The data does not appear to be normally distributed and the standard error appears to be larger than 0.007. The theory is not quite working here. 31.3 Pollster bias Notice that various pollsters are involved and some are taking several polls a week: polls %&gt;% group_by(pollster) %&gt;% summarize(n()) #&gt; # A tibble: 15 x 2 #&gt; pollster `n()` #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 ABC News/Washington Post 7 #&gt; 2 Angus Reid Global 1 #&gt; 3 CBS News/New York Times 2 #&gt; 4 Fox News/Anderson Robbins Research/Shaw &amp; Company Research 2 #&gt; 5 IBD/TIPP 8 #&gt; 6 Insights West 1 #&gt; # ... with 9 more rows Let’s visualize the data for the pollsters that are regularly polling: polls %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 6) %&gt;% ggplot(aes(pollster, spread)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) This plot reveals a unexpected result. First, consider that the standard error predicted by theory for each poll: polls %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 6) %&gt;% summarize(se = 2 * sqrt( p_hat * (1-p_hat) / median(samplesize))) #&gt; # A tibble: 5 x 2 #&gt; pollster se #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 ABC News/Washington Post 0.0265 #&gt; 2 IBD/TIPP 0.0333 #&gt; 3 Ipsos 0.0225 #&gt; 4 The Times-Picayune/Lucid 0.0196 #&gt; 5 USC Dornsife/LA Times 0.0183 is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We can also call them pollster bias. In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model. 31.4 Data driven model For each pollster, let’s collect their last reported result before the election: one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% filter(enddate == max(enddate)) %&gt;% ungroup() Here is a histogram of the data for these 15 pollsters: one_poll_per_pollster %&gt;% ggplot(aes(spread)) + geom_histogram(binwidth = 0.01) In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly. The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\). Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster to pollster variability. Our new urn, also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter. In summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\). Our task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT still works in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals. A problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as: \\[ s = \\sqrt{ \\frac{1}{N-1}\\sum_{i=1}^N (X_i - \\bar{X})^2} \\] Unlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here. The sd function in R computes the sample standard deviation: sd(one_poll_per_pollster$spread) #&gt; [1] 0.0242 We are now ready to form a new confidence interval based on our new data driven model: results &lt;- one_poll_per_pollster %&gt;% summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %&gt;% mutate(start = avg - 1.96*se, end = avg + 1.96*se) round(results*100,1) #&gt; avg se start end #&gt; 1 2.9 0.6 1.7 4.1 Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the electoral vote. Are we now ready to declare a probability of Clinton winning the popular vote? Not yet. In our model \\(d\\) is a fixed parameter so we can’t talk about probabilities. To provide probabilities, we will need to learn about Bayesian statistics. Exercises We have been using urn models to motivate the use of probability models. Most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population. Let’s revisit the heights dataset. Suppose we consider the males in our course the population. ```r library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% .$height ``` Mathematically speaking, `x` is our population. Using the urn analogy, we have an urn with the values of `x` in it. What are the average and standard deviation of our population? Call the population average computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\). Set the seed at 1 based on what has been described in this section. What does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)? A. It is practically identical to \\(\\mu\\). B. It is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). C. It is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\). D. Contains no information. So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard estimate of our error \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this section, show your estimate of \\(\\sigma\\). Now that we have an estimate of \\(\\sigma\\), let’s call our estimate \\(s\\). Construct a 95% confidence interval for \\(\\mu\\). Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include \\(\\mu\\)? Set the seed to 1. In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election. data(polls_us_election_2016) polls &lt;- polls_us_election_2016 %&gt;% filter(pollster %in% c(&quot;Rasmussen Reports/Pulse Opinion Research&quot;, &quot;The Times-Picayune/Lucid&quot;) &amp; enddate &gt;= &quot;2016-10-15&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) We want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll. The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance. The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\). To answer the question: is there an urn model?, we will model the observed data \\(Y_ij\\) in the following way: \\[ Y_{ij} = d + b_i + \\varepsilon_{ij} \\] with \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\). Which of the following best represents our question? A. Is \\(\\varepsilon_ij\\) = 0? B. How close are the \\(Y_ij\\) to \\(d\\)? C. Is \\(b_1 \\neq b_2\\)? D. Are \\(b_1 = 0\\) and \\(b_2 = 0\\) ? In the right side of this model only \\(\\varepsilon_ij\\) is a random variable. The other two are constants. What is the expected value of \\(Y_{1j}\\)? Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{11},\\dots,Y_{1N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster: polls %&gt;% filter(pollster==&quot;Rasmussen Reports/Pulse Opinion Research&quot;) %&gt;% summarize(N_1 = n()) What is the expected values \\(\\bar{Y}_1\\)? What is the standard error of \\(\\bar{Y}_1\\) ? What is the expected value \\(\\bar{Y}_2\\)? What is the standard error of \\(\\bar{Y}_2\\) ? Using what we learned by answering the questions above, what is the expected value of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)? Using what we learned by answering the questions above, what is the standard error of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)? \\[ \\mbox{SE}(\\bar{Y}_{2} - \\bar{Y}_1) = \\sqrt{\\mbox{SE}(\\bar{Y}_{2})^2 + \\mbox{SE}(\\bar{Y}_1)^2} = \\sqrt{\\sigma_2^2/N_2 + \\sigma_1^2/N_1}\\] The answer to the question above depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates. What does the CLT tells us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)? A. Nothing because this is not the average of a sample. B. Because the \\(Y_{ij}\\) are approximately normal, so are the averages. \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal. The data are not 0 or 1, so CLT does not apply. We have constructed a random variable that has expected value \\(b_2 - b_1\\), the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), but we can plug the sample standard deviations we computed above. We started off by asking: is \\(b_2 - b_1\\) different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference \\(b_2\\) and \\(b_1\\). The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value? The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error: \\[ \\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}} \\] is called the t-statistic. Later we learn of another approximation for the distribution of this statistic for values of \\(N_2\\) and \\(N_1\\) that aren’t large enough for the CLT. Now notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect? For this exercise, create a new table: polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-15&quot; &amp; state == &quot;U.S.&quot;) %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 5) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% ungroup() Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation. "],
["bayesian-statistics.html", "Chapter 32 Bayesian statistics 32.1 Bayes theorem 32.2 Bayes Theorem simulation 32.3 Bayes in practice 32.4 The hierarchical model Exercises", " Chapter 32 Bayesian statistics What does it mean when an election forecaster tell us that a given candidate has a 90% chance of winning? In the context of the urn model, this would be equivalent to stating that the probability \\(p&gt;0.5\\) is 90%. However, as we discussed earlier, in the urn model \\(p\\) is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we assume it is in fact random. Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics. 32.1 Bayes theorem We start by reviewing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example. Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation: \\[ \\mbox{Prob}(+ \\mid D=1)=0.99, \\mbox{Prob}(- \\mid D=0)=0.99 \\] with \\(+\\) meaning a positive test and \\(D\\) representing if you actually have the disease (1) or not (0). Suppose we select a random person and they test positive, what is the probability that they have the disease? We write this as \\(\\mbox{Prob}(D=1 \\mid +)?\\) The cystic fibrosis rate is 1 in 3,900 which implies that \\(\\mbox{Prob}(D=1)=0.00025\\). To answer this question, we will use Bayes Theorem, which in general tells us that: \\[ \\mbox{Pr}(A \\mid B) = \\frac{\\mbox{Pr}(B \\mid A)\\mbox{Pr}(A)}{\\mbox{Pr}(B)} \\] This equation applied to our problem becomes: \\[ \\begin{aligned} \\mbox{Pr}(D=1 \\mid +) &amp; = \\frac{ P(+ \\mid D=1) \\cdot P(D=1)} {\\mbox{Pr}(+)} \\\\ &amp; = \\frac{\\mbox{Pr}(+ \\mid D=1)\\cdot P(D=1)} {\\mbox{Pr}(+ \\mid D=1) \\cdot P(D=1) + \\mbox{Pr}(+ \\mid D=0) \\mbox{Pr}( D=0)} \\end{aligned} \\] Plugging in the numbers we get: \\[ \\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)} = 0.02 \\] This says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counterintuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this, we run a Monte Carlo simulation. 32.2 Bayes Theorem simulation The following simulation is meant to help you visualize Bayes Theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence. prev &lt;- 0.00025 N &lt;- 100000 outcome &lt;- sample(c(&quot;Disease&quot;,&quot;Healthy&quot;), N, replace = TRUE, prob = c(prev,1-prev)) Note that there are very few people with the disease: N_D &lt;- sum(outcome == &quot;Disease&quot;) N_D #&gt; [1] 23 N_H &lt;- sum(outcome == &quot;Healthy&quot;) N_H #&gt; [1] 99977 Also, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test which is correct 90% of the time. accuracy &lt;- 0.99 test &lt;- vector(&quot;character&quot;,N) test[outcome==&quot;Disease&quot;] &lt;- sample(c(&quot;+&quot;,&quot;-&quot;), N_D, replace=TRUE, prob = c(accuracy, 1 - accuracy)) test[outcome==&quot;Healthy&quot;] &lt;- sample(c(&quot;-&quot;,&quot;+&quot;), N_H, replace=TRUE, prob = c(accuracy, 1 - accuracy)) Because there are so many more controls than cases, even with a low false positive rate, we get more controls than cases in the group that tested positive (code not shown): table(outcome, test) #&gt; test #&gt; outcome - + #&gt; Disease 0 23 #&gt; Healthy 99012 965 From this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.022 32.3 Bayes in practice José Iglesias is a professional baseball player. In April 2013, when he was starting his career, he was performing rather well: Month At Bats H AVG April 20 9 .450 The batting average (AVG) statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. An AVG of .450 means José has been successful 45% of the times he has batted (At Bats) which is rather high, historically speaking. Keep in mind, for example, that no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! To illustrate the way hierarchical models are powerful, we will try to predict José’s batting average at the end of the season. Note that in a typical season, players have about 500 at bats. With the techniques we have learned up to now, referred to as frequentist techniques, the best we can do is provide a confidence interval. We can think of outcomes from hitting as a binomial with a success rate of \\(p\\). So if the success rate is indeed .450, the standard error of just 20 at bats is: \\[ \\sqrt{\\frac{.450 (1-.450)}{20}}=.111 \\] This means that our confidence interval is .450-.222 to .450+.222 or .228 to .672. This prediction has two problems. First, it is very large, so not very useful. Second, it is centered at .450, which implies that our best guess is that this new player will break Ted Williams’ record. If you follow baseball, this last statement will seem wrong and this is because you are implicitly using a hierarchical model that factors in information from years of following baseball. Here we show how we can quantify this intuition. First, let’s explore the distribution of batting averages for all players with more than 500 at bats during the previous three seasons: The average player had an AVG of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both. But how lucky and how good is he? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. 32.4 The hierarchical model The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(p\\). Then we see 20 random outcomes with success probability \\(p\\). We use a model to represents two levels of variability in our data. First, each player is assigned a natural ability to hit at birth. We will use the symbol \\(p\\) to represent this ability. You can think of \\(\\theta\\) as the batting average you would converge to if this particular player batted over and over again. Based on the plots we showed earlier, we assume that \\(p\\) has a normal distribution. With expected value .270 and standard error 0.027. Now the second level of variability has to do with luck when batting. Regardless of how good the player is, sometimes you have bad luck and sometimes you have good luck. At each at bat, this player has a probability of success \\(p\\). If we add up these successes and failures, then the CLT tells us that the observed average, call it \\(Y\\), has a normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/\\sqrt{N}}\\) with \\(N\\) the number of at bats. Statistical textbooks will write the model like this: \\[ \\begin{aligned} p &amp;\\sim N(\\mu, \\tau^2) \\mbox{ describes randomness in picking a player}\\\\ Y \\mid p &amp;\\sim N(p, \\sigma^2) \\mbox{ describes randomness in the performance of this particular player} \\end{aligned} \\] with \\(\\mu = .270\\), \\(\\tau = 0.027\\), and \\(\\sigma^2 = p(1-p)/N\\). Note the two levels (this is why we call them hierarchical): 1) Player to player variability and 2) variability due to luck when batting. In a Bayesian framework, the first level is called a prior distribution and the second the sampling distribution. Now, let’s use this model for José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(p\\). This would be the hierarchical model for our data: \\[ \\begin{aligned} p &amp;\\sim N(.275, .027^2) \\\\ Y \\mid p &amp;\\sim N(p, .111^2) \\end{aligned} \\] We now are ready to compute a posterior distribution to summarize our prediction of \\(p\\). The continuous version of Bayes’ rule can be used here to derive the posterior probability function, which is the distribution of \\(p\\) assuming we observe \\(Y=y\\). In our case, we can show that \\(p\\) when \\(Y=y\\) follows a normal distribution with expected value: \\[ \\begin{aligned} \\mbox{E}(p \\mid Y=y) &amp;= B \\mu + (1-B) y\\\\ &amp;= \\mu + (1-B)(y-\\mu)\\\\ \\mbox{with } B &amp;= \\frac{\\sigma^2}{\\sigma^2+\\tau^2} \\end{aligned} \\] This is a weighted average of the population average \\(\\mu\\) and the observed data \\(Y\\). The weight depends on the SD of the population \\(\\tau\\) and the SD of our observed data \\(\\sigma\\). This weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior mean. In the case of José Iglesias, we have: \\[ \\begin{aligned} \\mbox{E}(p \\mid Y=.450) &amp;= B \\times .275 + (1 - B) \\times .450 \\\\ &amp;= .275 + (1 - B)(.450 - .275) \\\\ B &amp;=\\frac{.111^2}{.111^2 + .027^2} = 0.944\\\\ \\mbox{E}(p \\mid Y=450) &amp;\\approx .285 \\end{aligned} \\] The standard error can be shown to be: \\[ \\mbox{SE}(p\\mid y)^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2} = \\frac{1}{1/.111^2 + 1/.027^2} = 0.00069 \\] and the standard deviation is therefore \\(0.026\\). So we started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Then we used a Bayesian approach that incorporated data from other players and other years to obtain a posterior probability. This is actually referred to as an empirical Bayes approach because we used data to construct the prior. From the posterior, we can report what is called a 95% credible interval by reporting a region, centered at the mean, with a 95% chance of occurring. In our case, this turns out to be: .285 \\(\\pm\\) 0.052. The Bayesian credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months: Month At Bat Hits AVG April 20 9 .450 May 26 11 .423 June 86 34 .395 July 83 17 .205 August 85 25 .294 September 50 10 .200 Total w/o April 330 97 .293 Although both intervals included the final batting average, the Bayesian credible interval provided a much more precise prediction. In particular, it predicted that he would not be as good during the remainder of the season. Exercises In 1999, in England, Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with? A. Sir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &lt; \\mbox{P}r(\\mbox{first case of SIDS})\\). B. Nothing. The multiplicative rule always applies in this way: \\(\\mbox{Pr}(A \\mbox{ and } B) =\\mbox{Pr}(A)\\mbox{Pr}(B)\\) C. Sir Meadow is an expert and we should trust his calculations. D. Numbers don’t lie. Let’s assume that there is in fact a genetic component to SIDS and the the probability of \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) = 1/100\\), is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS? Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written like as the probability of a mother is a son-murdering psychopath given that _two of her children are found dead with no evidence of physical harm__. According to Bayes’ rule, what is this? Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is: \\[ \\mbox{Pr}(A \\mid B) = 0.50 \\] with \\(A = \\mbox{two of her children are found dead with no evidence of physical harm}\\) and \\(B=\\mbox{a mother is a son-murdering psychopath} ) = 0.50\\). Assume that the rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ rule, what is the probability of \\(\\mbox{Pr}(B \\mid A)\\) ? After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clarke was acquitted in June 2003. What did the expert miss? A. He made an arithmetic error. B. He made two mistakes. First, he misused the multiplicative rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million. C. He mixed up the numerator and denominator of Bayes’ rule. D. He did not use R. Florida is one of the most closely watched states in the U.S. election because it has many electoral votes and the election is generally close. Create the following table with the polls taken during the last two weeks: library(tidyverse) library(dslabs) data(polls_us_election_2016) polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;Florida&quot; &amp; enddate &gt;= &quot;2016-11-04&quot; ) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Take the average of these 49. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(d\\) to be Normal with expected value \\(\\mu\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\mu\\) and \\(\\tau\\)? A. \\(\\mu\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(d\\). B. \\(\\mu\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\mu\\) close to 0 because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close. C. \\(\\mu\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\mu\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\). D. The choice of prior has no effect on Bayesian Analysis. The CLT tells us that our estimate of the spread \\(\\hat{d}\\) has normal distribution with expected value \\(d\\) and standard deviation \\(\\sigma\\) calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\mu = 0\\) and \\(\\tau = 0.01\\). Now compute the standard deviation of the posterior distribution. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals. According to this analysis, what was the probability that Trump wins Florida? Now use sapply function to change the prior variance from seq(0.05, 0.05, len = 100) and observe how the probability changes by making a plot. "],
["case-study-election-forecasting.html", "Chapter 33 Case study: Election forecasting 33.1 Bayesian approach 33.2 The general bias 33.3 Mathematical representations of models 33.4 Predicting the electoral college 33.5 Forecasting Exercise Exercises", " Chapter 33 Case study: Election forecasting In a previous section, we generated these data tables: polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% filter(enddate == max(enddate)) %&gt;% ungroup() results &lt;- one_poll_per_pollster %&gt;% summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %&gt;% mutate(start = avg - 1.96*se, end = avg + 1.96*se) Below, we will use these for our forecasting. 33.1 Bayesian approach Pollsters tend to make probabilistic statements about the results of the election. For example, “The chance that Obama wins the electoral colleges is 91%” is a probabilistic statement about the parameter \\(d\\). We showed that for the 2016 election, FiveThirtyEight gave Clinton a 81.4% chance of winning the popular vote. To do this, they used the Bayesian approach we described. We assume a hierarchical model similar to what we did to predict the performance of a baseball player. Statistical textbooks will write the model like this: \\[ \\begin{aligned} d &amp;\\sim N(\\mu, \\tau^2) \\mbox{ describes our best guess had we not seen any polling data}\\\\ \\bar{X} \\mid d &amp;\\sim N(d, \\sigma^2) \\mbox{ describes randomness due to sampling and the pollster effect} \\end{aligned} \\] For our best guess, we note that before any poll data is available, we can use data sources other than polling data. A popular approach is to use what are called fundamentals, which are based on properties about the current economy that historically appear to have an effect in favor or against the incumbent party. We won’t use these here. Instead, we will use \\(\\mu = 0\\), which is interpreted as a model that simply does not provide any information on who will win. For the standard deviation, we will use recent historical data that shows the winner of the popular vote has an average spread of about 3.5%. Therefore, we set \\(\\tau = 0.035\\). Now we can use the formulas for the posterior distribution for the parameter \\(d\\): the probability of \\(d&gt;0\\) given the observed poll data: mu &lt;- 0 tau &lt;- 0.035 sigma &lt;- results$se Y &lt;- results$avg B &lt;- sigma^2 / (sigma^2 + tau^2) posterior_mean &lt;- B*mu + (1-B)*Y posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2)) posterior_mean #&gt; [1] 0.0281 posterior_se #&gt; [1] 0.00615 To make a probability statement, we use the fact that the posterior distribution is also normal. And we have a credible interval of: posterior_mean + c(-1.96, 1.96)*posterior_se #&gt; [1] 0.0160 0.0401 The posterior probability \\(\\mbox{Pr}(d&gt;0 \\mid \\bar{X})\\) is: 1 - pnorm(0, posterior_mean, posterior_se) #&gt; [1] 1 This says we are 100% sure Clinton will win the popular vote which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference? 33.2 The general bias After elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our model does not take into account, is that it is common to see the general bias that affects many pollsters in the same way. There is no good explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%. Although we know this bias term affects our polls, we have no way of knowing what this bias is until election. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for this variability. 33.3 Mathematical representations of models Suppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\). The theory tells us that these random variables have expected value \\(d\\) and \\(2 \\sqrt{p(1-p)/N}\\). For reasons that will soon become clear, we can represent this model mathematically like this: \\[ X_j = d + \\varepsilon_j \\] We use the index \\(j\\) to represent the different polls and we define \\(\\varepsilon_j\\) to be a random variable that explains the poll to poll variability introduced by sampling error. To do this, we assume its average is 0 and standard error is \\(2 \\sqrt{p(1-p)/N}\\). If \\(d\\) is 2.1 and the sample size for these polls is 2,000, we can simulate \\(J=6\\) data points from this model like this: set.seed(3) J &lt;- 6 N &lt;- 2000 d &lt;- .021 p &lt;- (d + 1)/2 X &lt;- d + rnorm(J,0,2*sqrt(p*(1-p)/N)) Now suppose we have \\(J=6\\) data points from \\(I=5\\) different pollsters. To represent this we now need two indexes, one for pollster and one for the polls each pollster takes. We use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster. If we apply the same model, we write: \\[ X_{i,j} = d + h_i + \\varepsilon_{i,j} \\] To simulate data, we now have to loop through the pollsters: I &lt;- 5 J &lt;- 6 N &lt;- 2000 X &lt;- sapply(1:I, function(i){ d + rnorm(J,0,2*sqrt(p*(1-p)/N)) }) The simulated data: does not really seem to capture the features of the actual data: The model above does not account for pollster to pollster variability. To fix this, we add a new term for the pollster effect. We will use \\(\\theta_i\\) to represent the house effect of the \\(i\\)-th pollster. The model is now augmented to: \\[ X_{i,j} = d + h_i + \\varepsilon_{i,j} \\] To simulate data from a specific pollster, we now need to draw an \\(h_i\\) and then add the \\(\\varepsilon\\)s. Here is how we would do it for one specific pollster. We assume \\(\\sigma_h\\) is 0.025: I &lt;- 5 J &lt;- 6 N &lt;- 2000 d &lt;- .021 p &lt;- (d + 1)/2 h &lt;- rnorm(I, 0, 0.025) X &lt;- sapply(1:I, function(i){ d + h[i] + rnorm(J,0,2*sqrt(p*(1-p)/N)) }) The simulated data now looks more like the actual data: Note that \\(h_i\\) is common to all the observed spreads from a specific pollster. Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster. Now, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is \\(\\sigma_h\\). But historically we see that every election has a general bias affecting all polls. We can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another term to account for this variability: \\[ X_{ij} = d + b + h_i + \\varepsilon_{ij} \\] and model \\(b\\) as having expected value 0 and, based on historical data, assume the standard error for \\(b\\) is \\(\\sigma_b = 0.025\\). \\[ X_{ij} = d + b + h_i + \\varepsilon_{ij} \\] The variability of \\(b\\) is not observed because every single poll we observe in 2016 has this general bias. An implication of adding this term to the model is that the standard deviation for \\(X_{ij}\\) is actually higher than what we earlier called \\(\\sigma\\), which combines the pollster variability and the sample in variability, and was estimated with: sd(one_poll_per_pollster$spread) #&gt; [1] 0.0242 since we have to add \\(\\sigma_b\\). And note that: \\[ \\bar{X} = d + b + \\frac{1}{N}\\sum_{i=1}^N X_i \\] which implies that the standard deviation of \\(\\bar{X}\\) is: \\[ \\sqrt{\\sigma^2/N + \\sigma_b^2} \\] Since the same \\(b\\) is in every measurement, the average does not reduce its variance. This is an important point: it does not matter how many polls you take, this bias does not get reduced. If we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s: mu &lt;- 0 tau &lt;- 0.035 sigma &lt;- sqrt(results$se^2 + .025^2) Y &lt;- results$avg B &lt;- sigma^2 / (sigma^2 + tau^2) posterior_mean &lt;- B*mu + (1-B)*Y posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2)) 1 - pnorm(0, posterior_mean, posterior_se) #&gt; [1] 0.817 33.4 Predicting the electoral college Up to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes: results_us_election_2016 %&gt;% top_n(5, electoral_votes) #&gt; state electoral_votes clinton trump others #&gt; 1 California 55 61.7 31.6 6.7 #&gt; 2 Florida 29 47.8 49.0 3.2 #&gt; 3 Illinois 20 55.8 38.8 5.4 #&gt; 4 New York 29 59.0 36.5 4.5 #&gt; 5 Pennsylvania 20 47.9 48.6 3.6 #&gt; 6 Texas 38 43.2 52.2 4.5 With some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you win California by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000 and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election. Nonetheless, many people in the US consider the electoral college unfair and would like to see it abolished. We are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election: results &lt;- polls_us_election_2016 %&gt;% filter(state!=&quot;U.S.&quot; &amp; !grepl(&quot;CD&quot;, state) &amp; enddate &gt;=&quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% group_by(state) %&gt;% summarize(avg = mean(spread), sd = sd(spread), n = n()) %&gt;% mutate(state = as.character(state)) Here are the 10 closest races according to the polls: results %&gt;% arrange(abs(avg)) #&gt; # A tibble: 47 x 4 #&gt; state avg sd n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Florida 0.00356 0.0163 7 #&gt; 2 North Carolina -0.00730 0.0306 9 #&gt; 3 Ohio -0.0104 0.0252 6 #&gt; 4 Nevada 0.0169 0.0441 7 #&gt; 5 Iowa -0.0197 0.0437 3 #&gt; 6 Michigan 0.0209 0.0203 6 #&gt; # ... with 41 more rows We now introduce the command left_join that will let us easily add the number of electoral votes for each state from the data set us_electoral_votes_2016. We will describe this function in detail in the Wrangling chapter. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first: results &lt;- left_join(results, results_us_election_2016, by = &quot;state&quot;) Notice that some states have no polls because the winner is pretty much known: results_us_election_2016 %&gt;% filter(!state %in% results$state) #&gt; state electoral_votes clinton trump others #&gt; 1 Alaska 3 36.6 51.3 12.2 #&gt; 2 Rhode Island 4 54.4 38.9 6.7 #&gt; 3 Wyoming 3 21.9 68.2 10.0 #&gt; 4 District of Columbia 3 90.9 4.1 5.0 No polls were conducted in DC, Rhode Island, Alaska, and Wyoming because the first two are sure to be Democrats and the last two Republicans. The code below assigns a standard deviation, the median of the rest, to states with just one poll. results &lt;- results %&gt;% mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm=TRUE), sd)) To make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day \\(d\\). We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or \\(\\tau=0.02\\). The Bayesian calculation looks like this: mu &lt;- 0 tau &lt;- 0.02 results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sd^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2))) %&gt;% arrange(abs(posterior_mean)) #&gt; # A tibble: 47 x 12 #&gt; state avg sd n electoral_votes clinton trump others sigma #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Flor… 0.00356 0.0163 7 29 47.8 49 3.2 0.00618 #&gt; 2 Nort… -0.00730 0.0306 9 15 46.2 49.8 4 0.0102 #&gt; 3 Ohio -0.0104 0.0252 6 18 43.5 51.7 4.8 0.0103 #&gt; 4 Iowa -0.0197 0.0437 3 6 41.7 51.1 7.1 0.0252 #&gt; 5 Neva… 0.0169 0.0441 7 6 47.9 45.5 6.6 0.0167 #&gt; 6 Mich… 0.0209 0.0203 6 16 47.3 47.5 5.2 0.00827 #&gt; # ... with 41 more rows, and 3 more variables: B &lt;dbl&gt;, #&gt; # posterior_mean &lt;dbl&gt;, posterior_se &lt;dbl&gt; The estimates based on posterior do move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results: results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2))) %&gt;% ggplot(aes(avg, posterior_mean, size = n)) + geom_point() + geom_abline(slope = 1, intercept = 0) Now we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep the total number of electoral votes for Clinton. Note that we add 7 to account for Rhode Island and D.C.: mu &lt;- 0 tau &lt;- 0.02 clinton_EV &lt;- replicate(1000, { results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)), simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se), clinton = ifelse(simulated_result&gt;0, electoral_votes, 0)) %&gt;% summarize(clinton = sum(clinton)) %&gt;% .$clinton + 7## 7 for Rhode Island and D.C. }) mean(clinton_EV&gt;269) #&gt; [1] 0.998 This model gives Clinton over 99% chance of winning. Here is a histogram of the possible outcomes: A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened? The model above ignores the general bias. The general bias in 2016 was not that big compared to other years: it was between 1 and 2%. But because the election was close in several big states, a large number of polls made the estimates of standard errors small, and by ignoring the variability introduced by the general bias, pollsters were over confident on the poll data. FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at \\(\\sigma_b = 0.03\\): tau &lt;- 0.02 bias_sd &lt;- 0.03 clinton_EV_2 &lt;- replicate(1000, { results %&gt;% mutate(sigma = sqrt(sd^2/n + bias_sd^2), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)), simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se), clinton = ifelse(simulated_result&gt;0, electoral_votes, 0)) %&gt;% summarize(clinton = sum(clinton) + 7) %&gt;% .$clinton ## 7 for Rhode Island and D.C. }) mean(clinton_EV_2&gt;269) #&gt; [1] 0.837 This gives us a much more sensible result. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results. FiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. They predicted a probability of 71%. 33.5 Forecasting Forecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election? Here we study the variability of poll results across time. To make sure the variability we observe is not due to pollster effects, let’s study data from one pollster: one_pollster &lt;- polls_us_election_2016 %&gt;% filter(pollster == &quot;Ipsos&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Since there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here: se &lt;- one_pollster %&gt;% summarize(empirical = sd(spread), theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize))) se #&gt; empirical theoretical #&gt; 1 0.0403 0.0326 The empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict: one_pollster %&gt;% ggplot(aes(spread)) + geom_histogram(binwidth = 0.01, color = &quot;black&quot;) Where is the extra variability coming from? The following plots make a strong case that the extra variability comes from time annunciations not accounted for by the theory that assumes \\(p\\) is fixed: Some of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see them consistently across several pollsters: polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate&gt;=&quot;2016-07-01&quot;) %&gt;% group_by(pollster) %&gt;% filter(n()&gt;=10) %&gt;% ungroup() %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% ggplot(aes(enddate, spread)) + geom_smooth(method = &quot;loess&quot;, span = 0.1) + geom_point(aes(color=pollster), show.legend = FALSE, alpha=0.6) This implies that, if we are going to forecast, our model must include a term to model that accounts for the time effect. We need to write a model including a bias term for time: \\[ Y_{ijt} = d + b + h_j + b_t + \\varepsilon_{ijt} \\] The standard deviation of \\(b_t\\) would depend on \\(t\\) since the closer we get to election day, the smaller this bias term should be. Pollsters also try to estimate trends, call them \\(f(t)\\), from these data and incorporate these into their predictions. The blue lines in the plots above: \\[ Y_{ijt} = d + b + h_j + b_t + f(t) + \\varepsilon_{ijt} \\] We usually see the estimated \\(f(t)\\) not for the difference, but for the actual percentages for each candidate: polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate&gt;=&quot;2016-07-01&quot;) %&gt;% select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %&gt;% rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %&gt;% gather(candidate, percentage, -enddate, -pollster) %&gt;% mutate(candidate = factor(candidate, levels = c(&quot;Trump&quot;,&quot;Clinton&quot;)))%&gt;% group_by(pollster) %&gt;% filter(n()&gt;=10) %&gt;% ungroup() %&gt;% ggplot(aes(enddate, percentage, color = candidate)) + geom_point(show.legend = FALSE, alpha=0.4) + geom_smooth(method = &quot;loess&quot;, span = 0.15) + scale_y_continuous(limits = c(30,50)) Once a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There are a variety of methods for fitting models which we don’t discuss here. In a later chapter, we discuss some of these methods. Exercise Create this table: library(tidyverse) library(dslabs) data(&quot;polls_us_election_2016&quot;) polls &lt;- polls_us_election_2016 %&gt;% filter(state != &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Now for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, end date, pollster, grade, spread, lower, upper. You can add the final result to the cis table you just created using the right_join function like this: add &lt;- results_us_election_2016 %&gt;% mutate(actual_spread = clinton/100 - trump/100) %&gt;% select(state, actual_spread) cis &lt;- cis %&gt;% mutate(state = as.character(state)) %&gt;% left_join(add, by = &quot;state&quot;) Now determine how often the 95% confidence interval includes the actual result. Now repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use n=n(), grade = grade[1] in the call to summarize. Repeat exercise 3, but instead of pollster, stratify by state. Here we can’t show grades. Make a barplot based on the result of exercise 4. Use coord_flip. For forecasters, it is important to call the correct winner. Hence, even if your confidence interval is incorrect, if you correctly called the right winner, your overall predictions will do better. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: use the function sign. Call the object resids. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors? We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) to only include pollsters with high grades. Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use group_by, filter then ungroup. You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Some pollsters are now modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models. Exercises Create this table: library(tidyverse) library(dslabs) data(&quot;polls_us_election_2016&quot;) polls &lt;- polls_us_election_2016 %&gt;% filter(state != &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Now for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, end date, pollster, grade, spread, lower, upper. You can add the final result to the cis table you just created using the right_join function like this: add &lt;- results_us_election_2016 %&gt;% mutate(actual_spread = clinton/100 - trump/100) %&gt;% select(state, actual_spread) cis &lt;- cis %&gt;% mutate(state = as.character(state)) %&gt;% left_join(add, by = &quot;state&quot;) Now determine how often the 95% confidence interval includes the actual result. Now repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use n=n(), grade = grade[1] in the call to summarize. Repeat exercise 3, but instead of pollster, stratify by state. Here we can’t show grades. Make a barplot based on the result of exercise 4. Use coord_flip. For forecasters, it is important to call the correct winner. Hence, even if your confidence interval is incorrect, if you correctly called the right winner, your overall predictions will do better. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: use the function sign. Call the object errors. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors? We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states all it affected some states differently. Include only pollsters with filter(grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))). Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use group_by, filter then ungroup. Advanced: You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation in we did not model this behavior since we added general bias, rather than a regional bias. Some pollsters are now modeling correlation between similar states and estimating this correlation from historical data. Learn about random effects and mixed models and describe how you would use these to account for this type of correlation. "],
["association-tests.html", "Chapter 34 Association Tests 34.1 Lady Tasting Tea 34.2 Two-by-two tables 34.3 Chi-square Test 34.4 The Odds Ratio 34.5 Large samples, small p-values 34.6 Confidence intervals for the odds ratio Exercises", " Chapter 34 Association Tests The statistical tests we have studied up to now leave out a substantial portion of data types. Specifically, we have not discussed inference for binary, categorical and ordinal data. To give a very specific example, consider the following case study. A 2014 PNAS paper analyzed success rates from funding agencies in the Netherlands and concluded that their: results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials. The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes: data(&quot;research_funding_rates&quot;) research_funding_rates %&gt;% select(discipline, contains(&quot;total&quot;)) #&gt; discipline applications_total awards_total success_rates_total #&gt; 1 Chemical sciences 122 32 26.2 #&gt; 2 Physical sciences 174 35 20.1 #&gt; 3 Physics 76 20 26.3 #&gt; 4 Humanities 396 65 16.4 #&gt; 5 Technical sciences 251 43 17.1 #&gt; 6 Interdisciplinary 183 29 15.8 #&gt; 7 Earth/life sciences 282 56 19.9 #&gt; 8 Social sciences 834 112 13.4 #&gt; 9 Medical sciences 505 75 14.9 We have these values for each gender: names(research_funding_rates) #&gt; [1] &quot;discipline&quot; &quot;applications_total&quot; &quot;applications_men&quot; #&gt; [4] &quot;applications_women&quot; &quot;awards_total&quot; &quot;awards_men&quot; #&gt; [7] &quot;awards_women&quot; &quot;success_rates_total&quot; &quot;success_rates_men&quot; #&gt; [10] &quot;success_rates_women&quot; We can compute the totals that were successful and the totals that were not as follows: totals &lt;- research_funding_rates %&gt;% select(-discipline) %&gt;% summarize_all(funs(sum)) %&gt;% summarize(yes_men = awards_men, no_men = applications_men - awards_men, yes_women = awards_women, no_women = applications_women - awards_women) So we see that a larger percent of men than women received awards: totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men), percent_women = yes_women/(yes_women+no_women)) #&gt; percent_men percent_women #&gt; 1 0.177 0.149 But could this be due just to random variability? Here we learn how to perform inference for this type of data. 34.1 Lady Tasting Tea R.A. Fisher was one of the first to formalize hypothesis testing. The “Lady Testing Tea” is one of the most famous examples. The story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent. As an example, suppose she picked 3 out of 4 correctly, do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after. Under the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. The probability of picking 3 is \\({4 \\choose 3} {4 \\choose 1} / {8 \\choose 4} = 16/70\\). The probability of picking all 4 correct is \\({4 \\choose 4} {4 \\choose 0}/{8 \\choose 4}= 1/70\\). Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is \\(\\approx 0.24\\). This is the p-value. The procedure that produced this p-value is called Fisher’s exact test and it uses the hypergeometric distribution. 34.2 Two-by-two tables The data from the experiment is usually summarized by a table like this: tab &lt;- matrix(c(3,1,1,3),2,2) rownames(tab)&lt;-c(&quot;Poured Before&quot;,&quot;Poured After&quot;) colnames(tab)&lt;-c(&quot;Guessed before&quot;,&quot;Guessed after&quot;) tab #&gt; Guessed before Guessed after #&gt; Poured Before 3 1 #&gt; Poured After 1 3 These are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence. The function fisher.test performs the inference calculations above and can be obtained like this: fisher.test(tab, alternative=&quot;greater&quot;) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: tab #&gt; p-value = 0.2 #&gt; alternative hypothesis: true odds ratio is greater than 1 #&gt; 95 percent confidence interval: #&gt; 0.314 Inf #&gt; sample estimates: #&gt; odds ratio #&gt; 6.41 34.3 Chi-square Test Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four before tea and four after tea and the lady knew this, so the answers would also have to be four and four. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below. Imagine we have 2823 applicants, some are men and some are women and some get funded, whereas other don’t. We saw that the success rates for men and woman were: totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men), percent_women = yes_women/(yes_women+no_women)) #&gt; percent_men percent_women #&gt; 1 0.177 0.149 respectively. Would we see this again if we randomly assign funding at the rate: funding_rate &lt;- totals %&gt;% summarize(percent_total = (yes_men + yes_women)/ (yes_men + no_men +yes_women + no_women)) %&gt;% .$percent_total funding_rate #&gt; [1] 0.165 The Chi-square test answers this question. The first step is to create the two-by-two data table: two_by_two &lt;- data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), men = c(totals$no_men, totals$yes_men), women = c(totals$no_women, totals$yes_women)) two_by_two #&gt; awarded men women #&gt; 1 no 1345 1011 #&gt; 2 yes 290 177 The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be: data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), men = (totals$no_men + totals$yes_men) * c(1 - funding_rate, funding_rate), women = (totals$no_women + totals$yes_women)* c(1 - funding_rate, funding_rate)) #&gt; awarded men women #&gt; 1 no 1365 991 #&gt; 2 yes 270 197 We can see that more men than expected and less women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two by two table and returns the results from the test: chisq_test &lt;- two_by_two %&gt;% select(-awarded) %&gt;% chisq.test() chisq_test #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: . #&gt; X-squared = 4, df = 1, p-value = 0.05 We see that the p-value is 0.0509: chisq_test$p.value #&gt; [1] 0.0509 34.4 The Odds Ratio An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as \\(X = 1\\) if you are a male and 0 otherwise, and \\(Y=1\\) if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined: \\[\\mbox{Pr}(Y=1 \\mid X=1) / \\mbox{Pr}(Y=0 \\mid X=1)\\] and can be computed like this: odds_men &lt;- (two_by_two$men[2] / sum(two_by_two$men)) / (two_by_two$men[1] / sum(two_by_two$men)) And the odds of being funded if you are a woman is: \\[\\mbox{Pr}(Y=1 \\mid X=0) / \\mbox{Pr}(Y=0 \\mid X=0)\\] and can be computed like this: odds_women &lt;- (two_by_two$women[2] / sum(two_by_two$women)) / (two_by_two$women[1] / sum(two_by_two$women)) The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women? odds_men / odds_women #&gt; [1] 1.23 We often see two by two tables written out as Men Women Awarded a b Not Awarded c d In this case, the odds ratio is \\(\\frac{a/c}{b/d}\\) which is equivalent to \\((ad) / (bc)\\) 34.5 Large samples, small p-values As mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be practically significant or scientifically significant. Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10: two_by_two %&gt;% select(-awarded) %&gt;% mutate(men = men*10, women = women*10) %&gt;% chisq.test() #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: . #&gt; X-squared = 40, df = 1, p-value = 3e-10 Yet the odds ratio is unchanged. 34.6 Confidence intervals for the odds ratio Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT. However, statistical theory tells us that when all four entries of the two by two table are large enough, then the log of the odds ratio is approximately normal with standard error \\[ \\sqrt{1/a + 1/b + 1/c + 1/d} \\] This implies that a 95% confidence interval for the log odds ratio can be formed by: \\[ \\log \\{ (ad) / (cd) \\} \\pm 1.96 \\sqrt{1/a + 1/b + 1/c + 1/d} \\] By exponentiating these two numbers we can construct a confidence interval of the odds ratio. Using R we can compute this confidence interval as follows: log_or &lt;- log( odds_men / odds_women ) se &lt;- two_by_two %&gt;% select(-awarded) %&gt;% summarize(se = sqrt(sum(1/men) + sum(1/women))) %&gt;% .$se ci &lt;- log_or + c(-1,1) * qnorm(0.975) * se ci #&gt; [1] 0.0043 0.4123 If we want to convert it back to the odds ratio scale, we can exponentiate: exp(ci) #&gt; [1] 1.00 1.51 Note that 0 is not included in the confidence interval for the log odds ratio (1 not included for odds ratio) which must mean that the p-value is smaller then 0.05. We can confirm this using: 2*(1 - pnorm(log_or, 0, se)) #&gt; [1] 0.0454 This is a slightly different p-value that with the chi-squared test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio consult the book by McCullagh and Nelder, 1989. Exercises A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi squared test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure. Why did we use the Chi square test instead of Fisher’s exact test in the previous exercise? A. It actually does not matter, since they give the exact same p-value. B. Fisher’s exact and the Chi squareare different names for the same test. C. Because the sum of the rows and columns of the two by two table ere not fixedso the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data. D. Because the Chi square test runs faster. Compute the odds ratio of losing under pressure along with a confidence interval. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this? A. We made a mistake in our code. B. These are not t-tests so the connection between p-value and confidence intervals does not apply. C. Different approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better. D. We should use the Fisher exact test to get confidence intervals. Multiply the two by two table by 2 and see if the p-value and confidence retrieval are a better match. However, very rarely in a data science project is data easily available as part of a package. We did quite a bit of work “behind the scenes” to get the original raw data into the tidy tables you worked with. Much more typical is for the data to be in a file, a database, or extracted from a document, including web pages, tweets, or PDFs. In these cases, the first step is to import the data into R and, when using the tidyverse, tidy the data. This initial step in the data analysis process usually involves several, often complicated, steps to covert data from its raw form to the tidy form that greatly facilitates the rest of the analysis. We refer to this process as data wrangling. Here we cover several common steps of the data wrangling process including importing data into R from files, tidying data, string processing, html parsing, working with dates and times, and text mining. Rarely are all these wrangling steps necessary in a single analysis, but data scientists will likely face them all at some point. Some of the examples we use to demonstrate data wrangling techniques are based on the work we did to convert raw data into the tidy datasets provided by the dslabs package and used in the book as examples. "],
["tidy-data.html", "Chapter 35 Tidy data Exercises", " Chapter 35 Tidy data library(dslabs) ds_theme_set() To help define tidy data, we refer to an example from the data visualization chapter in which we plotted fertility data across time for two countries: South Korea and Germany. To make the plot, we used this subset of the data: data(&quot;gapminder&quot;) tidy_data &lt;- gapminder %&gt;% filter(country %in% c(&quot;South Korea&quot;, &quot;Germany&quot;)) %&gt;% select(country, year, fertility) head(tidy_data) #&gt; country year fertility #&gt; 1 Germany 1960 2.41 #&gt; 2 South Korea 1960 6.16 #&gt; 3 Germany 1961 2.44 #&gt; 4 South Korea 1961 5.99 #&gt; 5 Germany 1962 2.47 #&gt; 6 South Korea 1962 5.79 With the data in this format, we could quickly make the desired plot: tidy_data %&gt;% ggplot(aes(year, fertility, color = country)) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). One reason this code works seamlessly is because the data is tidy: each point is represented in a row. This brings us to the definition of tidy data: each row represents one observation and the columns represent the different variables available for each of these observations. If we go back to the original data provided by Gapminder, we see that it does not start out tidy. We include an example file with the data shown in this graph mimicking the way it was originally saved in a spreadsheet: path &lt;- system.file(&quot;extdata&quot;, package=&quot;dslabs&quot;) filename &lt;- file.path(path, &quot;fertility-two-countries-example.csv&quot;) wide_data &lt;- read_csv(filename) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_double(), #&gt; country = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. The object wide_data includes the same information as the object tidy_data except it’s in a different format: a wide format. Here are the first nine columns: select(wide_data, country, `1960`:`1967`) #&gt; # A tibble: 2 x 9 #&gt; country `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 #&gt; 2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 Two important differences between the wide and tidy formats are: 1) in the wide format, each row includes several observations and 2) one of the variables, year, is stored in the header. The ggplot code we introduced earlier no longer works here. For one, there is no year variable. To use the tidyverse, we need to wrangle this data into tidy format. The next two chapters describe tools that are available for tidying data. Exercises Examine the dataset co2. Which of the following is true: A. co2 is tidy data: it has one year for each row. B. co2 is not tidy: we need at least one column with a character vector. C. co2 is not tidy: it is a matrix not a data frame. D. co2 is not tidy: to be tidy we would have to wrangle it to have three columns: year, month and value; then each co2 observation has a row. Examine the dataset ChickWeight. Which of the following is true: ChickWeight is not tidy: each chick has more than one row. ChickWeight is tidy: each observation, here a weight, is represented by one row. The chick from which this measurement came from is one the variables. ChickWeight is not a tidy: we are missing the year column. ChickWeight is tidy: it is stored in a data frame. Examine the dataset BOD. Which of the following is true: A. BOD is not tidy: it only has six rows. B. BOD is not tidy: the first column is just an index. C. BOD is tidy: each row is an observation with two values, time and demand. D. BOD is tidy: all small datasets are tidy by definition. Which of the following datasets is tidy (you can pick more than one): A. BJsales B. EuStockMarkets C. DNase D. Formaldehyde E. Orange F. UCBAdmissions "],
["reshaping-data.html", "Chapter 36 Reshaping data 36.1 gather 36.2 spread 36.3 separate 36.4 unite Exercises", " Chapter 36 Reshaping data #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_double(), #&gt; country = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. As we have seen, having data in tidy format is what makes the tidyverse flow. After the first step in the data analysis process, importing data, a common next step is to reshape the data into a form that facilitates the rest of the analysis. The tidyr package includes several functions that are useful for tidying data. 36.1 gather One of the most used functions in the tidyr package is gather, which converts wide data into tidy data. In the third argument of the gather function, you specify the columns that will be gathered. The default is to gather all columns so, in most cases, we have to specify the columns. Here we want columns 1960, 1961 up to 2015. The first argument sets the column/variable name that will hold the variable that is currently kept in the wide data column names. In our case, it makes sense to set name to year, although we can name it anything. The second argument sets the new column/variable name that will hold the values. In this case, we call it fertility since this is what is stored in this file. Note that nowhere in this file does it tell us this is fertility data. Instead, this information was kept in the file name. The gathering code looks like this: new_tidy_data &lt;- wide_data %&gt;% gather(year, fertility, `1960`:`2015`) We can see that the data have been converted to tidy format with columns year and fertility: head(new_tidy_data) #&gt; # A tibble: 6 x 3 #&gt; country year fertility #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 South Korea 1960 6.16 #&gt; 3 Germany 1961 2.44 #&gt; 4 South Korea 1961 5.99 #&gt; 5 Germany 1962 2.47 #&gt; 6 South Korea 1962 5.79 However, each year resulted in two rows since we have two countries and this column was not gathered. A somewhat quicker way to write this code is to specify which column will not be gathered, rather than all the columns that will be gathered: new_tidy_data &lt;- wide_data %&gt;% gather(year, fertility, -country) The new_tidy_data object looks like the original tidy_data we used with just one minor difference. Can you spot it? Look at the data type of the year column: class(tidy_data$year) #&gt; [1] &quot;integer&quot; class(new_tidy_data$year) #&gt; [1] &quot;character&quot; The gather function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to numbers. The gather function has an argument for that, the convert argument: new_tidy_data &lt;- wide_data %&gt;% gather(year, fertility, -country, convert = TRUE) class(new_tidy_data$year) #&gt; [1] &quot;integer&quot; We could have also used the mutate and as.numeric. Now that the data is tidy, we can use the same ggplot code as before: new_tidy_data %&gt;% ggplot(aes(year, fertility, color = country)) + geom_point() 36.2 spread As we will see in later examples, it is sometimes useful for data wrangling purposes to convert tidy data into wide data. We often use this as an intermediate step in tidying up data. The spread function is basically the inverse of gather. The first argument is for the data, but since we are using the pipe, we don’t show it. The second argument tells spread which variable will be used as the column names. The third argument specifies which variable to use to fill out the cells: new_wide_data &lt;- new_tidy_data %&gt;% spread(year, fertility) select(new_wide_data, country, `1960`:`1967`) #&gt; # A tibble: 2 x 9 #&gt; country `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 #&gt; 2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 The following diagram can help remind you how these two functions work: (Source: RStudio) 36.3 separate The data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal. path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) filename &lt;- file.path(path, &quot;life-expectancy-and-fertility-two-countries-example.csv&quot;) raw_dat &lt;- read_csv(filename) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_double(), #&gt; country = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. select(raw_dat, 1:5) #&gt; # A tibble: 2 x 5 #&gt; country `1960_fertility` `1960_life_expe… `1961_fertility` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 69.3 2.44 #&gt; 2 South … 6.16 53.0 5.99 #&gt; # ... with 1 more variable: `1961_life_expectancy` &lt;dbl&gt; First, note that the data is in wide format. Second, notice that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable. This is not a recommended way to store information, encoded in column names, but it is quite common. We will put our wrangling skills to work to extract this information and store it in a tidy fashion. We can start the data wrangling with the gather function, but we should no longer use the column name year for the new column since it also contains the variable type. We will call it key, the default, for now: dat &lt;- raw_dat %&gt;% gather(key, value, -country) head(dat) #&gt; # A tibble: 6 x 3 #&gt; country key value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960_fertility 2.41 #&gt; 2 South Korea 1960_fertility 6.16 #&gt; 3 Germany 1960_life_expectancy 69.3 #&gt; 4 South Korea 1960_life_expectancy 53.0 #&gt; 5 Germany 1961_fertility 2.44 #&gt; 6 South Korea 1961_fertility 5.99 The result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the key column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore: dat$key[1:5] #&gt; [1] &quot;1960_fertility&quot; &quot;1960_fertility&quot; &quot;1960_life_expectancy&quot; #&gt; [4] &quot;1960_life_expectancy&quot; &quot;1961_fertility&quot; Encoding multiple variables in a column name is such a common problem that the readr package includes a function to separate these columns into two or more. Apart from the data, the separate function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;variable_name&quot;), &quot;_&quot;) Because “_&quot; is the default separator assumed by separate, we do not have to include it in the code: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;variable_name&quot;)) #&gt; Warning: Expected 2 pieces. Additional pieces discarded in 112 rows [3, 4, #&gt; 7, 8, 11, 12, 15, 16, 19, 20, 23, 24, 27, 28, 31, 32, 35, 36, 39, 40, ...]. #&gt; # A tibble: 224 x 4 #&gt; country year variable_name value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility 2.41 #&gt; 2 South Korea 1960 fertility 6.16 #&gt; 3 Germany 1960 life 69.3 #&gt; 4 South Korea 1960 life 53.0 #&gt; 5 Germany 1961 fertility 2.44 #&gt; 6 South Korea 1961 fertility 5.99 #&gt; # ... with 218 more rows The function does separate the values, but we run into a new problem. We receive the warning Too many values at 112 locations: and that the life_expectancy variable is truncated to life. This is because the _ is used to separate life and expectancy not just year and variable name! We could add a third column to catch this and let the separate function know which column to fill in with missing values, NA, when there is no third value. Here we tell it to fill the column on the right: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;first_variable_name&quot;, &quot;second_variable_name&quot;), fill = &quot;right&quot;) #&gt; # A tibble: 224 x 5 #&gt; country year first_variable_name second_variable_name value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility &lt;NA&gt; 2.41 #&gt; 2 South Korea 1960 fertility &lt;NA&gt; 6.16 #&gt; 3 Germany 1960 life expectancy 69.3 #&gt; 4 South Korea 1960 life expectancy 53.0 #&gt; 5 Germany 1961 fertility &lt;NA&gt; 2.44 #&gt; 6 South Korea 1961 fertility &lt;NA&gt; 5.99 #&gt; # ... with 218 more rows However, if we read the separate help file, we find that a better approach is to merge the last two variables when there is an extra separation: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;variable_name&quot;), extra = &quot;merge&quot;) #&gt; # A tibble: 224 x 4 #&gt; country year variable_name value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility 2.41 #&gt; 2 South Korea 1960 fertility 6.16 #&gt; 3 Germany 1960 life_expectancy 69.3 #&gt; 4 South Korea 1960 life_expectancy 53.0 #&gt; 5 Germany 1961 fertility 2.44 #&gt; 6 South Korea 1961 fertility 5.99 #&gt; # ... with 218 more rows This achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the spread function can do this: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;variable_name&quot;), extra = &quot;merge&quot;) %&gt;% spread(variable_name, value) #&gt; # A tibble: 112 x 4 #&gt; country year fertility life_expectancy #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 69.3 #&gt; 2 Germany 1961 2.44 69.8 #&gt; 3 Germany 1962 2.47 70.0 #&gt; 4 Germany 1963 2.49 70.1 #&gt; 5 Germany 1964 2.49 70.7 #&gt; 6 Germany 1965 2.48 70.6 #&gt; # ... with 106 more rows The data is now in tidy format with one row for each observation with three variables: year, fertility and life expectancy. 36.4 unite It is sometimes useful to do the inverse of separate, unite two columns into one. To demonstrate how to use unite, we show code that, although this is not an optimal approach, serves as an illustration. Suppose that we did not know about extra and used this command to separate: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;first_variable_name&quot;, &quot;second_variable_name&quot;), fill = &quot;right&quot;) #&gt; # A tibble: 224 x 5 #&gt; country year first_variable_name second_variable_name value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility &lt;NA&gt; 2.41 #&gt; 2 South Korea 1960 fertility &lt;NA&gt; 6.16 #&gt; 3 Germany 1960 life expectancy 69.3 #&gt; 4 South Korea 1960 life expectancy 53.0 #&gt; 5 Germany 1961 fertility &lt;NA&gt; 2.44 #&gt; 6 South Korea 1961 fertility &lt;NA&gt; 5.99 #&gt; # ... with 218 more rows We can achieve the same final result by uniting the second and third columns like this: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;first_variable_name&quot;, &quot;second_variable_name&quot;), fill = &quot;right&quot;) %&gt;% unite(variable_name, first_variable_name, second_variable_name, sep=&quot;_&quot;) #&gt; # A tibble: 224 x 4 #&gt; country year variable_name value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility_NA 2.41 #&gt; 2 South Korea 1960 fertility_NA 6.16 #&gt; 3 Germany 1960 life_expectancy 69.3 #&gt; 4 South Korea 1960 life_expectancy 53.0 #&gt; 5 Germany 1961 fertility_NA 2.44 #&gt; 6 South Korea 1961 fertility_NA 5.99 #&gt; # ... with 218 more rows Then spreading the columns: dat %&gt;% separate(key, c(&quot;year&quot;, &quot;first_variable_name&quot;, &quot;second_variable_name&quot;), fill = &quot;right&quot;) %&gt;% unite(variable_name, first_variable_name, second_variable_name, sep=&quot;_&quot;) %&gt;% spread(variable_name, value) %&gt;% rename(fertlity = fertility_NA) #&gt; # A tibble: 112 x 4 #&gt; country year fertlity life_expectancy #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 69.3 #&gt; 2 Germany 1961 2.44 69.8 #&gt; 3 Germany 1962 2.47 70.0 #&gt; 4 Germany 1963 2.49 70.1 #&gt; 5 Germany 1964 2.49 70.7 #&gt; 6 Germany 1965 2.48 70.6 #&gt; # ... with 106 more rows Exercises Run the following command to define the co2_wide object: co2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&gt;% setNames(1:12) %&gt;% mutate(year = as.character(1959:1997)) Use the gather function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy. Plot CO2 versus month with a different curve for each year using this code: co2_tidy %&gt;% ggplot(aes(month, co2, color = year)) + geom_line() If the expected plot is not made, it is probably because co2_tidy$month is not numeric: class(co2_tidy$month) Rewrite the call to gather using an argument that assures the month column will be numeric. Then make the plot. What do we learn from this plot? A. CO2 measures increase monotonically from 1959 to 1997. B. CO2 measures are higher in the summer and the yearly average increased from 1959 to 1997. C. CO2 measures appear constant and random variability explains the differences. D. CO2 measures do not have a seasonal trend. Now load the admissions data set which contains admission information for men and women across six majors and keep only the admitted percentage column: load(admissions) dat &lt;- admissions %&gt;% select(-applicants) If we think of an observation as a major, and that each observation has two variables, men admitted percentage and women admitted percentage, then this is not tidy. Use the spread function to wrangle into tidy shape: one row for each major. Now we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first gather to generate an intermediate data frame and then spread to obtain the tidy data we want. We will go step by step in this and the next two exercises. Use the gather function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns key and value. Now you have an object tmp with columns major, gender, key and value. Note that if you combine the key and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name. Now use the spread function to generate the tidy data with four variables for each major. Now use the pipe to write a line of code that turns admission to the table produced in the previous exercise. "],
["combining-tables.html", "Chapter 37 Combining tables 37.1 Joins 37.2 Binding 37.3 Set operators Exercises", " Chapter 37 Combining tables The information we need for a given analysis may not be just on one table. For example, when forecasting elections we used the function left_join to combine the information from two tables. Here we use a simpler example to illustrate the general challenge of combining tables. Suppose we want to explore the relationship between population size for US states and electoral votes. We have the population size in this table: data(murders) head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 and electoral votes in this one: data(polls_us_election_2016) head(results_us_election_2016) #&gt; state electoral_votes clinton trump others #&gt; 1 Alabama 9 34.4 62.1 3.6 #&gt; 2 Alaska 3 36.6 51.3 12.2 #&gt; 3 Arizona 11 45.1 48.7 6.2 #&gt; 4 Arkansas 6 33.7 60.6 5.8 #&gt; 5 California 55 61.7 31.6 6.7 #&gt; 6 Colorado 9 48.2 43.3 8.6 Just joining these two tables together will not work since the order of the states is not quite the same: identical(results_us_election_2016$state, murders$state) #&gt; [1] FALSE The join functions, described below, are designed to handle this challenge. 37.1 Joins The join functions in the dplyr package make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column so that the tables fit on the page): tab &lt;- left_join(murders, results_us_election_2016, by = &quot;state&quot;) %&gt;% select(-others) head(tab) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Alabama AL South 4779736 135 9 34.4 62.1 #&gt; 2 Alaska AK West 710231 19 3 36.6 51.3 #&gt; 3 Arizona AZ West 6392017 232 11 45.1 48.7 #&gt; 4 Arkansas AR South 2915918 93 6 33.7 60.6 #&gt; 5 California CA West 37253956 1257 55 61.7 31.6 #&gt; 6 Colorado CO West 5029196 65 9 48.2 43.3 The data has been successfully joined and we can now, for example, make a plot to explore the relationship: library(ggrepel) tab %&gt;% ggplot(aes(population/10^6, electoral_votes, label = abb)) + geom_point() + geom_text_repel() + scale_x_continuous(trans = &quot;log2&quot;) + scale_y_continuous(trans = &quot;log2&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) We see the relationship is close to linear with about 2 electoral votes for every million persons, but with smaller states getting higher ratios. In practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all: tab_1 &lt;- slice(murders, 1:6) %&gt;% select(state, population) tab_1 #&gt; state population #&gt; 1 Alabama 4779736 #&gt; 2 Alaska 710231 #&gt; 3 Arizona 6392017 #&gt; 4 Arkansas 2915918 #&gt; 5 California 37253956 #&gt; 6 Colorado 5029196 tab_2 &lt;- slice(results_us_election_2016, c(1:3, 5, 7:8)) %&gt;% select(state, electoral_votes) tab_2 #&gt; state electoral_votes #&gt; 1 Alabama 9 #&gt; 2 Alaska 3 #&gt; 3 Arizona 11 #&gt; 4 California 55 #&gt; 5 Connecticut 7 #&gt; 6 Delaware 3 We will use these two tables as examples in the next sections. 37.1.1 Left join Suppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left join with tab_1 as the first argument. left_join(tab_1, tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population electoral_votes #&gt; 1 Alabama 4779736 9 #&gt; 2 Alaska 710231 3 #&gt; 3 Arizona 6392017 11 #&gt; 4 Arkansas 2915918 NA #&gt; 5 California 37253956 55 #&gt; 6 Colorado 5029196 NA Note that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe: tab_1 %&gt;% left_join(tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population electoral_votes #&gt; 1 Alabama 4779736 9 #&gt; 2 Alaska 710231 3 #&gt; 3 Arizona 6392017 11 #&gt; 4 Arkansas 2915918 NA #&gt; 5 California 37253956 55 #&gt; 6 Colorado 5029196 NA 37.1.2 Right join If instead of a table like tab_1, we want one like tab_2, we can use right_join: tab_1 %&gt;% right_join(tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population electoral_votes #&gt; 1 Alabama 4779736 9 #&gt; 2 Alaska 710231 3 #&gt; 3 Arizona 6392017 11 #&gt; 4 California 37253956 55 #&gt; 5 Connecticut NA 7 #&gt; 6 Delaware NA 3 Now the NAs are in the column coming from tab_1. 37.1.3 Inner join If we want to keep only the rows that have information in both tables, we use inner join. You can think of this as an intersection: inner_join(tab_1, tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population electoral_votes #&gt; 1 Alabama 4779736 9 #&gt; 2 Alaska 710231 3 #&gt; 3 Arizona 6392017 11 #&gt; 4 California 37253956 55 37.1.4 Full join If we want to keep all the rows and fill the missing parts with NAs, we can use a full join. You can think of this as a union: full_join(tab_1, tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population electoral_votes #&gt; 1 Alabama 4779736 9 #&gt; 2 Alaska 710231 3 #&gt; 3 Arizona 6392017 11 #&gt; 4 Arkansas 2915918 NA #&gt; 5 California 37253956 55 #&gt; 6 Colorado 5029196 NA #&gt; 7 Connecticut NA 7 #&gt; 8 Delaware NA 3 37.1.5 Semi join The semi_join lets us keep the part of first table, for which we have information, in the second. It does not add the columns of the second: semi_join(tab_1, tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population #&gt; 1 Alabama 4779736 #&gt; 2 Alaska 710231 #&gt; 3 Arizona 6392017 #&gt; 4 California 37253956 37.1.6 Anti join The function anti_join is the opposite of semi_join. It keeps the elements of the first table, for which there is no information, in the second: anti_join(tab_1, tab_2) #&gt; Joining, by = &quot;state&quot; #&gt; state population #&gt; 1 Arkansas 2915918 #&gt; 2 Colorado 5029196 The following diagram summarizes the above joins: (Source: RStudio) 37.2 Binding Although we have yet to use it in this book, another common way in which datasets are combined is by binding them. Unlike the join function, the binding functions do no try to match by a variable, but instead simply combine datasets. If the datasets don’t match by the appropriate dimensions, one obtains an error. 37.2.1 Binding columns The dplyr function bind_cols binds two objects by making them columns in a tibble. For example, we quickly want to make a data frame consisting of numbers we can use. bind_cols(a = 1:3, b = 4:6) #&gt; # A tibble: 3 x 2 #&gt; a b #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 4 #&gt; 2 2 5 #&gt; 3 3 6 This function requires that we assign names to the columns. Here we chose a and b. Note that there is an R-base function cbind with the exact same functionality. An important difference is that cbind can create different types of objects, while bind_cols always produces a data frame. bind_cols can also bind two different data frames. For example, here we break up the tab data frame and then bind them back together: tab_1 &lt;- tab[, 1:3] tab_2 &lt;- tab[, 4:6] tab_3 &lt;- tab[, 7:8] new_tab &lt;- bind_cols(tab_1, tab_2, tab_3) head(new_tab) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Alabama AL South 4779736 135 9 34.4 62.1 #&gt; 2 Alaska AK West 710231 19 3 36.6 51.3 #&gt; 3 Arizona AZ West 6392017 232 11 45.1 48.7 #&gt; 4 Arkansas AR South 2915918 93 6 33.7 60.6 #&gt; 5 California CA West 37253956 1257 55 61.7 31.6 #&gt; 6 Colorado CO West 5029196 65 9 48.2 43.3 37.2.2 Binding by rows The bind_rows function is similar to bind_cols, but binds rows instead of columns: tab_1 &lt;- tab[1:2,] tab_2 &lt;- tab[3:4,] bind_rows(tab_1, tab_2) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Alabama AL South 4779736 135 9 34.4 62.1 #&gt; 2 Alaska AK West 710231 19 3 36.6 51.3 #&gt; 3 Arizona AZ West 6392017 232 11 45.1 48.7 #&gt; 4 Arkansas AR South 2915918 93 6 33.7 60.6 This is based on an R-base function rbind. 37.3 Set operators Another set of commands useful for combing datasets are the set operators. When applied to vectors, these behave as their names suggest, intersect and union. However, if the tidyverse or, more specifically, dplyr is loaded, these functions can be used on data frames as opposed to just on vectors. 37.3.1 Intersect You can take intersections of vectors of any type, such as numeric: intersect(1:10, 6:15) #&gt; [1] 6 7 8 9 10 or characters: intersect(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), c(&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)) #&gt; [1] &quot;b&quot; &quot;c&quot; The function dplyr also includes an intersect function that can be applied to tables that have the same column names. This function returns the rows in common between two tables. To make sure we use the dplyr version of interesect rather than the base package version we can use dplyr::intersect like this: tab_1 &lt;- tab[1:5,] tab_2 &lt;- tab[3:7,] dplyr::intersect(tab_1, tab_2) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Arizona AZ West 6392017 232 11 45.1 48.7 #&gt; 2 Arkansas AR South 2915918 93 6 33.7 60.6 #&gt; 3 California CA West 37253956 1257 55 61.7 31.6 37.3.2 Union Similarly union takes the union of vectors. For example: union(1:10, 6:15) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 union(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), c(&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; The dplyr package includes a version of union that combines all the rows of two tables with the same column names. tab_1 &lt;- tab[1:5,] tab_2 &lt;- tab[3:7,] dplyr::union(tab_1, tab_2) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Connecticut CT Northeast 3574097 97 7 54.6 40.9 #&gt; 2 Colorado CO West 5029196 65 9 48.2 43.3 #&gt; 3 California CA West 37253956 1257 55 61.7 31.6 #&gt; 4 Arkansas AR South 2915918 93 6 33.7 60.6 #&gt; 5 Arizona AZ West 6392017 232 11 45.1 48.7 #&gt; 6 Alaska AK West 710231 19 3 36.6 51.3 #&gt; 7 Alabama AL South 4779736 135 9 34.4 62.1 37.3.3 setdiff The set difference between a first and second argument can be obtained with setdiff. Not unlike instersect and union, this function is not symmetric: setdiff(1:10, 6:15) #&gt; [1] 1 2 3 4 5 setdiff(6:15, 1:10) #&gt; [1] 11 12 13 14 15 As for the function shown above, dplyr has a version for data frames: tab_1 &lt;- tab[1:5,] tab_2 &lt;- tab[3:7,] dplyr::setdiff(tab_1, tab_2) #&gt; state abb region population total electoral_votes clinton trump #&gt; 1 Alabama AL South 4779736 135 9 34.4 62.1 #&gt; 2 Alaska AK West 710231 19 3 36.6 51.3 37.3.4 setequal Finally, the function set_equal tells us if two sets are the same, regardless of order. So notice that: setequal(1:5, 1:6) #&gt; [1] FALSE but: setequal(1:5, 5:1) #&gt; [1] TRUE When applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different: dplyr::setequal(tab_1, tab_2) #&gt; FALSE: Rows in x but not y: 2, 1. Rows in y but not x: 5, 4. Exercises Install and load the Lahman library. This database includes data related to Baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players. Here is a data frame with the offensive statistics for all players in 2016. library(Lahman) Batting %&gt;% filter(yearID == 2016) %&gt;% as_tibble() You can see the top 10 hitters by running this code: top &lt;- Batting %&gt;% filter(yearID == 2016) %&gt;% arrange(desc(HR)) %&gt;% slice(1:10) top %&gt;% tbl_df But who are these players? We see an ID, but not the names. The player names are in this table Master %&gt;% as_tibble() We can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table. Now use the Salaries data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR and salary. In a previous exercise, we created a tidy version of the co2 dataset: co2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&gt;% setNames(1:12) %&gt;% mutate(year = 1959:1997) %&gt;% gather(month, co2, -year, convert = TRUE) We want to see if the monthly trend is changing so we are going to remove the year effects and the plot the data. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg. Now use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average. Make a plot of the seasonal trends by year but only after removing the year effect. "],
["data-import.html", "Chapter 38 Data import 38.1 Text versus binary files 38.2 Unicode versus ASCII 38.3 Importing spreadsheets 38.4 Paths and the Working Directory 38.5 The readr and readxl packages 38.6 R-base functions 38.7 Nuances Exercises", " Chapter 38 Data import 38.1 Text versus binary files For data science purposes, files can generally be classified into two categories: text files (also known as ASCII files) and binary files. You have already worked with text files. All your R scripts are text files and so are the R markdown files used to create this book. The csv tables you have read are also text files. One big advantage of these files is that we can easily “look” at them without having to purchase any kind of special software or follow complicated instructions. Any text editor can be used to examine a text file, including freely available editors such as RStudio, Notepad, textEdit, vi, emacs, nano, and pico. To see this, try opening a csv file using the “Open file” RStudio tool. You should be able to “see” the content right on your editor. However, if you try to open, say, an excel xls file, jpg or png file, you will not be able to see anything useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined. Although R includes tools for reading widely used binary files, such as xls files, in general you will want to find data sets stored in text files. Similarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). Extracting data from a spreadsheet stored as a text file is perhaps the easiest way to bring data from a file to an R session. Unfortunately, spreadsheets are not always available and the fact that you can look at text files does not necessarily imply that extracting data from them will be straightforward. Part of what we learn in this chapter is to extract data from more complex text files such as html files. 38.2 Unicode versus ASCII A challenge in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file: a Unicode text file. To understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s. ASCII is an encoding that maps characters to numbers. ASCII uses 7 bits (0s and 1s) which results in \\(2^7 = 128\\), enough to encode all the characters on an English language keyboard. However, other languages use characters not included in this encoding. For example, the é in México is not encoded by ASCII. For this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16 and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding. Although we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see “weird looking” characters you were not expecting. Here is an example. 38.3 Importing spreadsheets library(tidyverse) In the R chapter, we covered some of the basics of importing data. We described functions available in the default R installation. Here, we present a more general discussion and introduce the tidyverse packages readr and readxl. Currently, one of the most common ways of storing and sharing data for analysis is through electronic spreadsheets. A spreadsheet stores data in rows and columns. It is basically a file version of a data frame. When saving such a table to a computer file, one needs a way to define when a new row or column ends and the other begins. This in turn defines the cells in which single values are stored. When creating spreadsheets with text files, like the ones created with a simple text editor, a new row is defined with return and columns are separated with some predefined special character. The most common characters are comma (,), semicolon (;), white space (\\) and tab (\\ \\ \\ \\). Here is an example of what a comma separated file looks like if we open it with a basic text editor: The first row contains column names rather than data. We call this a header and when we read-in data from a spreadsheet it is important to know if the file has a header or not. Most reading functions assume there is a header. To know if the file has a header, it helps to look at the file before trying to read it. This can be done with a text editor or with RStudio. In RStudio, we can do this by either opening the file in the editor or navigating to the file location, double clicking on the file and hitting View File. However, not all spreadsheet files are in a text format. Google Sheets, which are rendered on a browser, are an example. Another example is the proprietary format used by Microsoft Excel. These can’t be viewed with a text editor. Given the widespread use of Microsoft Excel software, this format is widely used. Although there are R packages designed to read this format, if you are choosing a file format to save your own data, you generally want to avoid Microsoft Excel. We recommend Google Sheets as a free software tool for organizing data. We provide more recommendations in the section Data Organization with Spreadsheets. 38.4 Paths and the Working Directory We start by demonstrating how to read-in a file that is already saved on your computer. There are several ways to do this and we will discuss three of them. However, you only need to learn one to follow along. The first step is to find the file containing your data and know its location in your file system. When you are working in R, it is important to know your working directory. This is the directory in which R will save or look for files by default. You can see your working directory by typing: getwd() You can change your working directory using the function setwd. If you are using RStudio, you can change it by clicking on Session then Set Working Directory. One thing that file reading functions have in common is that, unless a full path is provided, they search for files in the working directory. For this reason, our recommended approach for beginners is that you create a directory for each analysis and keep the raw data files in that directory. To keep raw data files organized, we recommend creating a data directory especially when the project involves more than one data file. We provide more advice on how to keep files organized in the Productivity Tools chapter. Because you may not have a data file handy yet, we provide example data files in the dslabs package. Once you download and install the dslabs package, files will be in the external data (’extdata`) directory: system.file(&quot;extdata&quot;, package=&quot;dslabs&quot;) Note that the output of this function call will change depending on your operating system, how you installed R and the version of R. We therefore do not show the output of the call. But it will be consistent within your system and you will be able to see the files included in this directory using the function list.files: path &lt;- system.file(&quot;extdata&quot;, package=&quot;dslabs&quot;) list.files(path) #&gt; [1] &quot;fertility-two-countries-example.csv&quot; #&gt; [2] &quot;life-expectancy-and-fertility-two-countries-example.csv&quot; #&gt; [3] &quot;murders.csv&quot; #&gt; [4] &quot;olive.csv&quot; #&gt; [5] &quot;RD-Mortality-Report_2015-18-180531.pdf&quot; Now that we know the location of these files, we are ready to import them into R. To make the code simpler and following along easier, you can move this file to your working directory. You can do this through the file system directly, but you can also do it within R itself using the file.copy function. To do this, it helps to define a variable with the full path using the function file.path. Using paste is not recommended since Microsoft Windows and Macs/Linux/Unix use different slashes for the paths. The function file.path is aware of your system and chooses the correct slashes. Here is an example: filename &lt;- &quot;murders.csv&quot; fullpath &lt;- file.path(path, filename) We don’t show the result of this function call because it will be different on your computer. Run the command and notice all the folder names related to the location of this file in your system. You can now copy the file over to the working directory like this: file.copy(fullpath, getwd()) #&gt; [1] TRUE You can check if the file is now in your working directory using the file.exists function: file.exists(filename) #&gt; [1] TRUE 38.5 The readr and readxl packages Now we are ready to read-in the file. readr is the tidyverse library that includes functions for reading data stored in text file spreadsheets into R. The following functions are available to read-in spreadsheets: Function Format Typical suffix read_table white space separated values txt read_csv comma separated values csv read_csv2 semicolon separated values csv read_tsv tab delimited separated values tsv read_delim general text file format, must define delimiter txt The readxl package provides functions to read-in Microsoft Excel formats: Function Format Typical suffix read_excel auto detect the format xls, xlsx read_xls original format xls read_xlsx new format xlsx The Microsoft Excel formats permits you to have more than one spreadsheet in one file. These are referred to as sheets. The functions above read the first sheet by default, but the excel_sheets function gives us the names of the sheets in an excel file. These names can then be passed to the sheet argument in the three functions above to read sheets other than the first. Although the suffix usually tells us what type of file it is, there is no guarantee that these always match. We can open the file to take a look or use function read_lines to look at a few lines: read_lines(&quot;murders.csv&quot;, n_max = 3) #&gt; [1] &quot;state,abb,region,population,total&quot; &quot;Alabama,AL,South,4779736,135&quot; #&gt; [3] &quot;Alaska,AK,West,710231,19&quot; This also shows that there is a header. Now we are ready to read-in the data into R. From the suffix and the peek at the file, we know to use read_csv: dat &lt;- read_csv(filename) #&gt; Parsed with column specification: #&gt; cols( #&gt; state = col_character(), #&gt; abb = col_character(), #&gt; region = col_character(), #&gt; population = col_integer(), #&gt; total = col_integer() #&gt; ) we can also use the full path for the file: dat &lt;- read_csv(fullpath) Note that we receive a message letting us know what data types were used for each column. Also note that dat is a tibble not just a data frame. This is because read_csv is a tidyverse parser. We can see that the data has in fact been read-in with the content in the file: head(dat) #&gt; # A tibble: 6 x 5 #&gt; state abb region population total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 38.6 R-base functions R-base also provides import functions. These have similar names to those in the tidyverse, for example read.table, read.csv and read.delim. However, there are a couple of important differences. To show this we read-in the data with an R-base function: dat2 &lt;- read.csv(filename) One difference is that now we have a data frame not a tibble: class(dat2) #&gt; [1] &quot;data.frame&quot; The other difference is that the characters are converted to factors: class(dat2$abb) #&gt; [1] &quot;factor&quot; class(dat2$region) #&gt; [1] &quot;factor&quot; This can be avoided by setting the argument stringsAsFactors to FALSE. In our experience this can be a cause for confusion since a variable that was saved as characters in file is converted to factors regardless of what the variable represents. In fact, we highly recommend setting stringsAsFactors=FALSE to be your default approach when using the R-base parsers. 38.6.1 Downloading files Another common place for data to reside is on the internet. When these are data files, we can download them and then import them or even read them directly from the web. For example, we note that because our dslabs package is on GitHub, the file we downloaded with the package has a url: url &lt;- &quot;https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv&quot; The read_csv file can read these files directly: dat &lt;- read_csv(url) #&gt; Parsed with column specification: #&gt; cols( #&gt; state = col_character(), #&gt; abb = col_character(), #&gt; region = col_character(), #&gt; population = col_integer(), #&gt; total = col_integer() #&gt; ) If you want to have a local copy of the file, you can use the download.file function: download.file(url, &quot;murders.csv&quot;) Two functions that are sometimes useful when downloading data from the internet are tempdir and tempfile. The first creates a directory with a random name that is very likely to be unique. Similarly, tempfile creates a character string, not a file, that is likely to be a unique filename: tempfile() #&gt; [1] &quot;/var/folders/82/pnwl6fw15fsfzdjrft0l6c7h0000gn/T//RtmpCnc5nQ/file8e0d65ffda6b&quot; So you can run a command like this which erases the temporary file once it imports the data: tmp_filename &lt;- tempfile() download.file(url, tmp_filename) dat &lt;- read_csv(tmp_filename) file.remove(tmp_filename) head(dat) 38.7 Nuances When reading in spreadsheets many things can go wrong. The file might have a multiline header, be missing cells or it might use an unexpected encoding. We recommend you read this post. With experience you will learn how to deal with different challenges. Carefully reading the help files for the functions discussed here will be useful. Two other functions that are helpful are scan and readLines. With scan you can read-in each cell of a file. Here is an example: x &lt;- scan(filename, sep=&quot;,&quot;, what = &quot;c&quot;) x[1:10] #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; #&gt; [6] &quot;Alabama&quot; &quot;AL&quot; &quot;South&quot; &quot;4779736&quot; &quot;135&quot; Exercises Use the read_csv function to read each of the files that the following code saves in the files object: path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) files &lt;- list.files(path) files Note that the last one, the olive file, gives us a warning. This is because the first line of the file is missing the header for the first column. Read the help file for read.csv to figure out how to read in the file without reading this header. If you skip the header, you should not get this warning. Save the result to an object called dat A problem with the previous approach is that we don’t know what the columns represent. Type: names(dat) to see that the names are not informative. Use the readLines function to read in just the first line (we later learn how to extract values from the output) "],
["web-scraping.html", "Chapter 39 Web Scraping 39.1 HTML 39.2 The rvest package 39.3 CSS selectors 39.4 JSON Exercises", " Chapter 39 Web Scraping The data we need to answer a question is not always in a spreadsheet ready for us to read. For example, the US murders dataset we used in the R Basics chapter originally comes from this Wikipedia page: url &lt;- paste0(&quot;https://en.wikipedia.org/w/index.php?title=&quot;, &quot;Gun_violence_in_the_United_States_by_state&amp;oldid=843135608&quot;) You can see the data table when you visit the webpage: (Source: Wikipedia) Unfortunately, there is no link to a data file. To make the data frame that is loaded when we type data(murders), we had to do some web scraping. Web scraping, or web harvesting, are the terms we use to describe the process of extracting data from a website. The reason we can do this is because the information used by a browser to render webpages is received as a text file from a server. The text is code written in hyper text markup language (HTML). Every browser has a way to show the html source code for a page, each one different. On Chrome, you can use Control-U on a PC and command+alt+U on a Mac. 39.1 HTML Because this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data: p&gt;The 2015 U.S. population total was 320.9 million. The 2015 U.S. overall murder rate per 100,000 inhabitants was 4.89.&lt;/p&gt; &lt;h2&gt;&lt;span class=&quot;mw-headline&quot; id=&quot;States&quot;&gt;States&lt;/span&gt; &lt;span class=&quot;mw-editsection&quot;&gt;&lt;span class=&quot;mw-editsection-bracket&quot;&gt; [&lt;/span&gt; &lt;a href= &quot;/w/index.php?title=Murder_in_the_United_States_by_state&amp;amp;action=edit&amp;amp;section=1&quot; title=&quot;Edit section: States&quot;&gt; edit&lt;/a&gt;&lt;span class=&quot;mw-editsection-bracket&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; &lt;table class=&quot;wikitable sortable&quot;&gt; &lt;tr&gt; &lt;th&gt;State&lt;/th&gt; &lt;th&gt;&lt;a href=&quot;/wiki/List_of_U.S._states_and_territories_by_population&quot; title=&quot;List of U.S. states and territories by population&quot;&gt;Population&lt;/a&gt;&lt;br /&gt; &lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=&quot;cite_ref-1&quot; class=&quot;reference&quot;&gt; &lt;a href=&quot;#cite_note-1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt; &lt;th&gt;Murders and Nonnegligent &lt;p&gt;Manslaughter&lt;br /&gt; &lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=&quot;cite_ref-2&quot; class=&quot;reference&quot;&gt; &lt;a href=&quot;#cite_note-2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt;Murder and Nonnegligent &lt;p&gt;Manslaughter Rate&lt;br /&gt; &lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/wiki/Alabama&quot; title=&quot;Alabama&quot;&gt;Alabama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;4,853,875&lt;/td&gt; &lt;td&gt;348&lt;/td&gt; &lt;td&gt;7.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/wiki/Alaska&quot; title=&quot;Alaska&quot;&gt;Alaska&lt;/a&gt;&lt;/td&gt; &lt;td&gt;737,709&lt;/td&gt; &lt;td&gt;59&lt;/td&gt; &lt;td&gt;8.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; You can actually see the data! We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS). We say more about this in the CSS Selector Section. Although we provide tools that make it possible to scrape data without knowing HTML, for data scientists, it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase you work. There are plenty of online courses and tutorials for learning these. Two examples are Codeacademy and W3schools. 39.2 The rvest package The tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple: library(rvest) #&gt; Loading required package: xml2 #&gt; #&gt; Attaching package: &#39;rvest&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; pluck #&gt; The following object is masked from &#39;package:readr&#39;: #&gt; #&gt; guess_encoding h &lt;- read_html(url) Note that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is: class(h) #&gt; [1] &quot;xml_document&quot; &quot;xml_node&quot; The rvest package is actually more general; it handles XML documents. XML is a general markup language, that’s what the ML stand for, that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents. Now, how do we extract the table from the object h? If we print h, we don’t really see much: h #&gt; {xml_document} #&gt; &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset= ... #&gt; [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-sub ... When we know that the information is stored in an HTML table, you can see this in this line of the HTML code above &lt;table class=&quot;wikitable sortable&quot;&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use: tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) Now, instead of the entire webpage, we just have the html code for the tables in the page: tab #&gt; {xml_nodeset (4)} #&gt; [1] &lt;table id=&quot;revision-info&quot; class=&quot;plainlinks fmbox fmbox-warning&quot; rol ... #&gt; [2] &lt;table class=&quot;wikitable&quot;&gt;\\n&lt;caption&gt;Legend\\n&lt;/caption&gt;\\n&lt;tbody&gt;&lt;tr&gt;\\ ... #&gt; [3] &lt;table class=&quot;wikitable sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th data-sort-type= ... #&gt; [4] &lt;table class=&quot;nowraplinks hlist collapsible collapsed navbox-inner&quot; ... The table we are interested is the third one: tab[[3]] #&gt; {xml_node} #&gt; &lt;table class=&quot;wikitable sortable&quot;&gt; #&gt; [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th data-sort-type=&quot;text&quot;&gt;State\\n&lt;/th&gt;\\n&lt;th data-sort ... We are not quite there yet because this is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames: tab &lt;- tab[[3]] %&gt;% html_table class(tab) #&gt; [1] &quot;data.frame&quot; We are now much closer to having a usable data table: tab &lt;- tab %&gt;% setNames(c(&quot;state&quot;, &quot;population&quot;, &quot;total&quot;, &quot;murder_rate&quot;)) head(tab) #&gt; state population total murder_rate NA NA NA NA NA #&gt; 1 Alabama 4,853,875 348 3[a] 3[a] 48.9 7.2 0.1[a] 0.1[a] #&gt; 2 Alaska 737,709 59 57 39 61.7 8.0 7.7 5.3 #&gt; 3 Arizona 6,817,565 309 278 171 32.3 4.5 4.1 2.5 #&gt; 4 Arkansas 2,977,853 181 164 110 57.9 6.1 5.5 3.7 #&gt; 5 California 38,993,940 1,861 1,861 1,275 20.1 4.8 4.8 3.3 #&gt; 6 Colorado 5,448,819 176 176 115 34.3 3.2 3.2 2.1 We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites. 39.3 CSS selectors The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS, which is used to add style to webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more. If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the notes that define a particular piece of data. However, selector gadgets actually make this possible. SelectorGadget is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables, we highly recommend you install it if you are interested in scraping html pages. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including author Hadley Wickham’s vignetter, and these two tutorials based on the package vignette. 39.4 JSON Sharing data on the internet has become more and more common. Unfortunately, providers use different formats which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON files look more like the code you use to define a list. Here is an example of information stored in a JSON format: #&gt; #&gt; Attaching package: &#39;jsonlite&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; flatten #&gt; [ #&gt; { #&gt; &quot;name&quot;: &quot;Miguel&quot;, #&gt; &quot;student_id&quot;: 1, #&gt; &quot;exam_1&quot;: 85, #&gt; &quot;exam_2&quot;: 86 #&gt; }, #&gt; { #&gt; &quot;name&quot;: &quot;Sofia&quot;, #&gt; &quot;student_id&quot;: 2, #&gt; &quot;exam_1&quot;: 94, #&gt; &quot;exam_2&quot;: 93 #&gt; }, #&gt; { #&gt; &quot;name&quot;: &quot;Aya&quot;, #&gt; &quot;student_id&quot;: 3, #&gt; &quot;exam_1&quot;: 87, #&gt; &quot;exam_2&quot;: 88 #&gt; }, #&gt; { #&gt; &quot;name&quot;: &quot;Cheng&quot;, #&gt; &quot;student_id&quot;: 4, #&gt; &quot;exam_1&quot;: 90, #&gt; &quot;exam_2&quot;: 91 #&gt; } #&gt; ] The file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example: library(jsonlite) citi_bike &lt;- fromJSON(&quot;http://citibikenyc.com/stations/json&quot;) This downloads a list. The first argument tells you when you downloaded it: citi_bike$executionTime #&gt; [1] &quot;2018-11-11 11:37:55 PM&quot; and the second is a data table: head(citi_bike$stationBeanList) #&gt; id stationName availableDocks totalDocks latitude #&gt; 1 281 Grand Army Plaza &amp; Central Park S 43 57 40.8 #&gt; 2 72 W 52 St &amp; 11 Ave 33 39 40.8 #&gt; 3 79 Franklin St &amp; W Broadway 9 33 40.7 #&gt; 4 82 St James Pl &amp; Pearl St 15 27 40.7 #&gt; 5 83 Atlantic Ave &amp; Fort Greene Pl 0 0 40.7 #&gt; 6 119 Park Ave &amp; St Edwards St 14 19 40.7 #&gt; longitude statusValue statusKey availableBikes #&gt; 1 -74 In Service 1 9 #&gt; 2 -74 In Service 1 4 #&gt; 3 -74 In Service 1 22 #&gt; 4 -74 In Service 1 11 #&gt; 5 -74 Not In Service 3 0 #&gt; 6 -74 In Service 1 4 #&gt; stAddress1 stAddress2 city postalCode location #&gt; 1 Grand Army Plaza &amp; Central Park S #&gt; 2 W 52 St &amp; 11 Ave #&gt; 3 Franklin St &amp; W Broadway #&gt; 4 St James Pl &amp; Pearl St #&gt; 5 Atlantic Ave &amp; Fort Greene Pl #&gt; 6 Park Ave &amp; St Edwards St #&gt; altitude testStation lastCommunicationTime landMark #&gt; 1 FALSE 2018-11-11 11:35:32 PM #&gt; 2 FALSE 2018-11-11 11:37:42 PM #&gt; 3 FALSE 2018-11-11 11:35:31 PM #&gt; 4 FALSE 2018-11-11 11:34:33 PM #&gt; 5 FALSE 2018-10-09 09:05:09 AM #&gt; 6 FALSE 2018-11-11 11:36:59 PM You can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converging data into tables. For more flexibility, we recommend rjson. Exercises Visit the following web page: http://www.stevetheump.com/Payrolls.htm Notice there are several tables. Say we are interested in comparing the payrolls of teams across the years. The next few exercises take us through the steps needed to do this. Start by applying what you learned to read in the website into an object called h. Note that, although not very useful, we can actually see the content of the page by typing: html_text(h) The next step is to extract the tables. For this, we can use the html_nodes function. We learned that tables in html are associated with the table node. Use the html_nodes function and the table node to extract the first table. Store it in an object nodes. The html_nodes function returns a list of objects of class xml_node. We can see the content of each one using, for example, the html_text function. You can see the content for an arbitrarily picked component like this: html_text(nodes[[8]]) If the content of this object is an html table, we can use the html_table function to convert it to a data frame. Use the html_table function to convert the 8th entry of nodes into a table. Repeat the above for the first 4 components of nodes. Which of the following are payroll tables: A. All them. B. 1 C. 2 D. 2-4 Repeat the above for the first last 3 components of nodes. Which of the following is true: A. The last entry in nodes shows the average across all teams through time, not payroll per team. B. All three are payroll per team tables. C. All three are like the first entry, not a payroll table. D. All of the above. We have learned that the first and last entries of nodes are not payroll tables. Redefine nodes so that these two are removed. We saw in the previous analysis that the first table node is not actually a table. This happens sometimes in html because tables are used to make text look a certain way, as opposed to storing numeric values. Remove the first component and then use sappy and html_table to covert each node in nodes into a table. Note that in this case,sapply will return a list of tables. You can also use lapply to assure that a list is applied. Look through the resulting tables. Are they all the same? Could we just join them with row_bind? Create three tables, call them tab_1 and tab_2 using entries 10 and 19. Use a full_join function to combine these two tables. Before you do this you will have to fix the missing header problem. You will also need to make the names match. After joining the tables, you see several NAs. This is because some teams are in one table and not the other. Use the anti_join function to get a better idea of why this is happening. We see see that one of the problem is that Yankees are listed as both N.Y. Yankees and NY Yankees. In the next section, we will learn efficient approaches to fixing problems like this. Here we can do it “by hand” as follows: tab_1 &lt;- mutate(tab_1, Team = ifelse(Team == &quot;N.Y. Yankees&quot;, &quot;NY Yankees&quot;, Team)) Now join the tables and show only Oakland and the Yankees and the payroll columns. Advanced: extract the titles of the movies that won Best Picture from this website: https://m.imdb.com/chart/bestpicture/ "],
["string-processing.html", "Chapter 40 String Processing 40.1 Defining strings: single and double quotes and how to escape 40.2 The stringr package 40.3 Case study 1: US murders data 40.4 Case study 2: Reported heights 40.5 Regular expressions 40.6 Stringr package (continued) 40.7 Case study 2: Reported heights (continued) 40.8 separate 40.9 Case study 2: Reported heights (continued) 40.10 String splitting 40.11 Case study 3: Extracting tables from a PDF 40.12 Re-coding Exercises", " Chapter 40 String Processing library(tidyverse) library(rvest) One of the most common data wrangling challenges involves converting or extracting numeric data, contained in character strings, into the numeric representations required to make plots, summarize or fit models in R. Also common is processing unorganized text into meaningful variable names or categorical variables. In the Web Scraping Section, we encountered an example related to creating the murders dataset we explored in the R chapter. After successfully extracting the raw data from a webpage into a table: url &lt;- paste0(&quot;https://en.wikipedia.org/w/index.php?title=&quot;, &quot;Gun_violence_in_the_United_States_by_state&amp;direction=prev&amp;oldid=810166167&quot;) murders_raw &lt;- read_html(url) %&gt;% html_nodes(&quot;table&quot;) %&gt;% html_table() %&gt;% .[[2]] %&gt;% setNames(c(&quot;state&quot;, &quot;population&quot;, &quot;total&quot;, &quot;murder_rate&quot;)) head(murders_raw) #&gt; state population total murder_rate #&gt; 1 Alabama 4,853,875 348 7.2 #&gt; 2 Alaska 737,709 59 8.0 #&gt; 3 Arizona 6,817,565 309 4.5 #&gt; 4 Arkansas 2,977,853 181 6.1 #&gt; 5 California 38,993,940 1,861 4.8 #&gt; 6 Colorado 5,448,819 176 3.2 We realized that two of the columns that we expected to contain numbers actually contained characters: class(murders_raw$population) #&gt; [1] &quot;character&quot; class(murders_raw$total) #&gt; [1] &quot;character&quot; This is a very common occurrence when web scraping since webpages and other formal documents use commas in numbers to improve readability; for example 4,853,875 is easier to read than 4853875. Because this is such a common task, there is already a function, parse_number that readily does the conversion. However, many of the string processing challenges a data scientist faces are unique and often unexpected. It is therefore quite ambitious to write a comprehensive section on this topic. Here we use case studies that help us demonstrate how string processing is a powerful tool necessary for many data wrangling challenges. Specifically, we show the original raw data used to create the data frames we have studied in this book and describe the string processing that was needed. By going over these cases studies, we will cover some of the most common tasks in string processing including: removing unwanted characters from text extracting numeric values from text finding and replacing characters extracting specific parts of strings converting free form text to more uniform formats splitting strings into multiple values 40.1 Defining strings: single and double quotes and how to escape To define strings in R, we can use either double quotes: s &lt;- &quot;Hello!&quot; or single quotes: s &lt;- &#39;Hello!&#39; Make sure you choose the correct single quote since using the back quote will give you an error: s &lt;- `Hello` Now, what happens if our string includes double quotes? For example, if we want to write 10 inches like this 10&quot;? You can’t use: s &lt;- &quot;10&quot;&quot; because this is just the string 10 followed by a double quote. If you type this into R, you get an error because you have an unclosed double quote. To avoid this, we can use the single quotes: s &lt;- &#39;10&quot;&#39; In R, the function cat lets us see what the string actually looks like: cat(s) #&gt; 10&quot; Now, what if we want our string to be 5 feet written like this 5'? In this case, we can use the double quotes: s &lt;- &quot;5&#39;&quot; cat(s) #&gt; 5&#39; So we’ve learned how to write 5 feet and 10 inches separately, but what if we want to write them together to represent 5 feet and 10 inches like this 5'10&quot;? In this case, neither the single nor double quotes will work. This: s &lt;- &#39;5&#39;10&quot;&#39; closes the string after 5 and this: s &lt;- &quot;5&#39;10&quot;&quot; closes the string after 10. Keep in mind that if we type one of the above code snippets into R, it will get stuck waiting for you to close the open quote and you will have to escape with the esc button. In this situation, we need to escape the function of the quotes. This is done with the backslash \\. You can escape either character like this: s &lt;- &#39;5\\&#39;10&quot;&#39; cat(s) #&gt; 5&#39;10&quot; or: s &lt;- &quot;5&#39;10\\&quot;&quot; cat(s) #&gt; 5&#39;10&quot; Escaping character is something we often have to use when processing strings. 40.2 The stringr package In general, string processing involves a string and a pattern. In R, we usually store strings in a character vector such as murders$population. Above, we scraped the web to obtain the murders_raw data. The first three strings in this vector defined by the population variable are: murders_raw$population[1:3] #&gt; [1] &quot;4,853,875&quot; &quot;737,709&quot; &quot;6,817,565&quot; The usual coercion does not work here: as.numeric(murders_raw$population[1:3]) #&gt; Warning: NAs introduced by coercion #&gt; [1] NA NA NA This is because of the commas ,. The string processing we want to do here is remove the pattern , from the strings in murders_raw$population and then coerce to numbers. In general, string processing tasks can be divided into detecting, locating, extracting or replacing patterns in strings. In our example, we need to locate the , and replace them with the empty characters &quot;&quot;. Base R includes functions to perform all these tasks. However, they don’t follow a unifying convention which makes it a bit hard to memorize and use. The stringr package basically repackages this functionality, but uses a more consistent approach of naming functions and ordering their arguments. For example, in stringr, all the string processing functions start with str_. This means that if you type str_ and hit tab, R will auto-complete and show all the available functions. As a result, we don’t necessarily have to memorize all the function names. Another advantage is that the string is always the first argument, which means we can more easily use the pipe. We therefore will be focusing on the stringr package. However, because the R-base equivalents are so widely used, below we include a table describing the stringr functions and include R-base equivalent when available. 40.2.1 The str_ functions library(stringr) The table below includes the functions available to you in the stringr package. We split them by task. We also include the R-base equivalent when available. All these functions take a character vector as first argument. Also, for each function, operations are vectorized: the operation gets applied to each string in the vector. Finally, note that in this table we mention groups. These will be explained later. Task Description stringr R-base Detect Is the pattern in the string? str_detect grepl Detect Returns the index of entries that contain the pattern. str_which grep Detect Returns the subset of strings that contain the pattern. str_subset grep with value = TRUE Locate Returns positions of first occurrence of pattern in a string. str_locate regexpr Locate Returns position of all occurrences of pattern in a string. str_locate_all gregexpr Locate Show the first part of the string that matches pattern. str_view Locate Show me all the parts of the string that match the pattern. str_view_all Extract Extract the first part of the string that matches the pattern. str_extract Extract Extract all parts of the string that match the pattern. str_extract_all Extract Extract first part of the string that matches the groups and the patterns defined by the groups. str_match Extract Extract all parts of the string that matches the groups and the patterns defined by the groups. str_match_all Extract Extract a substring. str_sub substring Extract Split a string into a list with parts separated by pattern. str_split strsplit Extract Split a string into a matrix with parts separated by pattern. str_split_fixed strsplit with fixed = TRUE Describe Count number of times a pattern appears in a string str_count Describe Number of character in string. str_length nchar Replace Replace first part of a string matching a pattern with another. str_replace Replace Replace all parts of a string matching a pattern with another. str_replace_all gsub Replace Change all characters to upper case. str_to_upper toupper Replace Change all characters to lower case. str_to_lower tolower Replace Change first character to upper and rest to lower. str_to_title Replace Replace all NAs to a new value. str_replace_na Replace Remove white space from start and end of string. str_trim Manipulate Join multiple strings. str_c paste0 Manipulate Change the encoding of the string. str_conv Manipulate Sort the vector in alphabetical order. str_sort sort Manipulate Index needed to order the vector in alphabetical order. str_order order Manipulate Truncate a string to a fixed size. str_trunc Manipulate Add white space to string to make it a fixed size. str_pad Manipulate Repeat a string. str_dup rep then paste Manipulate Wrap things into formatted paragraphs. str_wrap Manipulate String interpolation. str_interp sprintf 40.3 Case study 1: US murders data Above we used used code to scrape the web and create the murders_raw table. We noted that three columns need to be parsed from characters into numbers, but that commas were making it hard. We can use the str_detect function to see that two of the three columns have commas in the entries: commas &lt;- function(x) any(str_detect(x, &quot;,&quot;)) murders_raw %&gt;% summarize_all(funs(commas)) #&gt; state population total murder_rate #&gt; 1 FALSE TRUE TRUE FALSE We can then use the str_replace_all function to remove them: test_1 &lt;- str_replace_all(murders_raw$population, &quot;,&quot;, &quot;&quot;) test_1 &lt;- as.numeric(test_1) We can then use mutate_all to apply this operation to each column, since it won’t affect the columns without commas. It turns out that this operation is so common, that readr includes the function parse_number specifically meant to remove non-numeric characters before coercing: test_2 &lt;- parse_number(murders_raw$population) identical(test_1, test_2) #&gt; [1] TRUE So we can obtain our desired table using: murders_new &lt;- murders_raw %&gt;% mutate_at(2:3, parse_number) murders_new %&gt;% head #&gt; state population total murder_rate #&gt; 1 Alabama 4853875 348 7.2 #&gt; 2 Alaska 737709 59 8.0 #&gt; 3 Arizona 6817565 309 4.5 #&gt; 4 Arkansas 2977853 181 6.1 #&gt; 5 California 38993940 1861 4.8 #&gt; 6 Colorado 5448819 176 3.2 This case is relatively simple compared to the string processing challenges that we typically face in data science. The next example is a rather complex one. 40.4 Case study 2: Reported heights The dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this: library(dslabs) data(reported_heights) These heights were obtained using a web form in which students were asked to enter their heights. They could enter anything, but the instructions asked for height in inches, a number. We compiled 1014 submissions, but unfortunately the column vector with the reported heights had several non-numeric entries and as a result became a character vector: class(reported_heights$height) #&gt; [1] &quot;character&quot; If we try to parse it into numbers, we get a warning: x &lt;- as.numeric(reported_heights$height) #&gt; Warning: NAs introduced by coercion Although most values appear to be height in inches as requested: head(x) #&gt; [1] 75 70 68 74 61 65 we do end up with many NAs: sum(is.na(x)) #&gt; [1] 81 We can see some of the entries that are not successfully converted by filter to keep only the entries resulting in NAs: reported_heights %&gt;% mutate(new_height = as.numeric(height)) %&gt;% filter(is.na(new_height)) %&gt;% head(n=10) #&gt; time_stamp sex height new_height #&gt; 1 2014-09-02 15:16:28 Male 5&#39; 4&quot; NA #&gt; 2 2014-09-02 15:16:37 Female 165cm NA #&gt; 3 2014-09-02 15:16:52 Male 5&#39;7 NA #&gt; 4 2014-09-02 15:16:56 Male &gt;9000 NA #&gt; 5 2014-09-02 15:16:56 Male 5&#39;7&quot; NA #&gt; 6 2014-09-02 15:17:09 Female 5&#39;3&quot; NA #&gt; 7 2014-09-02 15:18:00 Male 5 feet and 8.11 inches NA #&gt; 8 2014-09-02 15:19:48 Male 5&#39;11 NA #&gt; 9 2014-09-04 00:46:45 Male 5&#39;9&#39;&#39; NA #&gt; 10 2014-09-04 10:29:44 Male 5&#39;10&#39;&#39; NA We immediately see what is happening. Some of the students did not report their heights in inches as requested. We could discard these data and continue. However, many of the entries follow patterns that, in principle, we can easily convert to inches. For example, in the output above, we see various cases that use the format x'y'' with x and y representing feet and inches respectively. Each one of these cases can be read and converted to inches by a human, for example 5'4'' is 5*12 + 4 = 64. So we could fix all the problematic entries by hand. However, humans are prone to making mistakes. Also, because we plan on continuing to collect data, it will be convenient to write code that automatically does this. A first step in this type of task is to survey the problematic entries and try to define specific patterns followed by a large groups of entries. The larger these groups, the more entries we can fix with a single programmatic approach. We want to find patterns that can be accurately described with a rule, such as “a digit, followed by a feet symbol followed by one or two digits followed by an inches symbol”. To look for such patterns, it helps to remove the entries that are consistent with being in inches and to view only the problematic entries. We thus write a function to automatically do this. We keep entries that either result in NAs when applying as.numeric or are outside a range of plausible heights. We permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us. not_inches &lt;- function(x, smallest = 50, tallest = 84){ inches &lt;- suppressWarnings(as.numeric(x)) ind &lt;- is.na(inches) | inches &lt; smallest | inches &gt; tallest ind } We apply this function and find that there are: problems &lt;- reported_heights %&gt;% filter(not_inches(height)) %&gt;% .$height length(problems) #&gt; [1] 292 problematic entries! We can now view all the cases by simply printing them. We don’t do that here because there are 297, but after surveying them carefully, we see that three patterns can be used to define three large groups within these exceptions. A pattern of the form x'y or x' y'' or x'y\\&quot; with x and y representing feet and inches respectively. Here are ten examples: #&gt; 5&#39; 4&quot; 5&#39;7 5&#39;7&quot; 5&#39;3&quot; 5&#39;11 5&#39;9&#39;&#39; 5&#39;10&#39;&#39; 5&#39; 10 5&#39;5&quot; 5&#39;2&quot; A pattern of the form x.y or x,y with x feet and y inches. Here are ten examples: #&gt; 5.3 5.5 6.5 5.8 5.6 5,3 5.9 6,8 5.5 6.2 Entries that were reported in centimeters rather than inches. Here are ten examples: #&gt; 150 175 177 178 163 175 178 165 165 180 Once we see these large groups following specific patterns, we can develop a plan of attack. Remember that there is rarely just one way to perform these tasks. Here we pick one that helps us teach several useful techniques. But surely there is a more efficient way of performing the task. Plan of attack: we will convert entries fitting the first two patterns into a standardized one. We will then leverage the standardization to extract the feet and inches and convert to inches. We will then define a procedure for identifying entries that are in centimeters and convert them to inches. After applying these steps, we will then check again to see what entries were not fixed and see if we can tweak our approach to be more comprehensive. At the end, we hope to have a script that makes web-based data collection methods robust to the most common user mistakes. To achieve our goal, we will use a technique that enables us to accurately detect patterns and extract the parts we want: regular expressions (regex). 40.5 Regular expressions library(stringr) A regular expression (regex) is way to describe specific patterns of characters of text. They can be used to determine if a given string matches the pattern. A set of rules has been defined to do this efficiently and precisely and here we show some examples. We can learn more about these rules by reading a detailed tutorial such as this one or this one. This cheat sheet is also very useful. The patterns supplied to the stringr functions can be a regex rather than a standard string. We will learn how this works through a series of examples. 40.5.1 A string Technically any string is is a regex, perhaps the simplest example is a single character. So the comma , used here: pattern &lt;- &quot;,&quot; str_detect(murders_raw$total, pattern) #&gt; [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE #&gt; [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE #&gt; [45] FALSE FALSE FALSE FALSE FALSE FALSE FALSE is a simple example of searching with regex. Above, we noted that an entry included a cm. This is also a simple example of a regex. We can show all the entries that used cm like this: str_subset(reported_heights$height, &quot;cm&quot;) #&gt; [1] &quot;165cm&quot; &quot;170 cm&quot; 40.5.2 Special characters Now let’s consider a slightly more complicated example. Which of the following strings: yes &lt;- c(&quot;180 cm&quot;, &quot;70 inches&quot;) no &lt;- c(&quot;180&quot;, &quot;70&#39;&#39;&quot;) s &lt;- c(yes, no) contain the pattern cm or inches? We could call str_detect twice: str_detect(s, &quot;cm&quot;) | str_detect(s, &quot;inches&quot;) #&gt; [1] TRUE TRUE FALSE FALSE However, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if want to know if either cm or inches appears in the strings, we can use the regex cm|inches: str_detect(s, &quot;cm|inches&quot;) #&gt; [1] TRUE TRUE FALSE FALSE and obtain the correct answer. Another special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backlash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example: yes &lt;- c(&quot;5&quot;, &quot;6&quot;, &quot;5&#39;10&quot;, &quot;5 feet&quot;, &quot;4&#39;11&quot;) no &lt;- c(&quot;&quot;, &quot;.&quot;, &quot;Five&quot;, &quot;six&quot;) s &lt;- c(yes, no) pattern &lt;- &quot;\\\\d&quot; str_detect(s, pattern) #&gt; [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE We take this opportunity to introduce the str_view function, which is helpful for troubleshooting as it shows us the first match for each string: str_view(s, pattern) and str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three. str_view_all(s, pattern) There are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet mentioned earlier. 40.5.3 Testing Throughout this section you will see that we create strings to test our regex. To do this, we define patterns that we know should match and also patterns that we know should not. We will call them yes and no respectively. This permits us to check for the two types of errors: failing to match and incorrectly matching. 40.5.4 Character classes Character classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]: str_view(s, &quot;[56]&quot;) Suppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is [4-7]. yes &lt;- as.character(4:7) no &lt;- as.character(1:3) s &lt;- c(yes, no) str_detect(s, &quot;[4-7]&quot;) #&gt; [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE However, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 and the character 0. So [1-20] simply means the character class composed of 0, 1 and 2. Keep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lowercase letters as [a-z], uppercase letters as [A-Z], and [a-zA-z] as both. 40.5.5 Anchors What if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are ^ and $ which represent the beginning and end of a string respectively. So the pattern ^\\\\d$ is read as: start of the string followed by one digit followed by end of string. This pattern now only detects the strings with exactly one digit: pattern &lt;- &quot;^\\\\d$&quot; yes &lt;- c(&quot;1&quot;, &quot;5&quot;, &quot;9&quot;) no &lt;- c(&quot;12&quot;, &quot;123&quot;, &quot; 1&quot;, &quot;a4&quot;, &quot;b&quot;) s &lt;- c(yes, no) str_view(s, pattern) The 1 does not match because it does not start with the digit but rather with a space. 40.5.6 Quantifiers For the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate: the pattern for one or two digits is: pattern &lt;- &quot;^\\\\d{1,2}$&quot; yes &lt;- c(&quot;1&quot;, &quot;5&quot;, &quot;9&quot;, &quot;12&quot;) no &lt;- c(&quot;123&quot;, &quot;a4&quot;, &quot;b&quot;) str_view(c(yes, no), pattern) In this case, 123 does not match, but 12 does. So to look for our feet and inches pattern, we can add the symbols for feet ' and inches &quot; after the digits. With what we have learned, we can now construct an example for the pattern x'y\\&quot; with x feet and y inches. pattern &lt;- &quot;^[4-7]&#39;\\\\d{1,2}\\&quot;$&quot; The pattern is now getting complex, but you can look at it carefully and break it down: ^ = start of the string [4-7] = one digit, either 4,5,6 or 7 ' = feet symbol \\\\d{1,2} = one or two digits \\&quot; = inches symbol $ = end of the string Let’s test it out: yes &lt;- c(&quot;5&#39;7\\&quot;&quot;, &quot;6&#39;2\\&quot;&quot;, &quot;5&#39;12\\&quot;&quot;) no &lt;- c(&quot;6,2\\&quot;&quot;, &quot;6.2\\&quot;&quot;,&quot;I am 5&#39;11\\&quot;&quot;, &quot;3&#39;2\\&quot;&quot;, &quot;64&quot;) str_detect(yes, pattern) #&gt; [1] TRUE TRUE TRUE str_detect(no, pattern) #&gt; [1] FALSE FALSE FALSE FALSE FALSE For now, we are permitting the inches to be 12 or larger. We will add a restriction later as the regex for this is a bit more complex than we are ready to show. 40.5.7 Not To specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. For example, if we want to detect digits that are preceeded by anything except a letter we can do the following: pattern &lt;- &quot;[^a-zA-Z]\\\\d&quot; yes &lt;- c(&quot;.3&quot;, &quot;+2&quot;, &quot;-0&quot;,&quot;*4&quot;) no &lt;- c(&quot;A3&quot;, &quot;B2&quot;, &quot;C0&quot;, &quot;E4&quot;) str_detect(yes, pattern) #&gt; [1] TRUE TRUE TRUE TRUE str_detect(no, pattern) #&gt; [1] FALSE FALSE FALSE FALSE Another way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on. 40.5.8 Search and replace with regex Earlier we defined the object problems containing the strings that do not appear to be in inches. We can see that only: pattern &lt;- &quot;^[4-7]&#39;\\\\d{1,2}\\&quot;$&quot; sum(str_detect(problems, pattern)) #&gt; [1] 14 match the pattern we defined. To see why this is, we show some examples that expose why we don’t have more matches: problems[c(2, 10, 11, 12, 15)] %&gt;% str_view(pattern) An initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function: str_subset(problems, &quot;inches&quot;) #&gt; [1] &quot;5 feet and 8.11 inches&quot; &quot;Five foot eight inches&quot; #&gt; [3] &quot;5 feet 7inches&quot; &quot;5ft 9 inches&quot; #&gt; [5] &quot;5 ft 9 inches&quot; &quot;5 feet 6 inches&quot; We also see that some entries used two single quotes '' instead of a double quote &quot;&quot;. str_subset(problems, &quot;&#39;&#39;&quot;) #&gt; [1] &quot;5&#39;9&#39;&#39;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;3&#39;&#39;&quot; &quot;5&#39;7&#39;&#39;&quot; &quot;5&#39;6&#39;&#39;&quot; &quot;5&#39;7.5&#39;&#39;&quot; #&gt; [8] &quot;5&#39;7.5&#39;&#39;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;11&#39;&#39;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;5&#39;&#39;&quot; To correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly: pattern &lt;- &quot;^[4-7]&#39;\\\\d{1,2}$&quot; If we do this replacement before the matching, we get many more matches: problems %&gt;% str_replace(&quot;feet|ft|foot&quot;, &quot;&#39;&quot;) %&gt;% # replace feet, ft, foot with &#39; str_replace(&quot;inches|in|&#39;&#39;|\\&quot;&quot;, &quot;&quot;) %&gt;% # remove all inches symbols str_detect(pattern) %&gt;% sum #&gt; [1] 48 However, we still have many cases to go. Note that in the code above, we leveraged the stringr consistency and used the pipe. 40.5.9 White space \\s Another problem we have are spaces. For example, our pattern does not match 5' 4&quot; because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them: identical(&quot;Hi&quot;, &quot;Hi &quot;) #&gt; [1] FALSE In regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to: pattern_2 &lt;- &quot;^[4-7]&#39;\\\\s\\\\d{1,2}\\&quot;$&quot; str_subset(problems, pattern_2) #&gt; [1] &quot;5&#39; 4\\&quot;&quot; &quot;5&#39; 11\\&quot;&quot; &quot;5&#39; 7\\&quot;&quot; So do we need more than one regex pattern? It turns out we can use a quantifier for this as well. 40.5.10 Quantifiers: *, ?, + We want the pattern to permit spaces but not require them. Even if there are several spaces, like this 5' 4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example: yes &lt;- c(&quot;AB&quot;, &quot;A1B&quot;, &quot;A11B&quot;, &quot;A111B&quot;, &quot;A1111B&quot;) no &lt;- c(&quot;A2B&quot;, &quot;A21B&quot;) str_detect(yes, &quot;A1*B&quot;) #&gt; [1] TRUE TRUE TRUE TRUE TRUE str_detect(no, &quot;A1*B&quot;) #&gt; [1] FALSE FALSE The above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s. There are two other similar quantifiers. For none or once, we can use ? and, for one or more, we can use +. You can see how they differ: data.frame(string = c(&quot;AB&quot;, &quot;A1B&quot;, &quot;A11B&quot;, &quot;A111B&quot;, &quot;A1111B&quot;), none_or_more = str_detect(yes, &quot;A1*B&quot;), nore_or_once = str_detect(yes, &quot;A1?B&quot;), once_or_more = str_detect(yes, &quot;A1+B&quot;)) #&gt; string none_or_more nore_or_once once_or_more #&gt; 1 AB TRUE TRUE FALSE #&gt; 2 A1B TRUE TRUE TRUE #&gt; 3 A11B TRUE FALSE TRUE #&gt; 4 A111B TRUE FALSE TRUE #&gt; 5 A1111B TRUE FALSE TRUE We will actually use all three in our reported heights example, and we will see these in a later section. To improve our pattern, we can add \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries: pattern &lt;- &quot;^[4-7]\\\\s*&#39;\\\\s*\\\\d{1,2}$&quot; problems %&gt;% str_replace(&quot;feet|ft|foot&quot;, &quot;&#39;&quot;) %&gt;% # replace feet, ft, foot with &#39; str_replace(&quot;inches|in|&#39;&#39;|\\&quot;&quot;, &quot;&quot;) %&gt;% # remove all inches symbols str_detect(pattern) %&gt;% sum #&gt; [1] 53 We might be tempted to avoid doing this by removing all the spaces with str_replace_all. However, when doing such an operation we need to make sure that it does not have unintended effects. In our reported heights examples, this will be a problem because some entries are of the form x y with space separating the feet from the inches. If we remove all spaces, we will incorrectly turn x y into xy which implies that a 6 1 would become 61 instead of 73. 40.5.11 Groups The second large group of problematic entries were of the form x.y, x,y and x y. We want to change all these to our common format x'y. But we can’t just do a search and replace because we would change values such as 70.5 into 70'5. Our strategy will therefore be to search for a very specific pattern that assures us feet and inches are being provided and then, for those that match, replace appropriately. Groups are a powerful aspect of regex that permits the extraction values. Groups are defined using parentheses. They don’t affect the pattern matching per-se. Instead, it permits tools to identify specific parts of the pattern so we can extract them. We want to change heights written like 5.6 to 5'6. To avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*. Let’s start by defining a simple pattern that matches this: pattern_without_groups &lt;- &quot;^[4-7],\\\\d*$&quot; We want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses: pattern_with_groups &lt;- &quot;^([4-7]),(\\\\d*)$&quot; We encapsulate the part of the pattern that matches the parts we want. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups: yes &lt;- c(&quot;5,9&quot;, &quot;5,11&quot;, &quot;6,&quot;, &quot;6,1&quot;) no &lt;- c(&quot;5&#39;9&quot;, &quot;,&quot;, &quot;2,8&quot;, &quot;6.1.1&quot;) s &lt;- c(yes, no) str_detect(s, pattern_without_groups) #&gt; [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE str_detect(s, pattern_with_groups) #&gt; [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE Once we define groups, we can use the function str_match to extract the values these groups define: str_match(s, pattern_with_groups) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;5,9&quot; &quot;5&quot; &quot;9&quot; #&gt; [2,] &quot;5,11&quot; &quot;5&quot; &quot;11&quot; #&gt; [3,] &quot;6,&quot; &quot;6&quot; &quot;&quot; #&gt; [4,] &quot;6,1&quot; &quot;6&quot; &quot;1&quot; #&gt; [5,] NA NA NA #&gt; [6,] NA NA NA #&gt; [7,] NA NA NA #&gt; [8,] NA NA NA Notice that the second and third columns contains feet and inches respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA. Now we can understand the difference between the functions str_extract and str_match: str_extract extracts only strings that match a pattern, not the values defined by groups: str_extract(s, pattern_with_groups) #&gt; [1] &quot;5,9&quot; &quot;5,11&quot; &quot;6,&quot; &quot;6,1&quot; NA NA NA NA 40.5.12 Search and replace using groups Another powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing. The regex special character for the i-th groups is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits: pattern_with_groups &lt;- &quot;^([4-7]),(\\\\d*)$&quot; yes &lt;- c(&quot;5,9&quot;, &quot;5,11&quot;, &quot;6,&quot;, &quot;6,1&quot;) no &lt;- c(&quot;5&#39;9&quot;, &quot;,&quot;, &quot;2,8&quot;, &quot;6.1.1&quot;) s &lt;- c(yes, no) str_replace(s, pattern_with_groups, &quot;\\\\1&#39;\\\\2&quot;) #&gt; [1] &quot;5&#39;9&quot; &quot;5&#39;11&quot; &quot;6&#39;&quot; &quot;6&#39;1&quot; &quot;5&#39;9&quot; &quot;,&quot; &quot;2,8&quot; &quot;6.1.1&quot; We can use this to convert cases in our reported heights. We are now ready to define a pattern that helps us convert all the x.y, x,y and x y to our preferred format. We need to adapt pattern_with_groups to be bit more flexible and capture all the cases. pattern_with_groups &lt;-&quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$&quot; Let’s break this one down: - ^ = start of the string - [4-7] = one digit, either 4,5,6 or 7 - \\\\s* = none or more white space - [,\\\\.\\\\s+] = feet symbol is either ,, . or at least one space. - \\\\s* = none or more white space - \\\\d* = none or more digits - $ = end of the string We can see that it appears to be working: str_subset(problems, pattern_with_groups) %&gt;% head #&gt; [1] &quot;5.3&quot; &quot;5.25&quot; &quot;5.5&quot; &quot;6.5&quot; &quot;5.8&quot; &quot;5.6&quot; and will be able to perform the search and replace: str_subset(problems, pattern_with_groups) %&gt;% str_replace(pattern_with_groups, &quot;\\\\1&#39;\\\\2&quot;) %&gt;% head #&gt; [1] &quot;5&#39;3&quot; &quot;5&#39;25&quot; &quot;5&#39;5&quot; &quot;6&#39;5&quot; &quot;5&#39;8&quot; &quot;5&#39;6&quot; Again, we will deal with inches larger than twelve later. 40.5.13 Testing and improving We have developed a powerful string processing technique that can help us catch many of the problematic entries. Now it’s time to test our approach, search for further problems, and tweak our approach for possible improvements. Let’s write a function that captures all the entries that can’t be converted into numbers remembering that some are in centimeters (we will deal with those later): not_inches_or_cm &lt;- function(x, smallest = 50, tallest = 84){ inches &lt;- suppressWarnings(as.numeric(x)) ind &lt;- !is.na(inches) &amp; ((inches &gt;= smallest &amp; inches &lt;= tallest) | (inches/2.54 &gt;= smallest &amp; inches/2.54 &lt;= tallest)) !ind } problems &lt;- reported_heights %&gt;% filter(not_inches_or_cm(height)) %&gt;% .$height length(problems) #&gt; [1] 200 Let’s see how many of these fit our pattern after the processing steps we developed above: converted &lt;- problems %&gt;% str_replace(&quot;feet|foot|ft&quot;, &quot;&#39;&quot;) %&gt;% #convert feet symbols to &#39; str_replace(&quot;inches|in|&#39;&#39;|\\&quot;&quot;, &quot;&quot;) %&gt;% #remove inches symbols str_replace(&quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1&#39;\\\\2&quot;) ##change format pattern &lt;- &quot;^[4-7]\\\\s*&#39;\\\\s*\\\\d{1,2}$&quot; index &lt;- str_detect(converted, pattern) mean(index) #&gt; [1] 0.615 Note how we leveraged the pipe, one of the advantages of using stringr. This last piece of code shows that we have matched well over half of the strings. Let’s examine the remaining cases: converted[!index] #&gt; [1] &quot;6&quot; &quot;165cm&quot; &quot;511&quot; &quot;6&quot; #&gt; [5] &quot;2&quot; &quot;&gt;9000&quot; &quot;5 &#39; and 8.11 &quot; &quot;11111&quot; #&gt; [9] &quot;6&quot; &quot;103.2&quot; &quot;19&quot; &quot;5&quot; #&gt; [13] &quot;300&quot; &quot;6&#39;&quot; &quot;6&quot; &quot;Five &#39; eight &quot; #&gt; [17] &quot;7&quot; &quot;214&quot; &quot;6&quot; &quot;0.7&quot; #&gt; [21] &quot;6&quot; &quot;2&#39;33&quot; &quot;612&quot; &quot;1,70&quot; #&gt; [25] &quot;87&quot; &quot;5&#39;7.5&quot; &quot;5&#39;7.5&quot; &quot;111&quot; #&gt; [29] &quot;5&#39; 7.78&quot; &quot;12&quot; &quot;6&quot; &quot;yyy&quot; #&gt; [33] &quot;89&quot; &quot;34&quot; &quot;25&quot; &quot;6&quot; #&gt; [37] &quot;6&quot; &quot;22&quot; &quot;684&quot; &quot;6&quot; #&gt; [41] &quot;1&quot; &quot;1&quot; &quot;6*12&quot; &quot;87&quot; #&gt; [45] &quot;6&quot; &quot;1.6&quot; &quot;120&quot; &quot;120&quot; #&gt; [49] &quot;23&quot; &quot;1.7&quot; &quot;6&quot; &quot;5&quot; #&gt; [53] &quot;69&quot; &quot;5&#39; 9 &quot; &quot;5 &#39; 9 &quot; &quot;6&quot; #&gt; [57] &quot;6&quot; &quot;86&quot; &quot;708,661&quot; &quot;5 &#39; 6 &quot; #&gt; [61] &quot;6&quot; &quot;649,606&quot; &quot;10000&quot; &quot;1&quot; #&gt; [65] &quot;728,346&quot; &quot;0&quot; &quot;6&quot; &quot;6&quot; #&gt; [69] &quot;6&quot; &quot;100&quot; &quot;88&quot; &quot;6&quot; #&gt; [73] &quot;170 cm&quot; &quot;7,283,465&quot; &quot;5&quot; &quot;5&quot; #&gt; [77] &quot;34&quot; Four clear patterns arise and some other minor problems: Many students measuring exactly 5 or 6 feet did not enter any inches, for example 6', and our pattern requires that inches be included. Some students measuring exactly 5 or 6 feet entered just that number. Some of the inches were entered with decimal points. For example 5'7.5''. Our pattern only looks for two digits. Some entries have spaces at the end, for example 5 ' 9. Some entries are in meters and some of these use European decimals: 1.6, 1,7. Two students added cm. A student spelled out the numbers: Five foot eight inches. It is not necessarily clear that it is worth writing code to handle all these cases since they might be rare enough. However, some of them provide us with an opportunity to learn a few more regex techniques, so we will build a fix. 40.5.14 Using groups and quantifiers For case 1, if we add a '0 to first zero, for example, convert all 6 to 6'0, then our pattern will match. This can be done using groups: yes &lt;- c(&quot;5&quot;, &quot;6&quot;, &quot;5&quot;) no &lt;- c(&quot;5&#39;&quot;, &quot;5&#39;&#39;&quot;, &quot;5&#39;4&quot;) s &lt;- c(yes, no) str_replace(s, &quot;^([4-7])$&quot;, &quot;\\\\1&#39;0&quot;) #&gt; [1] &quot;5&#39;0&quot; &quot;6&#39;0&quot; &quot;5&#39;0&quot; &quot;5&#39;&quot; &quot;5&#39;&#39;&quot; &quot;5&#39;4&quot; The pattern says it has to start (^) followed by a digit between 4 and 7 and end there ($). The parenthesis defines the group that we pass as \\\\1 to generate the replacement regex string. We can adapt this code slightly to handle the case 2 as well, which covers the entry 5'. Note 5' is left untouched. This is because the extra ' makes the pattern not match since we have to end with a 5 or 6. We want to permit the 5 or 6 to be followed by 0 or 1 feet sign. So we can simply add '{0,1} after the ' to do this. However, we can use the none or once special character ?. As we saw above, this is different from * which is none or more. We now see that the fourth case is also converted: str_replace(s, &quot;^([56])&#39;?$&quot;, &quot;\\\\1&#39;0&quot;) #&gt; [1] &quot;5&#39;0&quot; &quot;6&#39;0&quot; &quot;5&#39;0&quot; &quot;5&#39;0&quot; &quot;5&#39;&#39;&quot; &quot;5&#39;4&quot; Here we only permit 5 and 6, but not 4 and 7. This is because 5 and 6 feet tall is quite common, so we assume those that typed 5 or 6 really meant 60 or 72 inches. However, 4 and 7 feet tall are so rare that, although we accept 84 as a valid entry, we assume 7 was entered in error. We can use quantifiers to deal with case 3. These entries are not matched because the inches include decimals and our pattern does not permit this. We need to allow the second group to include decimals not just digits. This means we must permit zero or one period . then zero or more digits. So we will be using both ? and *. Also remember that, for this particular case, the period needs to be escaped since it is a special character (it means any character except line break). Here is a simple example of how we can use *. So we can adapt our pattern, currently ^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$ to permit a decimal at the end: pattern &lt;- &quot;^[4-7]\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)$&quot; Case 4, meters using commas, we can approach similarly to how we converted the x.y to x'y. A difference is that we require that the first digit be 1 or 2: yes &lt;- c(&quot;1,7&quot;, &quot;1, 8&quot;, &quot;2, &quot; ) no &lt;- c(&quot;5,8&quot;, &quot;5,3,2&quot;, &quot;1.7&quot;) s &lt;- c(yes, no) str_replace(s, &quot;^([12])\\\\s*,\\\\s*(\\\\d*)$&quot;, &quot;\\\\1\\\\.\\\\2&quot;) #&gt; [1] &quot;1.7&quot; &quot;1.8&quot; &quot;2.&quot; &quot;5,8&quot; &quot;5,3,2&quot; &quot;1.7&quot; We will later check if the entries are meters using their numeric values. 40.6 Stringr package (continued) 40.6.1 Trimming In general, spaces at the start or end of the string are uninformative. These can be particularly deceptive because sometimes they can be hard to see: s &lt;- &quot;Hi &quot; cat(s) #&gt; Hi identical(s, &quot;Hi&quot;) #&gt; [1] FALSE This is a general enough problem that there is a function dedicated to removing them: str_trim. str_trim(&quot;5 &#39; 9 &quot;) #&gt; [1] &quot;5 &#39; 9&quot; 40.6.2 To uppercase and to lowercase One of the entries writes out numbers as words Five foot eight inches. Although not efficient, we could add 13 extra str_replace calls to convert zero to 0, one to 1, and so on. To avoid having to write two separate operations for Zero and zero, One and one, etc., we can use the str_to_lower function to make all works lowercase first: s &lt;- c(&quot;Five feet eight inches&quot;) str_to_lower(s) #&gt; [1] &quot;five feet eight inches&quot; We are now be ready to define a procedure that converts all the problematic cases to inches. 40.7 Case study 2: Reported heights (continued) We now put all this together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above. convert_format &lt;- function(s){ s %&gt;% str_replace(&quot;feet|foot|ft&quot;, &quot;&#39;&quot;) %&gt;% #convert feet symbols to &#39; str_replace_all(&quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;, &quot;&quot;) %&gt;% #remove inches and other symbols str_replace(&quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1&#39;\\\\2&quot;) %&gt;% #change x.y, x,y x y str_replace(&quot;^([56])&#39;?$&quot;, &quot;\\\\1&#39;0&quot;) %&gt;% #add 0 when to 5 or 6 str_replace(&quot;^([12])\\\\s*,\\\\s*(\\\\d*)$&quot;, &quot;\\\\1\\\\.\\\\2&quot;) %&gt;% #change european decimal str_trim() #remove extra space } We can also write a function that converts words to numbers: words_to_numbers &lt;- function(s){ str_to_lower(s) %&gt;% str_replace_all(&quot;zero&quot;, &quot;0&quot;) %&gt;% str_replace_all(&quot;one&quot;, &quot;1&quot;) %&gt;% str_replace_all(&quot;two&quot;, &quot;2&quot;) %&gt;% str_replace_all(&quot;three&quot;, &quot;3&quot;) %&gt;% str_replace_all(&quot;four&quot;, &quot;4&quot;) %&gt;% str_replace_all(&quot;five&quot;, &quot;5&quot;) %&gt;% str_replace_all(&quot;six&quot;, &quot;6&quot;) %&gt;% str_replace_all(&quot;seven&quot;, &quot;7&quot;) %&gt;% str_replace_all(&quot;eight&quot;, &quot;8&quot;) %&gt;% str_replace_all(&quot;nine&quot;, &quot;9&quot;) %&gt;% str_replace_all(&quot;ten&quot;, &quot;10&quot;) %&gt;% str_replace_all(&quot;eleven&quot;, &quot;11&quot;) } Now we can see which problematic entries remain: converted &lt;- problems %&gt;% words_to_numbers %&gt;% convert_format remaining_problems &lt;- converted[not_inches_or_cm(converted)] pattern &lt;- &quot;^[4-7]\\\\s*&#39;\\\\s*\\\\d+\\\\.?\\\\d*$&quot; index &lt;- str_detect(remaining_problems, pattern) remaining_problems[!index] #&gt; [1] &quot;511&quot; &quot;2&quot; &quot;&gt;9000&quot; &quot;11111&quot; &quot;103.2&quot; #&gt; [6] &quot;19&quot; &quot;300&quot; &quot;7&quot; &quot;214&quot; &quot;0.7&quot; #&gt; [11] &quot;2&#39;33&quot; &quot;612&quot; &quot;1.70&quot; &quot;87&quot; &quot;111&quot; #&gt; [16] &quot;12&quot; &quot;yyy&quot; &quot;89&quot; &quot;34&quot; &quot;25&quot; #&gt; [21] &quot;22&quot; &quot;684&quot; &quot;1&quot; &quot;1&quot; &quot;6*12&quot; #&gt; [26] &quot;87&quot; &quot;1.6&quot; &quot;120&quot; &quot;120&quot; &quot;23&quot; #&gt; [31] &quot;1.7&quot; &quot;86&quot; &quot;708,661&quot; &quot;649,606&quot; &quot;10000&quot; #&gt; [36] &quot;1&quot; &quot;728,346&quot; &quot;0&quot; &quot;100&quot; &quot;88&quot; #&gt; [41] &quot;7,283,465&quot; &quot;34&quot; they all seem to be cases that are impossible to fix. 40.8 separate In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate. If we have a simpler case like this: s &lt;- c(&quot;5&#39;10&quot;, &quot;6&#39;1&quot;) tab &lt;- data.frame(x = s) we have already learned to use the separate function: tab %&gt;% separate(x, c(&quot;feet&quot;, &quot;inches&quot;), sep = &quot;&#39;&quot;) #&gt; feet inches #&gt; 1 5 10 #&gt; 2 6 1 The extract function from the tidyr package lets us use regex groups to extract the desired values. Above we showed how to use separate to split the data as desired. Here is the equivalent code using extract: tab %&gt;% extract(x, c(&quot;feet&quot;, &quot;inches&quot;), regex = &quot;(\\\\d)&#39;(\\\\d{1,2})&quot;) #&gt; feet inches #&gt; 1 5 10 #&gt; 2 6 1 So why do we even need the new function extract? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define: s &lt;- c(&quot;5&#39;10&quot;, &quot;6&#39;1\\&quot;&quot;,&quot;5&#39;8inches&quot;) tab &lt;- data.frame(x = s) and we only want the numbers, separate fails: tab %&gt;% separate(x, c(&quot;feet&quot;,&quot;inches&quot;), sep = &quot;&#39;&quot;, fill = &quot;right&quot;) #&gt; feet inches #&gt; 1 5 10 #&gt; 2 6 1&quot; #&gt; 3 5 8inches However, we can use extract. The regex here is a bit more complicated since we have to permit ' with spaces and feet. We also do not want the &quot; included in the value, so we do not include that in the group: tab %&gt;% extract(x, c(&quot;feet&quot;, &quot;inches&quot;), regex = &quot;(\\\\d)&#39;(\\\\d{1,2})&quot;) #&gt; feet inches #&gt; 1 5 10 #&gt; 2 6 1 #&gt; 3 5 8 40.9 Case study 2: Reported heights (continued) We are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts. We start by cleaning up the height column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after. Now we are ready to wrangle our reported heights dataset: pattern &lt;- &quot;^([4-7])\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)$&quot; smallest &lt;- 50 tallest &lt;- 84 new_heights &lt;- reported_heights %&gt;% mutate(original = height, height = words_to_numbers(height) %&gt;% convert_format()) %&gt;% extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) %&gt;% mutate_at(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), as.numeric) %&gt;% mutate(guess = 12*feet + inches) %&gt;% mutate(height = case_when( !is.na(height) &amp; between(height, smallest, tallest) ~ height,#inches !is.na(height) &amp; between(height/2.54, smallest, tallest) ~ height/2.54,#centimeters !is.na(height) &amp; between(height*100/2.54, smallest, tallest) ~ height*100/2.54,#meters !is.na(guess) &amp; inches &lt; 12 &amp; between(guess, smallest, tallest) ~ guess,#feet&#39;inches TRUE ~ as.numeric(NA))) %&gt;% select(-guess) We can check all the entries we converted by typing: new_heights %&gt;% filter(not_inches(original)) %&gt;% select(original, height) %&gt;% arrange(height) %&gt;% View() A final observation is that if we look at the shortest students in our course: new_heights %&gt;% arrange(height) %&gt;% head(n=7) #&gt; time_stamp sex height feet inches original #&gt; 1 2017-07-04 01:30:25 Male 50.0 NA NA 50 #&gt; 2 2017-09-07 10:40:35 Male 50.0 NA NA 50 #&gt; 3 2014-09-02 15:18:30 Female 51.0 NA NA 51 #&gt; 4 2016-06-05 14:07:20 Female 52.0 NA NA 52 #&gt; 5 2016-06-05 14:07:38 Female 52.0 NA NA 52 #&gt; 6 2014-09-23 03:39:56 Female 53.0 NA NA 53 #&gt; 7 2015-01-07 08:57:29 Male 53.8 NA NA 53.77 We see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant 5'1, 5'2, 5'3, 5'4, and 5'5. Because we are not completely sure, we will leave them as reported. 40.10 String splitting Another very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function read_csv available to use. We instead have to read a csv file using the base R function readLines like this: filename &lt;- system.file(&quot;extdata/murders.csv&quot;, package = &quot;dslabs&quot;) lines &lt;- readLines(filename) This function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are: lines %&gt;% head() #&gt; [1] &quot;state,abb,region,population,total&quot; &quot;Alabama,AL,South,4779736,135&quot; #&gt; [3] &quot;Alaska,AK,West,710231,19&quot; &quot;Arizona,AZ,West,6392017,232&quot; #&gt; [5] &quot;Arkansas,AR,South,2915918,93&quot; &quot;California,CA,West,37253956,1257&quot; We want to extract the values that are separated by comma for each string in the vector. The command str_split does exactly this: x &lt;- str_split(lines, &quot;,&quot;) x %&gt;% head() #&gt; [[1]] #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Alabama&quot; &quot;AL&quot; &quot;South&quot; &quot;4779736&quot; &quot;135&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;Alaska&quot; &quot;AK&quot; &quot;West&quot; &quot;710231&quot; &quot;19&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;Arizona&quot; &quot;AZ&quot; &quot;West&quot; &quot;6392017&quot; &quot;232&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;Arkansas&quot; &quot;AR&quot; &quot;South&quot; &quot;2915918&quot; &quot;93&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;California&quot; &quot;CA&quot; &quot;West&quot; &quot;37253956&quot; &quot;1257&quot; Note that the first entry has the column names so we can separate that out: col_names &lt;- x[[1]] x &lt;- x[-1] To convert our list into a data frame, we can use a shortcut provided by the map functions in the purrr package. The map function applies the same function to each element in a list. So if we want to extract the first entry of each element in x, we can write: library(purrr) map(x, function(y) y[1]) %&gt;% head() #&gt; [[1]] #&gt; [1] &quot;Alabama&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Alaska&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;Arizona&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;Arkansas&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;California&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;Colorado&quot; However, because this is such a common task, purrr provides a shortcut. If the second argument receives an integer instead of a function, it assumes we want that entry: map(x, 1) %&gt;% head() #&gt; [[1]] #&gt; [1] &quot;Alabama&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Alaska&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;Arizona&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;Arkansas&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;California&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;Colorado&quot; To force map to return a character vector instead of a list, we can use map_chr. Similarly map_int returns integers. So to create our data frame, we can use: dat &lt;- data.frame(map_chr(x, 1), map_chr(x, 2), map_chr(x, 3), map_chr(x, 4), map_chr(x, 5)) %&gt;% mutate_all(parse_guess) %&gt;% setNames(col_names) dat %&gt;% head #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 If you learn more about the purrr package, you will be able to write the code above more efficiently: dat &lt;- x %&gt;% transpose() %&gt;% map( ~ parse_guess(unlist(.))) %&gt;% setNames(col_names) %&gt;% as.data.frame() It turns out that we can avoid all the work shown above after the call to str_split. Specifically, if we know that the data we are extracting can be represented as a table, we can use the argument simplify=TRUE and str_split return a matrix instead of a list: x &lt;- str_split(lines, &quot;,&quot;, simplify = TRUE) col_names &lt;- x[1,] x &lt;- x[-1,] x %&gt;% as_data_frame() %&gt;% setNames(col_names) %&gt;% mutate_all(parse_guess) #&gt; # A tibble: 51 x 5 #&gt; state abb region population total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 #&gt; # ... with 45 more rows 40.11 Case study 3: Extracting tables from a PDF One of the datasets provided in dslabs shows scientific funding rates by gender in the Netherlands: library(dslabs) data(&quot;research_funding_rates&quot;) research_funding_rates %&gt;% select(&quot;discipline&quot;, &quot;success_rates_men&quot;, &quot;success_rates_women&quot;) #&gt; discipline success_rates_men success_rates_women #&gt; 1 Chemical sciences 26.5 25.6 #&gt; 2 Physical sciences 19.3 23.1 #&gt; 3 Physics 26.9 22.2 #&gt; 4 Humanities 14.3 19.3 #&gt; 5 Technical sciences 15.9 21.0 #&gt; 6 Interdisciplinary 11.4 21.8 #&gt; 7 Earth/life sciences 24.4 14.3 #&gt; 8 Social sciences 15.3 11.5 #&gt; 9 Medical sciences 18.8 11.2 The data comes from a paper published in the prestigious journal PNAS. However, the data is not provided in a spreadsheet, it is in a table in a PDF document: (Source: Romy van der Lee and Naomi Ellemers, PNAS 2015 112 (40) 12349-12353) We could extract the number by hand, but this could lead to human error. Instead, we can try to wrangle the data using R. We start by downloading the pdf document, then importing into R: library(&quot;pdftools&quot;) temp_file &lt;- tempfile() url &lt;- paste0(&quot;http://www.pnas.org/content/suppl/2015/09/16/&quot;, &quot;1510159112.DCSupplemental/pnas.201510159SI.pdf&quot;) download.file(url, temp_file) txt &lt;- pdf_text(temp_file) file.remove(temp_file) If we examine the object text, we notice that it is a character vector with an entry for each page. So we keep the page we want: raw_data_research_funding_rates &lt;- txt[2] The steps above can actually be skipped because we include this raw data in the dslabs package as well: data(&quot;raw_data_research_funding_rates&quot;) Examining the object raw_data_research_funding_rates we see that it is a long string and each line on the page, including the table rows, are separated by the symbol for newline: \\n. We therefore can create a list with the lines of the text as elements as follows: tab &lt;- str_split(raw_data_research_funding_rates, &quot;\\n&quot;) Because we start off with just one element in the string, we end up with a list with just one entry. tab &lt;- tab[[1]] By examining tab we see that the information for the column names is the third and fourth entries: the_names_1 &lt;- tab[3] the_names_2 &lt;- tab[4] In this table, the column information is spread across two lines. the_names_1 %&gt;% str_trim() %&gt;% cat() #&gt; Applications, n Awards, n Success rates, % We want to create one vector with one name for each column. Using some of the functions we have just learned, we do this. Let’s start with the first line shown above. We want to remove the leading space and anything following the comma. We use regex for the latter. Then we can obtain the elements by splitting strings separated by space. We want to split only when there are 2 or more spaces to avoid splitting success rate. So we use the regex \\\\s{2,} the_names_1 &lt;- the_names_1 %&gt;% str_trim() %&gt;% str_replace_all(&quot;,\\\\s.&quot;, &quot;&quot;) %&gt;% str_split(&quot;\\\\s{2,}&quot;, simplify = TRUE) the_names_1 #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;Applications&quot; &quot;Awards&quot; &quot;Success rates&quot; Now we will look at the second line: the_names_2 %&gt;% str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) #&gt; [1] &quot; Discipline Total Men Women Total Men Women Total Men Women&quot; Here we want to trim the leading space and then split by space as we did for the first line: the_names_2 &lt;- the_names_2 %&gt;% str_trim() %&gt;% str_split(&quot;\\\\s+&quot;, simplify = TRUE) the_names_2 #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] &quot;Discipline&quot; &quot;Total&quot; &quot;Men&quot; &quot;Women&quot; &quot;Total&quot; &quot;Men&quot; &quot;Women&quot; &quot;Total&quot; #&gt; [,9] [,10] #&gt; [1,] &quot;Men&quot; &quot;Women&quot; We can then join these to generate one name for each column: tmp_names &lt;- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = &quot;_&quot;) the_names &lt;- c(the_names_2[1], tmp_names) %&gt;% str_to_lower() %&gt;% str_replace_all(&quot;\\\\s&quot;, &quot;_&quot;) the_names #&gt; [1] &quot;discipline&quot; &quot;applications_total&quot; &quot;applications_men&quot; #&gt; [4] &quot;applications_women&quot; &quot;awards_total&quot; &quot;awards_men&quot; #&gt; [7] &quot;awards_women&quot; &quot;success_rates_total&quot; &quot;success_rates_men&quot; #&gt; [10] &quot;success_rates_women&quot; Now we are ready to get the actual data. By examining the tab object, we notice that the information is in lines 6 through 14. We can use str_split again to achieve our goal: new_research_funding_rates &lt;- tab[6:14] %&gt;% str_trim %&gt;% str_split(&quot;\\\\s{2,}&quot;, simplify = TRUE) %&gt;% data.frame(stringsAsFactors = FALSE) %&gt;% setNames(the_names) %&gt;% mutate_at(-1, parse_number) new_research_funding_rates %&gt;% head() #&gt; discipline applications_total applications_men #&gt; 1 Chemical sciences 122 83 #&gt; 2 Physical sciences 174 135 #&gt; 3 Physics 76 67 #&gt; 4 Humanities 396 230 #&gt; 5 Technical sciences 251 189 #&gt; 6 Interdisciplinary 183 105 #&gt; applications_women awards_total awards_men awards_women #&gt; 1 39 32 22 10 #&gt; 2 39 35 26 9 #&gt; 3 9 20 18 2 #&gt; 4 166 65 33 32 #&gt; 5 62 43 30 13 #&gt; 6 78 29 12 17 #&gt; success_rates_total success_rates_men success_rates_women #&gt; 1 26.2 26.5 25.6 #&gt; 2 20.1 19.3 23.1 #&gt; 3 26.3 26.9 22.2 #&gt; 4 16.4 14.3 19.3 #&gt; 5 17.1 15.9 21.0 #&gt; 6 15.8 11.4 21.8 We can see that the objects are identical: identical(research_funding_rates, new_research_funding_rates) #&gt; [1] TRUE 40.12 Re-coding Another common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with case_when although the tidyverse offers options that are specifically designed for this task: the recode function. Here is an example that shows how to rename countries with long names: library(dslabs) data(&quot;gapminder&quot;) Suppose we want to show life expectancy time series by country for the Caribbean: gapminder %&gt;% filter(region==&quot;Caribbean&quot;) %&gt;% ggplot(aes(year, life_expectancy, color = country)) + geom_line() The plot is what we want, but much of the space is wasted to accommodate some of the long country names: gapminder %&gt;% filter(region==&quot;Caribbean&quot;) %&gt;% filter(str_length(country) &gt;= 12) %&gt;% distinct(country) #&gt; country #&gt; 1 Antigua and Barbuda #&gt; 2 Dominican Republic #&gt; 3 St. Vincent and the Grenadines #&gt; 4 Trinidad and Tobago We have four countries with names longer than 12 characters. These names appear once for each year in the Gapminder dataset. Once we pick nicknames, we need to change them all consistently. The recode function can be used to do this: gapminder %&gt;% filter(region==&quot;Caribbean&quot;) %&gt;% mutate(country = recode(country, `Antigua and Barbuda` = &quot;Barbuda&quot;, `Dominican Republic` = &quot;DR&quot;, `St. Vincent and the Grenadines` = &quot;St. Vincent&quot;, `Trinidad and Tobago` = &quot;Trinidad&quot;)) %&gt;% ggplot(aes(year, life_expectancy, color = country)) + geom_line() There are other similar functions in other R packages, such as recode_factor and fct_recoder in the forcats package. Exercises Complete all lessons and exercises in the https://regexone.com/ online interactive tutorial. In the extdata directory of the dslabs package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this: fn &lt;- system.file(&quot;extdata&quot;, &quot;RD-Mortality-Report_2015-18-180531.pdf&quot;, package=&quot;dslabs&quot;) Find and open the file or open it directly from RStudio. On a Mac, you can type: system2(&quot;open&quot;, args = fn) and on Windows, you can type: system(&quot;cmd.exe&quot;, input = paste(&quot;start&quot;, fn)) Which of the following best describes this file: A. It is a table. Extracting the data will be easy. B. It is a report written in prose. Extracting the data will be impossible. C. It is a report combining graphs and tables. Extracting the data seems possible. D. It shows graphs of the data. Extracting the data will be difficult. We are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day and deaths. Start by installing and loading the pdftools package: install.packages(&quot;pdftools&quot;) library(pdftools) Now read-in fn using the pdf_text function, store the results in an object called txt. Describe what you see in txt. A. A table with the mortality data. B. A character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere. C. A character string with one entry containing all the information in the PDF file. D. An html document. Extract the ninth page of the PDF file from the object txt, then use the str_split from the stringr package so that you have each line in a different entry. Call this string vector s. Then look at the result and choose the one that best describes what you see. A. It is an empty string. B. I can see the figure shown in page 1. C. It is a tidy table. D. I can see the table! But there is a bunch of other stuff we need to get rid of. What kind of object is s and how many entries does it have? We see that the output is a list with one component. Redefine s to be the first entry of the list. What kind of object is s and how many entries does it have? When inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them. We learned about the command str_trim that removes spaces at the start or end of the strings. Use this function to trim s. We want to extract the numbers from the strings stored in s. However, there a lot of non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation. Use the str_which function to find the rows with a header. Save these results to header_index. Hint: find the first string that matches the pattern 2015 using the str_which function. Now we are going to define two objects: month will store the month and header will store the column names. Identify which row contains the header of the table. Save the content of the row into an object called header, then use str_split to help define the two objects we need. Hints: the separator here is one or more spaces. Also, consider using the simplify argument. Notice that towards the end of the page you see a totals row followed by rows with other summary statistics. Create an object called tail_index with the index of the totals entry. Because our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the str_count function to create an object n with the number of numbers in each each row. Hint: you can write a regex for number like this \\\\d+. We are now ready to remove entries from rows that we know we don’t need. The entry header_index and everything before it should be removed. Entries for which n is 1 should also be removed, and the entry tail_index and everything that comes after it should be removed as well. Now we are ready to remove all the non-numeric entries. Do this using regex and the str_remove_all function. Hint: in regex, using the ^ inside the [] means not, like the ! means not in !=. To define the regex pattern to catch all non-numbers, you can type [^\\\\d]. But remember you also want to keep spaces. To convert the strings into a table, use the str_split_fixed function. Convert s into a data matrix with just the day and death count data. Hints: note that the separator is one or more spaces. Make the argument n a value that limits the number of columns to the values in the 4 columns and the last column captures all the extra stuff. Then keep only the first four columns. Now you are almost ready to finish. Add column names to the matrix, including one called day. Also, add a column with the month. Call the resulting object dat. Finally, make sure the day is an integer not a character. Hint: use only the first five columns. Now finish it up by tidying tab with the gather function. Make a plot of deaths versus day with color to denote year. Exclude 2018 since we have no data. Now that we have wrangled this data step-by-step, put it all together in one R chunk, using the pipe as much as possible. Hint: first define the indexes, then write one line of code that does all the string processing. Advanced: let’s return to the MLB Payroll example from the web scraping section. Use what you have learned in the web scraping and string processing chapters to extract the payroll for the New York Yankees, Boston Red Sox, and Oakland A’s and plot them as a function of time. "],
["parsing-dates-and-times.html", "Chapter 41 Parsing Dates and Times 41.1 The date data type 41.2 The lubridate package Exercises", " Chapter 41 Parsing Dates and Times 41.1 The date data type We have described three main types of vectors: numeric, character, and logical. In data science projects, we very often encounter variables that are dates. Although we can represent a date with a string, for example November 2, 2017, once we pick a reference day, referred to as the epoch, they can be converted to numbers as well. Computer languages usually use January 1, 1970, as the epoch. So November 2, 2017, is day 17,204. Now how should we represent dates and times when analyzing data in R? We could just use days since the epoch, but then it is almost impossible to interpret. If I tell you it’s November 2, 2017, you know what this means immediately. If I tell you it’s day 17,204, you will be quite confused. Similar problems arise with times and even more complications can appear due to time zones. For this reason, R defines a data type just for dates and times. We saw an example in the polls data: library(dslabs) data(&quot;polls_us_election_2016&quot;) polls_us_election_2016$startdate %&gt;% head #&gt; [1] &quot;2016-11-03&quot; &quot;2016-11-01&quot; &quot;2016-11-02&quot; &quot;2016-11-04&quot; &quot;2016-11-03&quot; #&gt; [6] &quot;2016-11-03&quot; These look like strings, but they are not: class(polls_us_election_2016$startdate) #&gt; [1] &quot;Date&quot; Look at what happens when we convert them to numbers: as.numeric(polls_us_election_2016$startdate) %&gt;% head #&gt; [1] 17108 17106 17107 17109 17108 17108 It turns them into dates since the epoch. Plotting functions, such as those in ggplot, are aware of the date format. This means that, for example, a scatterplot can use the numeric representation to decide on the position of the point, but include the string in the labels: polls_us_election_2016 %&gt;% filter(pollster == &quot;Ipsos&quot; &amp; state ==&quot;U.S.&quot;) %&gt;% ggplot(aes(startdate, rawpoll_trump)) + geom_line() Note in particular that the months are displayed. 41.2 The lubridate package The tidyverse includes functionality for dealing with dates through the lubridate package. library(lubridate) #&gt; #&gt; Attaching package: &#39;lubridate&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; date We will take a random sample of dates to show some of the useful things one can do: set.seed(2) dates &lt;- sample(polls_us_election_2016$startdate, 10) %&gt;% sort dates #&gt; [1] &quot;2015-11-09&quot; &quot;2016-02-15&quot; &quot;2016-07-09&quot; &quot;2016-08-12&quot; &quot;2016-08-17&quot; #&gt; [6] &quot;2016-09-18&quot; &quot;2016-10-04&quot; &quot;2016-10-10&quot; &quot;2016-10-18&quot; &quot;2016-10-25&quot; The functions year, month and day extract those values: data.frame(date = dates, month = month(dates), day = day(dates), year = year(dates)) #&gt; date month day year #&gt; 1 2015-11-09 11 9 2015 #&gt; 2 2016-02-15 2 15 2016 #&gt; 3 2016-07-09 7 9 2016 #&gt; 4 2016-08-12 8 12 2016 #&gt; 5 2016-08-17 8 17 2016 #&gt; 6 2016-09-18 9 18 2016 #&gt; 7 2016-10-04 10 4 2016 #&gt; 8 2016-10-10 10 10 2016 #&gt; 9 2016-10-18 10 18 2016 #&gt; 10 2016-10-25 10 25 2016 We can also extract the month labels: month(dates, label = TRUE) #&gt; [1] Nov Feb Jul Aug Aug Sep Oct Oct Oct Oct #&gt; 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec Another useful set of functions are the parsers that converts strings into dates. x &lt;- c(20090101, &quot;2009-01-02&quot;, &quot;2009 01 03&quot;, &quot;2009-1-4&quot;, &quot;2009-1, 5&quot;, &quot;Created on 2009 1 6&quot;, &quot;200901 !!! 07&quot;) ymd(x) #&gt; [1] &quot;2009-01-01&quot; &quot;2009-01-02&quot; &quot;2009-01-03&quot; &quot;2009-01-04&quot; &quot;2009-01-05&quot; #&gt; [6] &quot;2009-01-06&quot; &quot;2009-01-07&quot; A further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits) and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format. What if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002. In these cases, examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can use the many parses provided by lubridate. For example, if the string is: x &lt;- &quot;09/01/02&quot; The ymd function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to: ymd(x) #&gt; [1] &quot;2009-01-02&quot; The mdy function assumes the first entry is the month, then the day, then the year: mdy(x) #&gt; [1] &quot;2002-09-01&quot; The lubridate package provides a function for every possibility: ydm(x) #&gt; [1] &quot;2009-02-01&quot; myd(x) #&gt; [1] &quot;2001-09-02&quot; dmy(x) #&gt; [1] &quot;2002-01-09&quot; dym(x) #&gt; [1] &quot;2001-02-09&quot; The lubridate package is also useful for dealing with times. In R, you can get the current time typing Sys.time(). The lubridate package provides a slightly more advanced function, now, that permits you to define the time zone: now() #&gt; [1] &quot;2018-11-11 23:38:11 EST&quot; now(&quot;GMT&quot;) #&gt; [1] &quot;2018-11-12 04:38:11 GMT&quot; You can see all the available times zones with OlsonNames() function. The lubridate package also has functions to extract hours, minutes and seconds: now() %&gt;% hour() #&gt; [1] 23 now() %&gt;% minute() #&gt; [1] 38 now() %&gt;% second() #&gt; [1] 11.2 as a function to convert parse strings into times: x &lt;- c(&quot;12:34:56&quot;) hms(x) #&gt; [1] &quot;12H 34M 56S&quot; as well as parses for time objects that include dates: x &lt;- &quot;Nov/2/2012 12:34:56&quot; mdy_hms(x) #&gt; [1] &quot;2012-11-02 12:34:56 UTC&quot; This package has many other useful functions. Two particular ones not described above are make_date and round_date. Exercises In the previous exercise section, we wrangled data from a PDF file containing vital statistics from Puerto Rico. We did this for the month of September. Below we include code that does it for all 12 months. library(tidyverse) library(purrr) library(pdftools) fn &lt;- system.file(&quot;extdata&quot;, &quot;RD-Mortality-Report_2015-18-180531.pdf&quot;, package=&quot;dslabs&quot;) tab &lt;- map_df(str_split(pdf_text(fn), &quot;\\n&quot;), function(s){ s &lt;- str_trim(s) header_index &lt;- str_which(s, &quot;2015&quot;)[1] tmp &lt;- str_split(s[header_index], &quot;\\\\s+&quot;, simplify = TRUE) month &lt;- tmp[1] header &lt;- tmp[-1] tail_index &lt;- str_which(s, &quot;Total&quot;) n &lt;- str_count(s, &quot;\\\\d+&quot;) out &lt;- c(1:header_index, which(n==1), which(n&gt;=28), tail_index:length(s)) s[-out] %&gt;% str_remove_all(&quot;[^\\\\d\\\\s]&quot;) %&gt;% str_trim() %&gt;% str_split_fixed(&quot;\\\\s+&quot;, n = 6) %&gt;% .[,1:5] %&gt;% as_data_frame() %&gt;% setNames(c(&quot;day&quot;, header)) %&gt;% mutate(month = month, day = as.numeric(day)) %&gt;% gather(year, deaths, -c(day, month)) %&gt;% mutate(deaths = as.numeric(deaths)) }) We want to make a plot of death counts versus date. A first step is to convert the month variable from characters to numbers. Note that the month abbreviations are in Spanglish. Use the recode function to convert months to numbers and redefine tab. Create a new column date with the date for each observation. Hint: use the make_date function. Plot deaths versus date. Note that after May 31, 2018, the deaths are all 0. The data is probably not entered yet. We also see a drop off starting around May 1. Redefine tab to exclude observations taken on or after May 1, 2018. Then, remake the plot. Remake the plot above but this time plot deaths against the day of the year, e.g. Jan 12, 2016 and Jan 12, 2017 are both day 12. Use color to denote the different years. Hint: use the lubridate function yday. Remake the plot above but, this time, use two different colors for before and after September 20, 2017. Advanced: remake the plot above, but this time show the month in the x-axis. Hint: create a variable with the date for a given year. Then use the scale_x_date function to show just the months. Remake the deaths versus day but with weekly averages. Hint: use the function round_date. Remake the plot but with monthly averages. Hint: use the function round_date again. "],
["text-mining.html", "Chapter 42 Text mining 42.1 Case study: Trump tweets 42.2 Text as data 42.3 Sentiment analysis Exercises", " Chapter 42 Text mining With the exception of labels used to represent categorical data, we have focused on numerical data. But in many applications, data starts as text. Well known examples are spam filtering, cyber-crime prevention, counter-terrorism and sentiment analysis. In all these cases, the raw data is composed of free form texts. Our task is to extract insights from these data. In this section, we learn how to generate useful numerical summaries from text data to which we can apply some of the powerful data visualization and analysis techniques we have learned. 42.1 Case study: Trump tweets During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted about Trump that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” Data scientist David Robinson conducted an analysis to determine if data supported this assertion. Here, we go through David’s analysis to learn some of the basics of text mining. To learn more about text mining in R, we recommend this book. We will use the following libraries: library(lubridate) library(tidyr) library(scales) In general, we can extract data directly from twitter using the rtweet package. However, in this case, a group has already compiled data for us and made it available at http://www.trumptwitterarchive.com. We can get the data from their JSON API using a script like this: url &lt;- &#39;http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json&#39; trump_tweets &lt;- map(2009:2017, ~sprintf(url, .x)) %&gt;% map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %&gt;% filter(!is_retweet &amp; !str_detect(text, &#39;^&quot;&#39;)) %&gt;% mutate(created_at = parse_date_time(created_at, orders = &quot;a b! d! H!:M!:S! z!* Y!&quot;, tz=&quot;EST&quot;)) For convenience, we include the result of the code above in the dslabs package: library(dslabs) data(&quot;trump_tweets&quot;) You can see the data frame with information about the tweets by typing head(trump_tweets) with the following variables included: names(trump_tweets) #&gt; [1] &quot;source&quot; &quot;id_str&quot; #&gt; [3] &quot;text&quot; &quot;created_at&quot; #&gt; [5] &quot;retweet_count&quot; &quot;in_reply_to_user_id_str&quot; #&gt; [7] &quot;favorite_count&quot; &quot;is_retweet&quot; The help file ?trump_tweets provides details on what each variable represents. The tweets are represented by the text variable: trump_tweets$text[16413] %&gt;% str_wrap() %&gt;% cat #&gt; Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in Davenport- #&gt; this past winter. #MAGA https://t.co/A5IF0QHnic and the source variable tells us which device was used to compose and upload each tweet: trump_tweets %&gt;% count(source) %&gt;% arrange(desc(n)) #&gt; # A tibble: 19 x 2 #&gt; source n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Twitter Web Client 10718 #&gt; 2 Twitter for Android 4652 #&gt; 3 Twitter for iPhone 3962 #&gt; 4 TweetDeck 468 #&gt; 5 TwitLonger Beta 288 #&gt; 6 Instagram 133 #&gt; # ... with 13 more rows To start, we will use extract to remove the Twitter for part of the source and filter out retweets. trump_tweets %&gt;% extract(source, &quot;source&quot;, &quot;Twitter for (.*)&quot;) %&gt;% count(source) #&gt; # A tibble: 6 x 2 #&gt; source n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Android 4652 #&gt; 2 BlackBerry 78 #&gt; 3 iPad 39 #&gt; 4 iPhone 3962 #&gt; 5 Websites 1 #&gt; 6 &lt;NA&gt; 12029 We are interested in what happened during the campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. We define the following table containing just the tweets from that time period: campaign_tweets &lt;- trump_tweets %&gt;% extract(source, &quot;source&quot;, &quot;Twitter for (.*)&quot;) %&gt;% filter(source %in% c(&quot;Android&quot;, &quot;iPhone&quot;) &amp; created_at &gt;= ymd(&quot;2015-06-17&quot;) &amp; created_at &lt; ymd(&quot;2016-11-08&quot;)) %&gt;% filter(!is_retweet) %&gt;% arrange(created_at) We can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device: ds_theme_set() campaign_tweets %&gt;% mutate(hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;% count(source, hour) %&gt;% group_by(source) %&gt;% mutate(percent = n / sum(n)) %&gt;% ungroup %&gt;% ggplot(aes(hour, percent, color = source)) + geom_line() + geom_point() + scale_y_continuous(labels = percent_format()) + labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;) We notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. There seems to be a clear difference in these patterns. We will therefore assume that two different entities are using these two devices. We will now study how the tweets differ when we compare Android to iPhone. To do this, we introduce the tidytext package. 42.2 Text as data The tidytext package helps us convert free form text into a tidy table. Having the data in this format greatly facilitates data visualization and the use of statistical techniques. library(tidytext) The main function needed to achieve this is unnest_tokens. A token refers to the units that we are considering to be a data point. The most common token will be words, but they can also be single characters, ngrams, sentences, lines or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example: poem &lt;- c(&quot;Roses are red,&quot;, &quot;Violets are blue,&quot;, &quot;Sugar is sweet,&quot;, &quot;And so are you.&quot;) example &lt;- data_frame(line = c(1, 2, 3, 4), text = poem) example #&gt; # A tibble: 4 x 2 #&gt; line text #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 Roses are red, #&gt; 2 2 Violets are blue, #&gt; 3 3 Sugar is sweet, #&gt; 4 4 And so are you. example %&gt;% unnest_tokens(word, text) #&gt; # A tibble: 13 x 2 #&gt; line word #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 roses #&gt; 2 1 are #&gt; 3 1 red #&gt; 4 2 violets #&gt; 5 2 are #&gt; 6 2 blue #&gt; # ... with 7 more rows Now let’s look at an example from the tweets. We will look at tweet number 3008 because it will later permit us to illustrate a couple of points: i &lt;- 3008 campaign_tweets$text[i] %&gt;% str_wrap() %&gt;% cat() #&gt; Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in Davenport- #&gt; this past winter. #MAGA https://t.co/A5IF0QHnic campaign_tweets[i,] %&gt;% unnest_tokens(word, text) %&gt;% select(word) #&gt; word #&gt; 3008 great #&gt; 3008.1 to #&gt; 3008.2 be #&gt; 3008.3 back #&gt; 3008.4 in #&gt; 3008.5 iowa #&gt; 3008.6 tbt #&gt; 3008.7 with #&gt; 3008.8 jerryjrfalwell #&gt; 3008.9 joining #&gt; 3008.10 me #&gt; 3008.11 in #&gt; 3008.12 davenport #&gt; 3008.13 this #&gt; 3008.14 past #&gt; 3008.15 winter #&gt; 3008.16 maga #&gt; 3008.17 https #&gt; 3008.18 t.co #&gt; 3008.19 a5if0qhnic Note that the function tries to convert tokens into words. To do this, however, it strips characters that are important in the context of twitter. Namely, the function removes all the # and @. A token in the context of twitter is not the same as in the context of spoken or written English. For this reason, instead of using the default, words, we define a regex that captures twitter characters. This may appear complex, but all we are defining is a pattern that starts with @, # or neither, and is followed by any combination of letters or digits: pattern &lt;- &quot;([^A-Za-z\\\\d#@&#39;]|&#39;(?![A-Za-z\\\\d#@]))&quot; We can now use the unnest_tokens function with the regex option and appropriately extract the hashtags and mentions. We demonstrate with our example tweet: campaign_tweets[i,] %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = pattern) %&gt;% select(word) #&gt; word #&gt; 1 great #&gt; 2 to #&gt; 3 be #&gt; 4 back #&gt; 5 in #&gt; 6 iowa #&gt; 7 #tbt #&gt; 8 with #&gt; 9 @jerryjrfalwell #&gt; 10 joining #&gt; 11 me #&gt; 12 in #&gt; 13 davenport #&gt; 14 this #&gt; 15 past #&gt; 16 winter #&gt; 17 #maga #&gt; 18 https #&gt; 19 t #&gt; 20 co #&gt; 21 a5if0qhnic Another minor adjustment we want to make is to remove the links to pictures: campaign_tweets[i,] %&gt;% mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\\\d]+|&amp;amp;&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = pattern) %&gt;% select(word) #&gt; word #&gt; 1 great #&gt; 2 to #&gt; 3 be #&gt; 4 back #&gt; 5 in #&gt; 6 iowa #&gt; 7 #tbt #&gt; 8 with #&gt; 9 @jerryjrfalwell #&gt; 10 joining #&gt; 11 me #&gt; 12 in #&gt; 13 davenport #&gt; 14 this #&gt; 15 past #&gt; 16 winter #&gt; 17 #maga Now we are now ready to extract the words for all our tweets. tweet_words &lt;- campaign_tweets %&gt;% mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\\\d]+|&amp;amp;&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = pattern) And we can now answer questions such as “what are the most commonly used words?”: tweet_words %&gt;% count(word) %&gt;% arrange(desc(n)) #&gt; # A tibble: 6,211 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 the 2335 #&gt; 2 to 1413 #&gt; 3 and 1246 #&gt; 4 a 1210 #&gt; 5 in 1189 #&gt; 6 i 1161 #&gt; # ... with 6,205 more rows It is not surprising that these are the top words. The top words are not informative. The tidytext package has a database of these commonly used words, referred to as stop words, in text mining: stop_words #&gt; # A tibble: 1,149 x 2 #&gt; word lexicon #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a SMART #&gt; 2 a&#39;s SMART #&gt; 3 able SMART #&gt; 4 about SMART #&gt; 5 above SMART #&gt; 6 according SMART #&gt; # ... with 1,143 more rows If we filter out rows representing stop words with filter(!word %in% stop_words$word): tweet_words &lt;- campaign_tweets %&gt;% mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\\\d]+|&amp;amp;&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = pattern) %&gt;% filter(!word %in% stop_words$word ) we end up with a much more informative set of top 10 tweeted words: tweet_words %&gt;% count(word) %&gt;% top_n(10, n) %&gt;% mutate(word = reorder(word, n)) %&gt;% arrange(desc(n)) #&gt; # A tibble: 10 x 2 #&gt; word n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 #trump2016 415 #&gt; 2 hillary 407 #&gt; 3 people 303 #&gt; 4 #makeamericagreatagain 296 #&gt; 5 america 254 #&gt; 6 clinton 239 #&gt; # ... with 4 more rows Some exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years, for example). We want to remove these and we can find them using the regex ^\\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it is at the start of a word so we will just str_replace. We add these two lines to the code above to generate our final table: tweet_words &lt;- campaign_tweets %&gt;% mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\\\d]+|&amp;amp;&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = pattern) %&gt;% filter(!word %in% stop_words$word &amp; !str_detect(word, &quot;^\\\\d+$&quot;)) %&gt;% mutate(word = str_replace(word, &quot;^&#39;&quot;, &quot;&quot;)) Now that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone. For each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We previously introduced the odds ratio as a summary statistic useful for quantifying these differences. For each device and a given word, let’s call it y, we compute the odds or the ratio between the proportion of words that are y and not y and compute the ratio of those odds. Here we will have many proportions that are 0, so we use the 0.5 correction described in the Association Test section. android_iphone_or &lt;- tweet_words %&gt;% count(word, source) %&gt;% spread(source, n, fill = 0) %&gt;% mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5))) android_iphone_or %&gt;% arrange(desc(or)) #&gt; # A tibble: 5,509 x 4 #&gt; word Android iPhone or #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mails 22 0 39.3 #&gt; 2 poor 13 0 23.6 #&gt; 3 poorly 12 0 21.8 #&gt; 4 @cbsnews 11 0 20.1 #&gt; 5 bosses 11 0 20.1 #&gt; 6 turnberry 11 0 20.1 #&gt; # ... with 5,503 more rows android_iphone_or %&gt;% arrange(or) #&gt; # A tibble: 5,509 x 4 #&gt; word Android iPhone or #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 #makeamericagreatagain 0 296 0.00144 #&gt; 2 #americafirst 0 71 0.00607 #&gt; 3 #draintheswamp 0 63 0.00683 #&gt; 4 #trump2016 3 412 0.00718 #&gt; 5 #votetrump 0 56 0.00769 #&gt; 6 join 1 157 0.00821 #&gt; # ... with 5,503 more rows Given that several of these words are overall low frequency words, we can impose a filter based on the total frequency like this: android_iphone_or %&gt;% filter(Android+iPhone &gt; 100) %&gt;% arrange(desc(or)) #&gt; # A tibble: 30 x 4 #&gt; word Android iPhone or #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 @cnn 104 18 4.95 #&gt; 2 bad 104 26 3.45 #&gt; 3 crooked 157 49 2.79 #&gt; 4 ted 85 28 2.62 #&gt; 5 interviewed 76 25 2.62 #&gt; 6 media 77 26 2.56 #&gt; # ... with 24 more rows android_iphone_or %&gt;% filter(Android+iPhone &gt; 100) %&gt;% arrange(or) #&gt; # A tibble: 30 x 4 #&gt; word Android iPhone or #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 #makeamericagreatagain 0 296 0.00144 #&gt; 2 #trump2016 3 412 0.00718 #&gt; 3 join 1 157 0.00821 #&gt; 4 tomorrow 25 101 0.218 #&gt; 5 vote 46 67 0.600 #&gt; 6 america 114 141 0.703 #&gt; # ... with 24 more rows We already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri’s assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as anger, fear, joy and surprise. In the next section, we demonstrate basic sentiment analysis. 42.3 Sentiment analysis In sentiment analysis, we assign a word to one or more “sentiments”. Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights. The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments: table(sentiments$lexicon) #&gt; #&gt; AFINN bing loughran nrc #&gt; 2476 6788 4149 13901 The bing lexicon divides words into positive and negative sentiments. We can see this using the tidytext function get_sentiments: get_sentiments(&quot;bing&quot;) #&gt; # A tibble: 6,788 x 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2-faced negative #&gt; 2 2-faces negative #&gt; 3 a+ positive #&gt; 4 abnormal negative #&gt; 5 abolish negative #&gt; 6 abominable negative #&gt; # ... with 6,782 more rows The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive. get_sentiments(&quot;afinn&quot;) #&gt; # A tibble: 2,476 x 2 #&gt; word score #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 abandon -2 #&gt; 2 abandoned -2 #&gt; 3 abandons -2 #&gt; 4 abducted -2 #&gt; 5 abduction -2 #&gt; 6 abductions -2 #&gt; # ... with 2,470 more rows The loughran and nrc lexicons provide several different sentiments: get_sentiments(&quot;loughran&quot;) %&gt;% count(sentiment) #&gt; # A tibble: 6 x 2 #&gt; sentiment n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 constraining 184 #&gt; 2 litigious 903 #&gt; 3 negative 2355 #&gt; 4 positive 354 #&gt; 5 superfluous 56 #&gt; 6 uncertainty 297 get_sentiments(&quot;nrc&quot;) %&gt;% count(sentiment) #&gt; # A tibble: 10 x 2 #&gt; sentiment n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 anger 1247 #&gt; 2 anticipation 839 #&gt; 3 disgust 1058 #&gt; 4 fear 1476 #&gt; 5 joy 689 #&gt; 6 negative 3324 #&gt; # ... with 4 more rows To start learning about how these lexicons were developed, you can read this help file ?sentiments. For our analysis, we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon: nrc &lt;- sentiments %&gt;% filter(lexicon == &quot;nrc&quot;) %&gt;% select(word, sentiment) We can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets: tweet_words %&gt;% inner_join(nrc, by = &quot;word&quot;) %&gt;% select(source, word, sentiment) %&gt;% sample_n(10) #&gt; source word sentiment #&gt; 4545 Android phony anger #&gt; 6369 Android vote joy #&gt; 9804 Android virtue trust #&gt; 15542 iPhone tomorrow anticipation #&gt; 3451 Android finally trust #&gt; 15372 iPhone honor trust #&gt; 16163 iPhone vote negative #&gt; 11305 iPhone bomb sadness #&gt; 10763 iPhone negative sadness #&gt; 1057 Android spirit positive Now we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet by tweet analysis, assigning a sentiment to each tweet. However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device. sentiment_counts &lt;- tweet_words %&gt;% left_join(nrc, by = &quot;word&quot;) %&gt;% count(source, sentiment) %&gt;% spread(source, n) %&gt;% mutate(sentiment = replace_na(sentiment, replace = &quot;none&quot;)) sentiment_counts #&gt; # A tibble: 11 x 3 #&gt; sentiment Android iPhone #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 anger 965 527 #&gt; 2 anticipation 915 710 #&gt; 3 disgust 641 318 #&gt; 4 fear 802 486 #&gt; 5 joy 698 540 #&gt; 6 negative 1668 935 #&gt; # ... with 5 more rows For each sentiment, we can compute the odds of being in the device: proportion of words with sentiment versus proportion of words without, and then compute the odds ratio comparing the two devices. sentiment_counts %&gt;% mutate(Android = Android / (sum(Android) - Android) , iPhone = iPhone / (sum(iPhone) - iPhone), or = Android/iPhone) %&gt;% arrange(desc(or)) #&gt; # A tibble: 11 x 4 #&gt; sentiment Android iPhone or #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 disgust 0.0304 0.0184 1.66 #&gt; 2 anger 0.0465 0.0308 1.51 #&gt; 3 negative 0.0831 0.0560 1.49 #&gt; 4 sadness 0.0435 0.0300 1.45 #&gt; 5 fear 0.0383 0.0283 1.35 #&gt; 6 surprise 0.0250 0.0211 1.18 #&gt; # ... with 5 more rows So we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative! But are they statistically significant? How does this compare if we are just assigning sentiments at random? To answer this question we can compute, for each sentiment, an odds ratio and a confidence interval. We will add the two values we need to form a two-by-two table and the odd ratio: library(broom) log_or &lt;- sentiment_counts %&gt;% mutate(log_or = log((Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))), se = sqrt(1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)), conf.low = log_or - qnorm(0.975)*se, conf.high = log_or + qnorm(0.975)*se) %&gt;% arrange(desc(log_or)) log_or #&gt; # A tibble: 11 x 7 #&gt; sentiment Android iPhone log_or se conf.low conf.high #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 disgust 641 318 0.504 0.0694 0.368 0.640 #&gt; 2 anger 965 527 0.411 0.0551 0.303 0.519 #&gt; 3 negative 1668 935 0.395 0.0422 0.313 0.478 #&gt; 4 sadness 907 514 0.372 0.0562 0.262 0.482 #&gt; 5 fear 802 486 0.302 0.0584 0.187 0.416 #&gt; 6 surprise 530 365 0.168 0.0688 0.0332 0.303 #&gt; # ... with 5 more rows A graphical visualization shows some sentiments that are clearly overrepresented: log_or %&gt;% mutate(sentiment = reorder(sentiment, log_or)) %&gt;% ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) + geom_errorbar() + geom_point(aes(sentiment, log_or)) + ylab(&quot;Log odds ratio for association between Android and sentiment&quot;) + coord_flip() We see that the disgust, anger, negative, sadness and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, which is in agreement with the original claim about hyperbolic tweets. If we are interested in exploring which specific words are driving these differences, we can refer back to our android_iphone_or object: android_iphone_or %&gt;% inner_join(nrc) %&gt;% filter(sentiment == &quot;disgust&quot; &amp; Android + iPhone &gt; 10) %&gt;% arrange(desc(or)) #&gt; Joining, by = &quot;word&quot; #&gt; # A tibble: 20 x 5 #&gt; word Android iPhone or sentiment #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 mess 15 2 5.41 disgust #&gt; 2 finally 12 2 4.36 disgust #&gt; 3 unfair 12 2 4.36 disgust #&gt; 4 bad 104 26 3.45 disgust #&gt; 5 lie 13 3 3.37 disgust #&gt; 6 terrible 31 8 3.24 disgust #&gt; # ... with 14 more rows and we can make a graph: android_iphone_or %&gt;% inner_join(nrc, by = &quot;word&quot;) %&gt;% mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %&gt;% mutate(log_or = log(or)) %&gt;% filter(Android + iPhone &gt; 10 &amp; abs(log_or)&gt;1) %&gt;% mutate(word = reorder(word, log_or)) %&gt;% ggplot(aes(word, log_or, fill = log_or &lt; 0)) + facet_wrap(~sentiment, scales = &quot;free_x&quot;, nrow = 2) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) This is just a simple example of the many analyses one can perform with tidytext. To learn more, we again recommend the Tidy Text Mining book. Exercises Project Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R. You can install and load it like this: install.packages(&quot;gutenbergr&quot;) library(gutenbergr) You can see the books that are available like this: gutenberg_metadata Use str_detect to find the ID of the novel Pride and Prejudice. We notice that there are several versions. The gutenberg_works() function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for Pride and Prejudice Use the gutenberg_download function to download the text for Pride and Prejudice. Save it to an object called book. Use the tidytext package to create a tidy table with all the words in the text. We will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table. Remove the stop words and numbers from the words object. Hint: use the anti_join. Now use the AFINN lexicon to assign a sentiment value to each word. Make a plot of sentiment score versus location in the book and add a smoother. Assume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data. "],
["organizing-data-with-spreadsheets.html", "Chapter 43 Organizing Data with Spreadsheets Exercises", " Chapter 43 Organizing Data with Spreadsheets This book focuses on data analysis. Yet often a data scientist needs to collect data or work with others collecting data. Filling out a spreadsheet by hand is a practice we highly discourage and instead recommend that the process be automatized as much as possible. But sometimes you just have to do it. In this section, we provide recommendations on how to store data in a spreadsheet. We summarize a paper by Karl Broman and Kara Woo. Below are their general recommendations. Please read the paper for important details. Be Consistent - Before you commence entering data, have a plan. Once you have a plan, be consistent and stick to it. Choose Good Names for Things - You want the names you pick for objects, files and directories to be memorable, easy to spell, and descriptive. This is actually a hard balance to achieve and it does require time and thought. One important rule to follow is do not use spaces, use underscores _ or dashes instead -. Also, avoid symbols, stick to letters and numbers. Write Dates as YYYY-MM-DD - To avoid confusion, we strongly recommend using this global ISO 8601 standard. No Empty Cells - Fill in all cells and use some common code for missing data. Put Just One Thing in a Cell - It is better to add columns to store the extra information rather than having more than one piece of information in one cell. Make it a Rectangle - The spreadsheet should be a rectangle. Create a Data Dictionary - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file. No Calculations in the Raw Data Files - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script. Do Not Use Font Color or Highlighting as Data - Most import functions are not able to import this information. Encode this information as a variable instead. Make Backups - Make regular backups of your data. Use Data Validation to Avoid Errors - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. Save the Data as Text Files - Save files for sharing as comma or tab delimiters. Exercises Pick a measurement you can take on a regular basis. For example, your daily weight or how long it takes you to run 5 miles. Keep a spreadsheet that includes the date, the hour, the measurement, and any other informative variable you think is worth keeping. Do this for 2 weeks. Then make a plot. The example is from Genetics. Francis Galton studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, and a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected, our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. For example, we will study if bases on ball predict runs. Regression can be applied in many other circumstances as well. "],
["case-study-is-height-hereditary.html", "Chapter 44 Case study: Is height hereditary?", " Chapter 44 Case study: Is height hereditary? We have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and the firstborn son of each family: library(HistData) data(&quot;GaltonFamilies&quot;) set.seed(1) galton_heights &lt;- GaltonFamilies %&gt;% filter(gender == &quot;male&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) In the exercises, we will look at other relationships including mothers and daughters. Suppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries: galton_heights %&gt;% summarize(mean(father), sd(father), mean(son), sd(son)) #&gt; # A tibble: 1 x 4 #&gt; `mean(father)` `sd(father)` `mean(son)` `sd(son)` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 69.1 2.55 69.2 2.51 However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son. galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) We will learn that the correlation coefficient is a summary of how two variables move together and then see how this can be used to predict one variable using the other. "],
["correlation-and-the-regression-line.html", "Chapter 45 Correlation and the regression line 45.1 The correlation coefficient 45.2 Sample correlation is a random variable 45.3 When is correlation a useful summary? 45.4 Prediction based on stratification 45.5 The regression line 45.6 Regression improves precision 45.7 Bivariate normal distribution (advanced) 45.8 Variance explained 45.9 Warning: there are two regression lines Exercises", " Chapter 45 Correlation and the regression line 45.1 The correlation coefficient The correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as: \\[ \\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right) \\] with \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\) respectively and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter \\(\\rho\\) is commonly used in statistics books to denote the correlation. The reason is that \\(\\rho\\) is the Greek letter for \\(r\\), the first letter of regression. Soon we learn about the connection between correlation and regression. To understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( \\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation. The correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is: \\[ \\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 = 1/\\sigma^2 \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 = 1 \\] A similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1. So, for example, the correlation between a variable and itself is 1 and the correlation between a variable and its negative is -1. For other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5: galton_heights %&gt;% summarize(r = cor(father, son)) #&gt; # A tibble: 1 x 1 #&gt; r #&gt; &lt;dbl&gt; #&gt; 1 0.457 To see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99: 45.2 Sample correlation is a random variable Before we continue connecting correlation to regression, let’s remind ourselves about random variability. In most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable. By way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with: R &lt;- sample_n(galton_heights, 25, replace = TRUE) %&gt;% summarize(r = cor(father, son)) R #&gt; # A tibble: 1 x 1 #&gt; r #&gt; &lt;dbl&gt; #&gt; 1 0.407 R is a random variable. We can run a Monte Carlo simulation to see its distribution: B &lt;- 1000 N &lt;- 25 R &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% summarize(r=cor(father, son)) %&gt;% .$r }) data.frame(R) %&gt;% ggplot(aes(R)) + geom_histogram(binwidth = 0.05, color = &quot;black&quot;) We see that the expected value of R is the population correlation: mean(R) #&gt; [1] 0.455 and that it has a relatively high standard error relative to the range of values R can take: sd(R) #&gt; [1] 0.162 So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty. Also, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\). In our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one: data.frame(R) %&gt;% ggplot(aes(sample=R)) + stat_qq() + geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2))) If you increase \\(N\\), you will see the distribution converging to normal. 45.3 When is correlation a useful summary? Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82: Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting son’s height using the father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction. 45.4 Prediction based on stratification Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.179, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.179 for the son? It turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction. Can we figure out what this average is? We call this approach stratifying or conditioning. The distribution of the strata is called the conditional distribution and the average of this distribution the conditional average. In our case, we are computing the average son height conditioned on the father being 72 inches tall. A challenge when using this approach in practice is that for continuous data we don’t have many data points matching exactly one value. For example, we have only: sum(galton_heights$father == 72) #&gt; [1] 8 fathers that are exactly 72 inches. If we change the number to 72.5, we get even fewer: sum(galton_heights$father == 72.5) #&gt; [1] 1 The small samples sizes will result in averages with large standard errors that are not useful for prediction. For now, we will take the approach of creating strata of fathers with very similar heights. Specifically, we will round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall: conditional_avg &lt;- galton_heights %&gt;% filter(round(father) == 72) %&gt;% summarize(avg = mean(son)) %&gt;% .$avg conditional_avg #&gt; [1] 70.5 Note that a 72 inch father is taller than average. Specifically, (72 - 69.099/sd(galton_heights$father) = 1.139 standard deviations taller than the average father. Our prediction 70.55 is also taller than average, but only 0.545 standard deviations larger than the average son. The sons of 72 inch fathers have regressed some to the average height. We notice that the reduction in how much taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence. If we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group: galton_heights %&gt;% mutate(father_strata = factor(round(father))) %&gt;% ggplot(aes(father_strata, son)) + geom_boxplot() + geom_point() Not surprisingly, the centers of the groups are increasing with height. galton_heights %&gt;% mutate(father = round(father)) %&gt;% group_by(father) %&gt;% summarize(son_conditional_avg = mean(son)) %&gt;% ggplot(aes(father, son_conditional_avg)) + geom_point() Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line: In the next section, we explain that the line these averages follow is what we call the regression line and describe Galton’s theoretical justification for using this line to estimate conditional means. Here we define it and compute the regression line for the data at hand. 45.5 The regression line If we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(mu_X\\), \\(Y\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore: \\[ \\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\] We can rewrite it like this: \\[ Y = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y \\] If there is perfect correlation, we predict an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase. For negative values, we predict in the opposite direction. Note that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value using to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature To add regression lines to plots, we will need the above formula in the form: \\[ y= b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x \\] Here we add the regression line to the original data: mu_x &lt;- mean(galton_heights$father) mu_y &lt;- mean(galton_heights$son) s_x &lt;- sd(galton_heights$father) s_y &lt;- sd(galton_heights$son) r &lt;- cor(galton_heights$father, galton_heights$son) m &lt;- r * s_y / s_x b &lt;- mu_y - m*mu_x galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b, slope = m ) If we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). Here is the same plot, but using standard units: galton_heights %&gt;% ggplot(aes(scale(father), scale(son))) + geom_point(alpha = 0.5) + geom_abline(intercept = 0, slope = r) 45.6 Regression improves precision Let’s compare the two approaches to prediction that we have presented: Round father’s heights to closest inch, stratify, and then take the average. Compute the regression line and use it to predict. We use a Monte Carlo simulation sampling \\(N=50\\) families: B &lt;- 1000 N &lt;- 50 set.seed(1) conditional_avg &lt;- replicate(B, { dat &lt;- sample_n(galton_heights, N) dat %&gt;% filter(round(father) == 72) %&gt;% summarize(avg = mean(son)) %&gt;% .$avg }) regression_prediction &lt;- replicate(B, { dat &lt;- sample_n(galton_heights, N) mu_x &lt;- mean(dat$father) mu_y &lt;- mean(dat$son) s_x &lt;- sd(dat$father) s_y &lt;- sd(dat$son) r &lt;- cor(dat$father, dat$son) mu_y + r*(72 - mu_x)/s_x*s_y }) Although the expected value of these two random variables is about the same: mean(conditional_avg, na.rm = TRUE) #&gt; [1] 70.6 mean(regression_prediction) #&gt; [1] 70.5 The standard error for the regression prediction is substantially smaller: sd(conditional_avg, na.rm = TRUE) #&gt; [1] 0.958 sd(regression_prediction) #&gt; [1] 0.377 The regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data. So why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of the chapter. 45.7 Bivariate normal distribution (advanced) Correlation and the regression line are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real life examples. The main way we motivate the use of correlation involves what is called the bivariate normal distribution. When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation). A more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and for any stratum of \\(X\\), say \\(X=x\\), \\(Y\\) is approximately normal in that stratum, then the pair is approximately bivariate normal. When we fix \\(X=x\\) in this way, we then refer to the resulting distribution of the \\(Y\\)s in the strata as the conditional distribution of \\(Y\\) given \\(X=x\\). We can write it using mathematical notation like this: \\[ f_{Y \\mid X=x} \\mbox{ is the notation for conditional distribution and } \\mbox{E}(Y \\mid X=x) \\mbox{ is the notation for conditional expected value.}\\] If we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold: galton_heights %&gt;% mutate(z_father = round((father - mean(father))/sd(father))) %&gt;% filter(z_father %in% -2:2) %&gt;% ggplot() + stat_qq(aes(sample=son)) + facet_wrap(~z_father) Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is: \\[ \\mbox{E}(Y | X=x) = \\mu_Y + \\rho \\frac{X-\\mu_X}{\\sigma_X}\\sigma_Y \\] This is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the line we saw earlier: \\[ \\frac{\\mbox{E}(Y \\mid X=x) - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X} \\] This implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict. In summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line. 45.8 Variance explained The bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is: \\[ \\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2} \\] To see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72 inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced. Specifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.86 of what it was originally. We could say that the fathers height “explains” 14% of the sons height variability. The statement ‘\\(X\\) explains such and such percent of the variability’ is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance. But it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution. 45.9 Warning: there are two regression lines We computed a regression line to predict the son’s height from father’s height. We used these calculations: mu_x &lt;- mean(galton_heights$father) mu_y &lt;- mean(galton_heights$son) s_x &lt;- sd(galton_heights$father) s_y &lt;- sd(galton_heights$son) r &lt;- cor(galton_heights$father, galton_heights$son) m_1 &lt;- r * s_y / s_x b_1 &lt;- mu_y - m_1*mu_x which gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.996 + 0.451 \\(x\\). What if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.996 \\(\\} /\\) 0.451. We need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept: m_2 &lt;- r * s_x / s_y b_2 &lt;- mu_x - m_2*mu_y So we get \\(\\mbox{E}(X \\mid Y=y) =\\) 37.083 + 0.463y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average. Here is a plot showing the two regression lines: galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b_1, slope = m_1, col = &quot;blue&quot;) + geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &quot;red&quot;) with blue for the predicting son heights with father heights and red for predicting father heights with son heights. Exercises Load the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random. Set the seed to 1 before running this code as it involves a random sample. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons. "],
["case-study-moneyball.html", "Chapter 46 Case Study: Moneyball 46.1 Sabermetics 46.2 Baseball basics 46.3 No awards for BB 46.4 Base on Ball or Stolen Bases? 46.5 Regression applied to baseball statistics", " Chapter 46 Case Study: Moneyball Moneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics baseball team and its general manager, the person tasked with building the team, Billy Beane. Traditionally, baseball teams use scouts to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries. From 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach. As motivation for this chapter, we will pretend it is 2002 and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746: 46.1 Sabermetics Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistics we will describe soon, the batting average, has been used for decades to summarize a batter’s success. Other statistics such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win. This changed with Bill James. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win sabermetrics. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well. In this chapter, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts. 46.2 Baseball basics To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem. The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s pitcher throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (tag all 4 bases). Each team gets nine tries, referred to as innings, to score runs and each inning ends after three outs (three failures). Here is a video showing a success and here one showing a failure. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved. Now there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many bases as possible. There are four bases with the fourth one called home plate. Home plate is where batters start by trying to hit, so the bases form a cycle. (Source: Wikipedia Commons) A batter who goes around the bases and arrives home, scores a run. We are simplifying a bit, but there are five ways a batter can succeed, that is not make an out: Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base. Single - Batter hits the ball and gets to first base. Double (X2B) - Batter hits the ball and gets to second base. Triple (X3B) - Batter hits the ball and gets to third base. Home Run (HR) - Batter hits the ball and goes all the way home and scores a run. Here is an example of a HR. If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is on base, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from first to second without the other team tagging him. Here is an example of a stolen base. All these events are kept track of during the season and are available to us through the Lahman package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players. 46.3 No awards for BB (Source: John Vennavally-Rao) Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 25%, we call it batting 250. One of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the on base percentage (OPB) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion to plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OPB as an important statistic. In contrast, total stolen bases were considered important and an award given to the player with the most. But players with high totals of SB, also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for BB or SB? 46.4 Base on Ball or Stolen Bases? One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some. Let’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot: library(Lahman) Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs: Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(SB_per_game = SB/G, R_per_game = R/G) %&gt;% ggplot(aes(SB_per_game, R_per_game)) + geom_point(alpha = 0.5) Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs: Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) Here again we see a clear association. But does this mean that increasing a team BBs cause an increase in runs? One of the most important lessons you learn in this chapter is that assocation is not causation. In fact, it looks like BB and HRs are also associated: Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(HR_per_game = HR/G, BB_per_game = BB/G) %&gt;% ggplot(aes(HR_per_game, BB_per_game)) + geom_point(alpha = 0.5) We know that HR cause runs because, as the name implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is confounding, an important concept we will learn more about throughout this chapter. Linear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one. 46.5 Regression applied to baseball statistics Can we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal: library(Lahman) p &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) p We can see that the histograms of each strata confirm that the conditional distributions are normal: Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(z_HR = round((HR - mean(HR))/sd(HR)), R_per_game = R/G) %&gt;% filter(z_HR %in% -2:3) %&gt;% ggplot(aes(x=R_per_game)) + geom_histogram(binwidth = 0.25, color = &quot;black&quot;) + facet_wrap(~z_HR) The qq-plots confirm that the normal approximation holds: Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(z_HR = round((HR - mean(HR))/sd(HR)), R_per_game = R/G) %&gt;% filter(z_HR %in% -2:3) %&gt;% ggplot() + stat_qq(aes(sample=R_per_game)) + facet_wrap(~z_HR) Now we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics: summary_stats &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;% summarize(avg_HR = mean(HR_per_game), s_HR = sd(HR_per_game), avg_R = mean(R_per_game), s_R = sd(R_per_game), r = cor(HR_per_game, R_per_game)) summary_stats #&gt; avg_HR s_HR avg_R s_R r #&gt; 1 0.855 0.243 4.36 0.589 0.762 and use the formulas given above to create the regression lines: reg_line &lt;- summary_stats %&gt;% summarize(slope = r*s_R/s_HR, intercept = avg_R - slope*avg_HR) p + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope) Soon we will learn R functions, such as lm, that make fitting regression lines much easier. Another example is the ggplot2 function geom_smooth which computes and adds a regression line to plot along with confidence intervals, which we also learn about later. We use the argument method = &quot;lm&quot; which stands for linear model, the title of an upcoming section. So we can simplify the code above like this: p + geom_smooth(method = &quot;lm&quot;) In the example above, the slope is 1.845. So this tells us that teams that hit 1 more HR per game than the average team, score 1.845 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB. "],
["confounding.html", "Chapter 47 Confounding 47.1 Understanding confounding through stratification 47.2 Multivariate regression", " Chapter 47 Confounding Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of: bb_slope &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;% lm(R_per_game ~ BB_per_game, data = .) %&gt;% .$coef %&gt;% .[2] bb_slope #&gt; BB_per_game #&gt; 0.735 So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.471 more runs per game? We are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.471 runs per game. But this does not mean that BB are the cause. Note that if we compute the regression line slope for singles we get: singles_slope &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&gt;% lm(R_per_game ~ Singles_per_game, data = .) %&gt;% .$coef %&gt;% .[2] singles_slope #&gt; Singles_per_game #&gt; 0.449 which is a lower value than what we obtain for BB. Also, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles: Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&gt;% summarize(cor(BB, HR), cor(Singles, HR), cor(BB,Singles)) #&gt; cor(BB, HR) cor(Singles, HR) cor(BB, Singles) #&gt; 1 0.404 -0.174 -0.0561 It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BB and a team with many HR will also have more BB. Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are confounded with HR. Nonetheless, could it be that BB still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well. 47.1 Understanding confounding through stratification A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates: dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR_strata = round(HR/G,1), BB_per_game = BB/G, R_per_game = R/G) %&gt;% filter(HR_strata &gt;= 0.4 &amp; HR_strata &lt;=1.2) and then make a scatterplot for each strata: dat %&gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;) + facet_wrap( ~ HR_strata) Remember that the regression slope for predicting runs with BB was bb_slope. Once we stratify by HR, these slopes are substantially reduced: dat %&gt;% group_by(HR_strata) %&gt;% summarize(slope = cor(BB_per_game, R_per_game)*sd(R_per_game)/sd(BB_per_game)) #&gt; # A tibble: 9 x 2 #&gt; HR_strata slope #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 0.734 #&gt; 2 0.5 0.566 #&gt; 3 0.6 0.412 #&gt; 4 0.7 0.285 #&gt; 5 0.8 0.365 #&gt; 6 0.9 0.261 #&gt; # ... with 3 more rows The slopes are reduced, but they are not 0, which indicates that BB are helpful for producing runs. In fact, the values above are closer to the slope we obtained from singles 0.449, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power. Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot: In this case, the slopes do not change much from the original: #&gt; # A tibble: 12 x 2 #&gt; BB_strata slope #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2.8 1.53 #&gt; 2 2.9 1.57 #&gt; 3 3 1.52 #&gt; 4 3.1 1.49 #&gt; 5 3.2 1.58 #&gt; 6 3.3 1.56 #&gt; # ... with 6 more rows They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs. hr_slope &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;% lm(R_per_game ~ HR_per_game, data = .) %&gt;% .$coef %&gt;% .[2]; hr_slope #&gt; HR_per_game #&gt; 1.84 Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs. 47.2 Multivariate regression It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this: \\[ \\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2 \\] with the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and vice versa. But is there an easier approach? If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this: \\[ \\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] This model suggests that if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1 x_1\\). In this analysis, referred to as multivariate regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate \\(\\beta_1\\) and \\(\\beta_2\\) from the data? For this, we learn about linear models and least squares estimates. "],
["linear-models-and-least-squared-estimates.html", "Chapter 48 Linear Models and Least Squared Estimates 48.1 Interpreting linear models 48.2 Least Squares Estimates (LSE) 48.3 The lm function 48.4 LSE are random variables 48.5 LSE are correlated (advanced) 48.6 Predicted values are random variables Exercises", " Chapter 48 Linear Models and Least Squared Estimates Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both, as we have just shown for BB, HR and runs in baseball. This has been particularly popular in fields where randomized experiments are hard to run, such as Economics and Epidemiology. When we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of a negative health effect. So how do we do account for confounding in practice? We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model. We note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combinations of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x, y\\) and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also linear combination of \\(x, y\\) and \\(z\\). So \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\), is a linear combination of \\(x_1\\) and \\(x_2\\). The simplest linear model is a constant \\(\\beta_0\\); the second simplest is a line \\(\\beta_0 + \\beta_1 x\\). If we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N. \\] Here \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We further assume that \\(\\varepsilon_i\\) are independent from each other, have expected value 0 and the standard deviation, call it \\(\\sigma\\), does not depend on \\(i\\). In the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section. Note that if we further assume that the \\(\\varepsilon\\) is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model. 48.1 Interpreting linear models One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness. Given how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as: \\[ Y_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N \\] \\(x_i\\) to \\(x_i - \\bar{x}\\) in which case \\(\\beta_0\\) would be the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father. 48.2 Least Squares Estimates (LSE) For linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this chapter. For Galton’s data, we would write: \\[ RSS = \\sum_{i=1}^n \\left\\{ Y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2 \\] This quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Let’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\): rss &lt;- function(beta0, beta1, data){ resid &lt;- galton_heights$son - (beta0+beta1*galton_heights$father) return(sum(resid^2)) } So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25. beta1 = seq(0, 1, len=nrow(galton_heights)) results &lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 25)) results %&gt;% ggplot(aes(beta1, rss)) + geom_line() + geom_line(aes(beta1, rss), col=2) We can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs. Trial and error is not going to work in this case. Instead, we will use calculus: take the partial derivatives, set them to 0 and solve. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models. 48.3 The lm function In R, we can obtain the least squares estimates using the the lm function. To fit the model: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] with \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height, we write: fit &lt;- lm(son ~ father, data = galton_heights) and obtain the least squares estimates. fit #&gt; #&gt; Call: #&gt; lm(formula = son ~ father, data = galton_heights) #&gt; #&gt; Coefficients: #&gt; (Intercept) father #&gt; 35.712 0.503 The most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit. The object fit includes more information about the fit. We can use the function summary to extract more of this information: summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = son ~ father, data = galton_heights) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.902 -1.405 0.092 1.342 8.092 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 35.7125 4.5174 7.91 2.8e-13 *** #&gt; father 0.5028 0.0653 7.70 9.5e-13 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.22 on 177 degrees of freedom #&gt; Multiple R-squared: 0.251, Adjusted R-squared: 0.246 #&gt; F-statistic: 59.2 on 1 and 177 DF, p-value: 9.47e-13 To understand some of the columns included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables 48.4 LSE are random variables The LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\) and compute the regression slope coefficient for each one: B &lt;- 1000 N &lt;- 50 lse &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% lm(son ~ father, data = .) %&gt;% .$coef }) lse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) We can see the variability of the estimates by plotting their distributions: library(gridExtra) p1 &lt;- lse %&gt;% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = &quot;black&quot;) p2 &lt;- lse %&gt;% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;) grid.arrange(p1, p2, ncol = 2) The reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\) respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. Here it is for one of our simulated data sets: sample_n(galton_heights, N, replace = TRUE) %&gt;% lm(son ~ father, data = .) %&gt;% summary #&gt; #&gt; Call: #&gt; lm(formula = son ~ father, data = .) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.098 -1.094 -0.125 1.353 4.115 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 43.743 7.041 6.21 1.2e-07 *** #&gt; father 0.393 0.102 3.86 0.00034 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.87 on 48 degrees of freedom #&gt; Multiple R-squared: 0.237, Adjusted R-squared: 0.221 #&gt; F-statistic: 14.9 on 1 and 48 DF, p-value: 0.000342 You can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation: lse %&gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1)) #&gt; se_0 se_1 #&gt; 1 8.92 0.129 The summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\) follow a t-distribution with \\(N-p\\) degrees of freedom and with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\) respectively. Remember that, as we described previously, for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy. Although we do not show examples in this book, hypothesis testing with regression models is commonly used in Epidemiology and Economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y and Z”. However, several assumptions have to hold for these statements to be true. 48.5 LSE are correlated (advanced) Although interpretation is not straightforward, it is useful to know that the LSE can be strongly correlated. lse %&gt;% summarise(cor(beta_0, beta_1)) #&gt; cor(beta_0, beta_1) #&gt; 1 -0.999 However, the correlation depends on how the predictors are defined or transformed. Here we standardize father heights which changes \\(x_i\\) to \\(x_i - \\bar{x}\\): B &lt;- 1000 N &lt;- 50 lse &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% mutate(father = father - mean(father)) %&gt;% lm(son ~ father, data = .) %&gt;% .$coef }) cor(lse[1,], lse[2,]) #&gt; [1] -0.187 48.6 Predicted values are random variables Once we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height we will: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] When we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line. Keep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = &quot;lm&quot;) that we previously used, plots \\(\\hat{Y}\\) and surrounds it by confidence intervals: galton_heights %&gt;% ggplot(aes(son, father)) + geom_point() + geom_smooth(method = &quot;lm&quot;) The R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided: galton_heights %&gt;% mutate(Y_hat = predict(lm(son ~ father, data = .))) %&gt;% ggplot(aes(father, Y_hat)) + geom_line() fit &lt;- galton_heights %&gt;% lm(son ~ father, data = .) Y_hat &lt;- predict(fit, se.fit = TRUE) names(Y_hat) #&gt; [1] &quot;fit&quot; &quot;se.fit&quot; &quot;df&quot; &quot;residual.scale&quot; Exercises We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs. Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. We keep only players with more than 100 plate appearances. library(Lahman) dat &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %&gt;% filter(pa &gt;= 100) %&gt;% select(playerID, singles, bb) Now compute a similar table but with rates computed over 1999-2001. Use the inner_join function to have the 2001 data and averages in the same table. Compute the correlation between 2002 and the previous seasons for singles and BB. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate. Now fit a linear model for each metric and use the confint function to compare the estimates. "],
["advanced-tidyverse-tibbles-and-do.html", "Chapter 49 Advanced tidyverse: tibbles and do 49.1 Tibbles 49.2 do", " Chapter 49 Advanced tidyverse: tibbles and do To see how we use lm function in a more complex analysis, let’s go back to baseball. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this: dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR = round(HR/G, 1), BB = BB/G, R = R/G) %&gt;% select(HR, BB, R) %&gt;% filter(HR &gt;= 0.4 &amp; HR&lt;=1.2) Since we didn’t know the lm function, to compute the regression line in each strata, we used the formula directly like this: dat %&gt;% group_by(HR) %&gt;% summarize(slope = cor(BB,R)*sd(R)/sd(BB)) #&gt; # A tibble: 9 x 2 #&gt; HR slope #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 0.734 #&gt; 2 0.5 0.566 #&gt; 3 0.6 0.412 #&gt; 4 0.7 0.285 #&gt; 5 0.8 0.365 #&gt; 6 0.9 0.261 #&gt; # ... with 3 more rows We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the lm function provides enough information to construct them. First, note that if we try to use the lm function to get the estimated slope like this: dat %&gt;% group_by(HR) %&gt;% lm(R ~ BB, data = .) %&gt;% .$coef #&gt; (Intercept) BB #&gt; 2.199 0.638 we don’t get the result we want. The lm function ignores the group_by. This is expected because lm is not part of the tidyverse and does not know how to handle the outcome of group_by, a grouped tibble, a class of objects we briefly introduce in the next section before getting back to fitting regression lines. 49.1 Tibbles When summarize receives the output of group_by, it somehow knows which rows of the tables go with which groups. Where is this information stored in the data.frame? dat %&gt;% group_by(HR) %&gt;% head() #&gt; # A tibble: 6 x 3 #&gt; # Groups: HR [5] #&gt; HR BB R #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.9 3.56 4.24 #&gt; 2 0.7 3.97 4.47 #&gt; 3 0.8 3.37 4.69 #&gt; 4 1.1 3.46 4.42 #&gt; 5 1 2.75 4.61 #&gt; 6 0.9 3.06 4.58 Notice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble: 6 x 3. We can learn the class of the returned object using: dat %&gt;% group_by(HR) %&gt;% class() #&gt; [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The tbl, pronounced tibble, is a special kind of data frame. We have seen them before because tidyverse functions such as group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. The manipulation verbs, select, filter, mutate, and arrange, preserve the class of the input: if they receive a data frame, they return a data frame. If they receive a tibble they return a tibble. But tibbles are the default data frame in the tidyverse: if you use one of the tidyverse parsers, such as read_csv or read_excel, you will get a tibble. Tibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe in the next section. 49.1.1 Tibbles display better The print method for tibbles is more readable than that of a data frame. To see this, compare these two outputs: Teams Teams is a data frame with many rows and columns. Nevertheless, the output shows everything, wraps around and is hard to read. It is so bad that we don’t print it here; we let you print it on your screen. If you convert this data frame to a tibble data frame, the output is much more readable: as_tibble(Teams) #&gt; # A tibble: 2,835 x 48 #&gt; yearID lgID teamID franchID divID Rank G Ghome W L DivWin #&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1871 NA BS1 BNA &lt;NA&gt; 3 31 NA 20 10 &lt;NA&gt; #&gt; 2 1871 NA CH1 CNA &lt;NA&gt; 2 28 NA 19 9 &lt;NA&gt; #&gt; 3 1871 NA CL1 CFC &lt;NA&gt; 8 29 NA 10 19 &lt;NA&gt; #&gt; 4 1871 NA FW1 KEK &lt;NA&gt; 7 19 NA 7 12 &lt;NA&gt; #&gt; 5 1871 NA NY2 NNA &lt;NA&gt; 5 33 NA 16 17 &lt;NA&gt; #&gt; 6 1871 NA PH1 PNA &lt;NA&gt; 1 28 NA 21 7 &lt;NA&gt; #&gt; # ... with 2,829 more rows, and 37 more variables: WCWin &lt;chr&gt;, #&gt; # LgWin &lt;chr&gt;, WSWin &lt;chr&gt;, R &lt;int&gt;, AB &lt;int&gt;, H &lt;int&gt;, X2B &lt;int&gt;, #&gt; # X3B &lt;int&gt;, HR &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, #&gt; # HBP &lt;int&gt;, SF &lt;int&gt;, RA &lt;int&gt;, ER &lt;int&gt;, ERA &lt;dbl&gt;, CG &lt;int&gt;, #&gt; # SHO &lt;int&gt;, SV &lt;int&gt;, IPouts &lt;int&gt;, HA &lt;int&gt;, HRA &lt;int&gt;, BBA &lt;int&gt;, #&gt; # SOA &lt;int&gt;, E &lt;int&gt;, DP &lt;int&gt;, FP &lt;dbl&gt;, name &lt;chr&gt;, park &lt;chr&gt;, #&gt; # attendance &lt;int&gt;, BPF &lt;int&gt;, PPF &lt;int&gt;, teamIDBR &lt;chr&gt;, #&gt; # teamIDlahman45 &lt;chr&gt;, teamIDretro &lt;chr&gt; Also, notice that the output adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown. 49.1.2 Subsets of tibbles are tibbles If you subset the columns of a data frame, you may get back an object that is not a data frame. For example: class(Teams[,20]) #&gt; [1] &quot;integer&quot; is not a data frame. With tibbles this does not happen: class(as_tibble(Teams)[,20]) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; This is useful in the tidyverse since functions require data frames as input. With tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $: class(as_tibble(Teams)$HR) #&gt; [1] &quot;integer&quot; A related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write hr instead of HR this: Teams$hr #&gt; NULL returns a NULL with no warning, which can make it harder to debug. In contrast, this: as_tibble(Teams)$hr #&gt; Warning: Unknown or uninitialised column: &#39;hr&#39;. #&gt; NULL gives you an informative warning. 49.1.3 Tibbles can have complex entries While data frame columns need to be vectors of numbers, strings or Boolean, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions: tibble(id = c(1, 2, 3), func = c(mean, median, sd)) #&gt; # A tibble: 3 x 2 #&gt; id func #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;fn&gt; #&gt; 2 2 &lt;fn&gt; #&gt; 3 3 &lt;fn&gt; 49.1.4 Tibbles can be grouped The function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information. In our example above, we saw that the lm function, which is not part of the tidyverse, does not know how to deal with grouped tibbles. The object is basically converted to a regular data frame and the function runs ignoring the groups. This is why we only get one pair of estimates: dat %&gt;% group_by(HR) %&gt;% lm(R ~ BB, data = .) #&gt; #&gt; Call: #&gt; lm(formula = R ~ BB, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) BB #&gt; 2.199 0.638 To make these non-tidyverse functions work with the tidyverse, we will learn about do. 49.2 do The tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %&gt;%, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The lm function is an example. The do functions serves as a bridge between R functions, such as lm and the tidyverse. The do function understands grouped tibbles and always returns a data frame. So, let’s try to use the do function to fit a regression line to each HR strata: dat %&gt;% group_by(HR) %&gt;% do(fit = lm(R ~ BB, data = .)) #&gt; Source: local data frame [9 x 2] #&gt; Groups: &lt;by row&gt; #&gt; #&gt; # A tibble: 9 x 2 #&gt; HR fit #&gt; * &lt;dbl&gt; &lt;list&gt; #&gt; 1 0.4 &lt;S3: lm&gt; #&gt; 2 0.5 &lt;S3: lm&gt; #&gt; 3 0.6 &lt;S3: lm&gt; #&gt; 4 0.7 &lt;S3: lm&gt; #&gt; 5 0.8 &lt;S3: lm&gt; #&gt; 6 0.9 &lt;S3: lm&gt; #&gt; # ... with 3 more rows Notice that we did in fact fit a regression line to each strata. The do function will create a data frame with the first column being the strata value and a column named fit (we chose the name, but it can be anything). The column will contain the result of the lm call. Therefore, the returned tibble has a column with lm objects, which is not very useful. Also, if we do not name a column (note above we named it fit), then do will return the actual output of lm, not a data frame, and this will result in an error since do is expecting a data frame as output. dat %&gt;% group_by(HR) %&gt;% do(lm(R ~ BB, data = .)) Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm For a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame: get_slope &lt;- function(data){ fit &lt;- lm(R ~ BB, data = data) data.frame(slope = fit$coefficients[2], se = summary(fit)$coefficient[2,2]) } And then use do without naming the output, since we are already getting a data frame: dat %&gt;% group_by(HR) %&gt;% do(get_slope(.)) #&gt; # A tibble: 9 x 3 #&gt; # Groups: HR [9] #&gt; HR slope se #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 0.734 0.208 #&gt; 2 0.5 0.566 0.110 #&gt; 3 0.6 0.412 0.0974 #&gt; 4 0.7 0.285 0.0705 #&gt; 5 0.8 0.365 0.0652 #&gt; 6 0.9 0.261 0.0754 #&gt; # ... with 3 more rows If we name the output, then we get something we do not want, a column containing a data frames: dat %&gt;% group_by(HR) %&gt;% do(slope = get_slope(.)) #&gt; Source: local data frame [9 x 2] #&gt; Groups: &lt;by row&gt; #&gt; #&gt; # A tibble: 9 x 2 #&gt; HR slope #&gt; * &lt;dbl&gt; &lt;list&gt; #&gt; 1 0.4 &lt;data.frame [1 × 2]&gt; #&gt; 2 0.5 &lt;data.frame [1 × 2]&gt; #&gt; 3 0.6 &lt;data.frame [1 × 2]&gt; #&gt; 4 0.7 &lt;data.frame [1 × 2]&gt; #&gt; 5 0.8 &lt;data.frame [1 × 2]&gt; #&gt; 6 0.9 &lt;data.frame [1 × 2]&gt; #&gt; # ... with 3 more rows This is not very useful. Now, let’s cover one last feature of do. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters: get_lse &lt;- function(data){ fit &lt;- lm(R ~ BB, data = data) data.frame(term = names(fit$coefficients), slope = fit$coefficients, se = summary(fit)$coefficient[,2]) } dat %&gt;% group_by(HR) %&gt;% do(get_lse(.)) #&gt; # A tibble: 18 x 4 #&gt; # Groups: HR [9] #&gt; HR term slope se #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 (Intercept) 1.36 0.631 #&gt; 2 0.4 BB 0.734 0.208 #&gt; 3 0.5 (Intercept) 2.01 0.344 #&gt; 4 0.5 BB 0.566 0.110 #&gt; 5 0.6 (Intercept) 2.53 0.305 #&gt; 6 0.6 BB 0.412 0.0974 #&gt; # ... with 12 more rows If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the broom package which was designed to facilitate the use of model fitting functions, such as lm, with the tidyverse. "],
["the-broom-package.html", "Chapter 50 The broom package Exercises", " Chapter 50 The broom package Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The broom package will make this quite easy. The broom package has three main functions, all of which extract information from the object returned by lm and return it in a tidyverse friendly data frame. These functions are tidy, glance and augment. The tidy function returns estimates and related information as a data frame: library(broom) fit &lt;- lm(R ~ BB, data = dat) tidy(fit) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 2.20 0.113 19.4 1.06e-70 #&gt; 2 BB 0.638 0.0344 18.5 1.37e-65 We can add other important summaries, such as confidence intervals: tidy(fit, conf.int = TRUE) #&gt; # A tibble: 2 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 2.20 0.113 19.4 1.06e-70 1.98 2.42 #&gt; 2 BB 0.638 0.0344 18.5 1.37e-65 0.570 0.705 Because the outcome is a data frame, we can immediately use it with do to string together the commands that produce the table we are after: dat %&gt;% group_by(HR) %&gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) #&gt; # A tibble: 18 x 8 #&gt; # Groups: HR [9] #&gt; HR term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 (Interce… 1.36 0.631 2.16 4.05e- 2 0.0631 2.66 #&gt; 2 0.4 BB 0.734 0.208 3.54 1.54e- 3 0.308 1.16 #&gt; 3 0.5 (Interce… 2.01 0.344 5.84 2.07e- 7 1.32 2.69 #&gt; 4 0.5 BB 0.566 0.110 5.14 3.02e- 6 0.346 0.786 #&gt; 5 0.6 (Interce… 2.53 0.305 8.32 2.43e-13 1.93 3.14 #&gt; 6 0.6 BB 0.412 0.0974 4.23 4.80e- 5 0.219 0.605 #&gt; # ... with 12 more rows Because a data frame is returned, we can filter and select the rows and columns we want: dat %&gt;% group_by(HR) %&gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;BB&quot;) %&gt;% select(HR, estimate, conf.low, conf.high) #&gt; # A tibble: 9 x 4 #&gt; # Groups: HR [9] #&gt; HR estimate conf.low conf.high #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.4 0.734 0.308 1.16 #&gt; 2 0.5 0.566 0.346 0.786 #&gt; 3 0.6 0.412 0.219 0.605 #&gt; 4 0.7 0.285 0.146 0.425 #&gt; 5 0.8 0.365 0.236 0.494 #&gt; 6 0.9 0.261 0.112 0.410 #&gt; # ... with 3 more rows A table like this can then be easily visualized with ggplot2: dat %&gt;% group_by(HR) %&gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;BB&quot;) %&gt;% select(HR, estimate, conf.low, conf.high) %&gt;% ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) + geom_errorbar() + geom_point() Now we return to discussing our original task of determining if slopes changed. The plot we just made, using do and broom, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe. The other functions provided by broom, glance and augment relate to model specific and observation specific outcomes respectively. Here, we can see the model fit summaries glance returns: glance(fit) #&gt; # A tibble: 1 x 11 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.266 0.265 0.454 343. 1.37e-65 2 -596. 1199. 1213. #&gt; # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; You can learn more about these summaries in any regression text book. We will see an example of augment in the next section. Exercises In a previous section, we computed the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons, and noticed that the highest correlation is between fathers and sons and the lowest is between mothers and sons. We can compute these correlations using: data(&quot;GaltonFamilies&quot;) set.seed(1) galton_heights &lt;- GaltonFamilies %&gt;% group_by(family, gender) %&gt;% sample_n(1) %&gt;% ungroup() cors &lt;- galton_heights %&gt;% gather(parent, parentHeight, father:mother) %&gt;% mutate(child = ifelse(gender == &quot;female&quot;, &quot;daughter&quot;, &quot;son&quot;)) %&gt;% unite(pair, c(&quot;parent&quot;, &quot;child&quot;)) %&gt;% group_by(pair) %&gt;% summarize(cor = cor(parentHeight, childHeight)) Are these differences statistically significant? To answer this, we will compute the slopes of the regression line along with their standard errors. Start by using lm and the broom package to compute the slopes LSE and the standard errors. Repeat the exercise above, but compute a confidence interval as well. Plot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex. Because we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: use similar code to what we used with simulations. Fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the tidy function in the broom package to obtain the results in a data frame. Now let’s repeat the above for each year since 1961 and make a plot. Use do and the broom package to fit this model for every year since 1961. Use the results of the previous exercise to plot the estimated effects of BB on runs. Advanced. Write a function that takes R, HR and BB as arguments and fits two linear models: R ~ BB and R~BB+HR. Then use the do function to obtain the BB for both models for each year since 1961. Then plot these against each other as a function of time. "],
["case-study-money-ball-continued.html", "Chapter 51 Case study: Money Ball continued 51.1 Adding salary and position information 51.2 Picking 9 players Exercises", " Chapter 51 Case study: Money Ball continued In trying to answer how well BB predict runs, data exploration led us to a model: \\[ \\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] Here, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model: \\[ Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon_i \\] with \\(Y_i\\) runs per game, \\(x_1\\) walks per game, and \\(x_2\\). To use lm here, we need to let it know we have two predictor variables. So we use the + symbol as follows: fit &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB = BB/G, HR = HR/G, R = R/G) %&gt;% lm(R ~ BB + HR, data = .) We can use tidy to see a nice summary: tidy(fit, conf.int = TRUE) #&gt; # A tibble: 3 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 1.74 0.0823 21.2 7.30e- 83 1.58 1.91 #&gt; 2 BB 0.387 0.0270 14.3 1.20e- 42 0.334 0.440 #&gt; 3 HR 1.56 0.0490 31.9 1.75e-155 1.47 1.66 When we fit the model with only one variable, the estimated slopes were 0.735 and 1.845 for BB and HR respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more. So if we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes? We now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is: \\[ Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i \\] with \\(x_1, x_2, x_3, x_4, x_5\\) representing BB, singles, doubles, triples, and HR respectively. Using lm, we can quickly find the LSE for the parameters using: fit &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB = BB/G, singles = (H-X2B-X3B-HR)/G, doubles = X2B/G, triples =X3B/G, HR=HR/G, R=R/G) %&gt;% lm(R ~ BB + singles + doubles + triples + HR, data = .) We can see the coefficients using tidy: coefs &lt;- tidy(fit, conf.int = TRUE) coefs #&gt; # A tibble: 6 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -2.77 0.0862 -32.1 5.32e-157 -2.94 -2.60 #&gt; 2 BB 0.371 0.0117 31.6 2.08e-153 0.348 0.394 #&gt; 3 singles 0.519 0.0127 40.8 9.81e-217 0.494 0.544 #&gt; 4 doubles 0.771 0.0226 34.1 8.93e-171 0.727 0.816 #&gt; 5 triples 1.24 0.0768 16.1 2.24e- 52 1.09 1.39 #&gt; 6 HR 1.44 0.0244 59.3 0. 1.40 1.49 To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot: Teams %&gt;% filter(yearID %in% 2002) %&gt;% mutate(BB = BB/G, singles = (H-X2B-X3B-HR)/G, doubles = X2B/G, triples =X3B/G, HR=HR/G, R=R/G) %&gt;% mutate(R_hat = predict(fit, newdata = .)) %&gt;% ggplot(aes(R_hat, R, label = teamID)) + geom_point() + geom_text(nudge_x=0.1, cex = 2) + geom_abline() Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line. So instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.769 + 0.371 \\(\\times\\) BB + 0.519 \\(\\times\\) singles + 0.771 \\(\\times\\) doubles + 1.24 \\(\\times\\) triples + 1.443 \\(\\times\\) HR + To define a player specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets less opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate. To make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game: pa_per_game &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% group_by(teamID) %&gt;% summarize(pa_per_game = sum(AB+BB)/max(G)) %&gt;% .$pa_per_game %&gt;% mean We compute the per-plate-appearance rates for players available in 2002 on data from 1999-2001. To avoid small sample artifacts, we filter players with few plate appearances. Here is the entire calculation in one line: players &lt;- Batting %&gt;% filter(yearID %in% 1999:2001) %&gt;% group_by(playerID) %&gt;% mutate(PA = BB + AB) %&gt;% summarize(G = sum(PA)/pa_per_game, BB = sum(BB)/G, singles = sum(H-X2B-X3B-HR)/G, doubles = sum(X2B)/G, triples = sum(X3B)/G, HR = sum(HR)/G, AVG = sum(H)/sum(AB), PA = sum(PA)) %&gt;% filter(PA &gt;= 300) %&gt;% select(-G) %&gt;% mutate(R_hat = predict(fit, newdata = .)) The player specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players: players %&gt;% ggplot(aes(R_hat)) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;) 51.1 Adding salary and position information To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the players data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function in a later chapter. Start by adding the 2002 salary of each player: players &lt;- Salaries %&gt;% filter(yearID == 2002) %&gt;% select(playerID, salary) %&gt;% right_join(players, by=&quot;playerID&quot;) Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. Here, we pick the one position the player most played using the top_n function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the OF position which stands for outfielder, a generalization of three positions left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play. players &lt;- Fielding %&gt;% filter(yearID == 2002) %&gt;% filter(!POS %in% c(&quot;OF&quot;,&quot;P&quot;)) %&gt;% group_by(playerID) %&gt;% top_n(1, G) %&gt;% filter(row_number(G) == 1) %&gt;% ungroup() %&gt;% select(playerID, POS) %&gt;% right_join(players, by=&quot;playerID&quot;) %&gt;% filter(!is.na(POS) &amp; !is.na(salary)) Finally, we add their first and last name: players &lt;- Master %&gt;% select(playerID, nameFirst, nameLast, debut) %&gt;% right_join(players, by=&quot;playerID&quot;) If you are a baseball fan, you will recognize the top 10 players: players %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&gt;% arrange(desc(R_hat)) %&gt;% top_n(10) #&gt; Selecting by R_hat #&gt; nameFirst nameLast POS salary R_hat #&gt; 1 Todd Helton 1B 5000000 8.23 #&gt; 2 Jason Giambi 1B 10428571 7.99 #&gt; 3 Albert Pujols 3B 600000 7.54 #&gt; 4 Nomar Garciaparra SS 9000000 7.51 #&gt; 5 Jeff Bagwell 1B 11000000 7.48 #&gt; 6 Alex Rodriguez SS 22000000 7.44 #&gt; 7 Carlos Delgado 1B 19400000 7.37 #&gt; 8 Rafael Palmeiro 1B 8712986 7.26 #&gt; 9 Mike Piazza C 10571429 7.16 #&gt; 10 Jim Thome 1B 8000000 7.16 51.2 Picking 9 players Notice the very high salaries for most players. In fact, we see that players with a higher metric have higher salaries: players %&gt;% ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() We do see some low cost players with very high metrics. These will be great for our team. Unfortunately, many of these are likely young players that have not yet been able to negotiate a salary and are unavailable. For example, the lowest earner in our top 10 list, Albert Pujols, was a rookie in 2001 and could not negotiate with other teams. Here is the plot without players that debuted before 1997: players %&gt;% filter(debut &lt; 1998) %&gt;% ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() We can search for good deals by looking at players that produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists called linear programming. This is not something we teach, but we include the code anyway: library(reshape2) #&gt; #&gt; Attaching package: &#39;reshape2&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; smiths library(lpSolve) players &lt;- players %&gt;% filter(debut &lt;= 1997 &amp; debut &gt; 1988) constraint_matrix &lt;- acast(players, POS ~ playerID, fun.aggregate = length) #&gt; Using R_hat as value column: use value.var to override. npos &lt;- nrow(constraint_matrix) constraint_matrix &lt;- rbind(constraint_matrix, salary = players$salary) constraint_dir &lt;- c(rep(&quot;==&quot;, npos), &quot;&lt;=&quot;) constraint_limit &lt;- c(rep(1, npos), 50*10^6) lp_solution &lt;- lp(&quot;max&quot;, players$R_hat, constraint_matrix, constraint_dir, constraint_limit, all.int = TRUE) This algorithm chooses these 9 players: our_team &lt;- players %&gt;% filter(lp_solution$solution == 1) %&gt;% arrange(desc(R_hat)) our_team %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) #&gt; nameFirst nameLast POS salary R_hat #&gt; 1 Jason Giambi 1B 10428571 7.99 #&gt; 2 Nomar Garciaparra SS 9000000 7.51 #&gt; 3 Mike Piazza C 10571429 7.16 #&gt; 4 Phil Nevin 3B 2600000 6.75 #&gt; 5 Jeff Kent 2B 6000000 6.68 We see that these players all have above average BB and HR rates, while the same is not true for singles: my_scale &lt;- function(x) (x - median(x))/mad(x) players %&gt;% mutate(BB = my_scale(BB), singles = my_scale(singles), doubles = my_scale(doubles), triples = my_scale(triples), HR = my_scale(HR), AVG = my_scale(AVG), R_hat = my_scale(R_hat)) %&gt;% filter(playerID %in% our_team$playerID) %&gt;% select(nameFirst, nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) %&gt;% arrange(desc(R_hat)) #&gt; nameFirst nameLast BB singles doubles triples HR AVG R_hat #&gt; 1 Jason Giambi 3.118 -0.5382 0.908 -0.633 1.905 2.496 3.67 #&gt; 2 Nomar Garciaparra 0.154 1.6114 3.141 0.467 0.917 3.795 3.06 #&gt; 3 Mike Piazza 0.459 -0.0811 -0.188 -1.266 2.476 1.584 2.63 #&gt; 4 Phil Nevin 0.649 -0.6745 0.809 -1.076 1.938 0.989 2.11 #&gt; 5 Jeff Kent 0.732 -0.2916 2.178 1.176 0.883 1.548 2.03 Exercises Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples and HR, should be weighed more than singles. As a result, they proposed the following metric: \\[ \\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}} \\] They called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression. Compute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS. For every year since 1961, compute the correlation between runs per game and OPS then plot these correlations as a function of year. Note that we can rewrite OPS as a weighted average of BB, singles, doubles, triples and HR. We know that the weight for doubles, triples, and HR are 2, 3 and 4 times that of singles. But what about BB? What is the weight for BB relative to singles. Hint: the weight for BB relative to singles will be a function of AB and PA. Note that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average. So now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles. We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game. We see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1961 season and after, compute the OPS and the predicted runs from our model for each player and plot them. Use the PA per game correction we used in the previous chapter: What players have show the largest difference between their rank by predicted runs and OPS? Answer res %&gt;% select(playerID, teamID, yearID, OPS, R_hat, BB, singles, doubles, triples, HR) %&gt;% mutate_if(is.numeric, function(x) rank(-x)) %&gt;% arrange(desc(abs(OPS-R_hat))) %&gt;% View() "],
["regression-fallacy.html", "Chapter 52 Regression fallacy", " Chapter 52 Regression fallacy Wikipedia defines the sophomore slump as: A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels). In Major League Baseball, the rookie of the year (ROY) award is given to the first year player that is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this recent Fox Sports article asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”. Does the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true. The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position. library(Lahman) playerInfo &lt;- Fielding %&gt;% group_by(playerID) %&gt;% arrange(desc(G)) %&gt;% slice(1) %&gt;% ungroup %&gt;% left_join(Master, by=&quot;playerID&quot;) %&gt;% select(playerID, nameFirst, nameLast, POS) Now, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump: ROY &lt;- AwardsPlayers %&gt;% filter(awardID == &quot;Rookie of the Year&quot;) %&gt;% left_join(playerInfo, by=&quot;playerID&quot;) %&gt;% rename(rookie_year = yearID) %&gt;% right_join(Batting, by=&quot;playerID&quot;) %&gt;% mutate(AVG = H/AB) %&gt;% filter(POS != &quot;P&quot;) We also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons: ROY &lt;- ROY %&gt;% filter(yearID == rookie_year | yearID == rookie_year+1) %&gt;% group_by(playerID) %&gt;% mutate(rookie = ifelse(yearID == min(yearID), &quot;rookie&quot;, &quot;sophomore&quot;)) %&gt;% filter(n() == 2) %&gt;% ungroup %&gt;% select(playerID, rookie_year, rookie, nameFirst, nameLast, AVG) Finally, we will use the spread function to have one column for the rookie and sophomore years batting averages: ROY &lt;- ROY %&gt;% spread(rookie, AVG) %&gt;% arrange(desc(rookie)) We can see the top performs in their first year: ROY #&gt; # A tibble: 99 x 6 #&gt; playerID rookie_year nameFirst nameLast rookie sophomore #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mccovwi01 1959 Willie McCovey 0.354 0.238 #&gt; 2 suzukic01 2001 Ichiro Suzuki 0.350 0.321 #&gt; 3 bumbral01 1973 Al Bumbry 0.337 0.233 #&gt; 4 lynnfr01 1975 Fred Lynn 0.331 0.314 #&gt; 5 pujolal01 2001 Albert Pujols 0.329 0.314 #&gt; 6 troutmi01 2012 Mike Trout 0.326 0.323 #&gt; # ... with 93 more rows and just by eyeballing, we see the sophomore slump. In fact, the proportion of players that have a lower batting average their sophomore year is: mean(ROY$sophomore - ROY$rookie &lt;= 0) #&gt; [1] 0.677 So is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year). We perform similar operations to what we did above: two_years &lt;- Batting %&gt;% filter(yearID %in% 2013:2014) %&gt;% group_by(playerID, yearID) %&gt;% filter(sum(AB) &gt;= 130) %&gt;% summarize(AVG = sum(H)/sum(AB)) %&gt;% ungroup %&gt;% spread(yearID, AVG) %&gt;% filter(!is.na(`2013`) &amp; !is.na(`2014`)) %&gt;% left_join(playerInfo, by=&quot;playerID&quot;) %&gt;% filter(POS!=&quot;P&quot;) %&gt;% select(-POS) %&gt;% arrange(desc(`2013`)) %&gt;% select(-playerID) The same pattern arises when we look at the top performers: batting averages go down for the most of the top performers. two_years #&gt; # A tibble: 312 x 4 #&gt; `2013` `2014` nameFirst nameLast #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.348 0.313 Miguel Cabrera #&gt; 2 0.345 0.283 Hanley Ramirez #&gt; 3 0.331 0.332 Michael Cuddyer #&gt; 4 0.324 0.289 Scooter Gennett #&gt; 5 0.324 0.277 Joe Mauer #&gt; 6 0.323 0.287 Mike Trout #&gt; # ... with 306 more rows But these are not rookies! Also, look at what happens to the worst performers of 2013: arrange(two_years, `2013`) #&gt; # A tibble: 312 x 4 #&gt; `2013` `2014` nameFirst nameLast #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.158 0.219 Danny Espinosa #&gt; 2 0.179 0.149 Dan Uggla #&gt; 3 0.181 0.2 Jeff Mathis #&gt; 4 0.184 0.208 Melvin Upton #&gt; 5 0.190 0.262 Adam Rosales #&gt; 6 0.192 0.215 Aaron Hicks #&gt; # ... with 306 more rows Their batting averages go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect: two_years %&gt;% ggplot(aes(`2013`, `2014`)) + geom_point() The correlation is: summarize(two_years, cor(`2013`,`2014`)) #&gt; # A tibble: 1 x 1 #&gt; `cor(\\`2013\\`, \\`2014\\`)` #&gt; &lt;dbl&gt; #&gt; 1 0.460 The data look very much like a bivariate normal distribution which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with: \\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\] Because the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean. "],
["measurement-error-models.html", "Chapter 53 Measurement error models", " Chapter 53 Measurement error models Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability. To understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The dslabs function rfalling_object generates these simulations: falling_object &lt;- rfalling_object() The assistants hand the data to Galileo and this is what he sees: falling_object %&gt;% ggplot(aes(time, observed_distance)) + geom_point() + ylab(&quot;Distance in meters&quot;) + xlab(&quot;Time in seconds&quot;) Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this: \\[ f(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\] The data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n \\] with \\(Y_i\\) representing distance in meters, \\(x_i\\) representing time in seconds, and \\(\\varepsilon\\) accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each \\(i\\). We also assume that there is no bias, which means the expected value \\(\\mbox{E}[\\varepsilon] = 0\\). Note that this is a linear model because it is a linear combination of known quantities (\\(x\\) and \\(x^2\\) are known) and unknown parameters (the \\(\\beta\\) s are unknown parameters to Galileo). Unlike our previous examples, here \\(x\\) is a fixed quantity; we are not conditioning. To pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. The LSE seem like a reasonable approach. How do we find the LSE? LSE calculations do not require the errors to be approximately normal. The lm function will find the \\(\\beta\\) s that will minimize the residual sum of squares: fit &lt;- falling_object %&gt;% mutate(time_sq = time^2) %&gt;% lm(observed_distance~time+time_sq, data=.) tidy(fit) #&gt; # A tibble: 3 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 55.0 0.652 84.3 8.13e-17 #&gt; 2 time 0.730 0.931 0.784 4.50e- 1 #&gt; 3 time_sq -5.04 0.276 -18.2 1.44e- 9 Let’s check if the estimated parabola fits the data. The broom function augment lets us do this easily: augment(fit) %&gt;% ggplot() + geom_point(aes(time, observed_distance)) + geom_line(aes(time, .fitted), col = &quot;blue&quot;) Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is: \\[d = h_0 + v_0 t - 0.5 \\times 9.8 t^2\\] with \\(h_0\\) and \\(v_0\\) the starting height and velocity respectively. The data we simulated above followed this equation and added measurement error to simulate n observations for dropping the ball \\((v_0=0)\\) from the tower of Pisa \\((h_0=56.67)\\). These are consistent with the parameter estimates: tidy(fit, conf.int = TRUE) #&gt; # A tibble: 3 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 55.0 0.652 84.3 8.13e-17 53.5 56.4 #&gt; 2 time 0.730 0.931 0.784 4.50e- 1 -1.32 2.78 #&gt; 3 time_sq -5.04 0.276 -18.2 1.44e- 9 -5.65 -4.43 The Tower of Pisa height is within the confidence interval for \\(\\beta_0\\), the initial velocity 0 is in the confidence interval for \\(\\beta_1\\) (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for \\(-2 \\times \\beta_2\\). "],
["association-is-not-causation.html", "Chapter 54 Association is not causation 54.1 Spurious correlation 54.2 Outliers 54.3 Reversing cause and effect 54.4 Confounders 54.5 Simpson’s Paradox Exercises", " Chapter 54 Association is not causation Association is not causation is perhaps the most important lesson one learns in a statistics class. Correlation is not causation is another way to say this. In this chapter, we have described tools useful for quantifying associations between variables. However, we must be careful not to over interpret these associations. There are many reasons that a variable \\(X\\) can be correlated with a variable \\(Y\\) without either being a cause for the other. Here, we examine three common ways that can lead to misinterpreting data. 54.1 Spurious correlation The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption. Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation. You can see many more absurd examples on this website completely dedicated to spurious correlations. The cases presented in the spurious correlation site are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend. A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble: N &lt;- 25 G &lt;- 1000000 sim_data &lt;- tibble(group = rep(1:G, each = N), X = rnorm(N*G), Y = rnorm(N*G)) The first column denotes group. We created groups and for each one we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated. Next, we compute the correlation between X and Y for each group and look at the max: res &lt;- sim_data %&gt;% group_by(group) %&gt;% summarize(r = cor(X, Y)) %&gt;% arrange(desc(r)) res #&gt; # A tibble: 1,000,000 x 2 #&gt; group r #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 539258 0.787 #&gt; 2 685168 0.774 #&gt; 3 96351 0.772 #&gt; 4 200064 0.764 #&gt; 5 443444 0.761 #&gt; 6 788827 0.755 #&gt; # ... with 1e+06 more rows We see a maximum correlation of 0.787 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated: sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% ggplot(aes(X, Y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation: res %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;) It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204, the largest one will be close 1. If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation: sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% do(tidy(lm(Y ~ X, data = .))) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -0.150 0.124 -1.21 0.237 #&gt; 2 X 0.640 0.105 6.13 0.00000301 This particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you learn methods to adjust for these multiple comparisons. 54.2 Outliers Suppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using: set.seed(1) x &lt;- rnorm(100,100,1) y &lt;- rnorm(100,84,1) x[-23] &lt;- scale(x[-23]) y[-23] &lt;- scale(y[-23]) The data look like this: tibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_point(alpha = 0.5) Not surprisingly, the correlation is very high: cor(x,y) #&gt; [1] 0.988 But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be: cor(x[-23], y[-23]) #&gt; [1] -0.00107 There is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other: tibble(x,y) %&gt;% ggplot(aes(rank(x),rank(y))) + geom_point(alpha = 0.5) The outlier is no longer associated with a very large value and the correlation comes way down: cor(rank(x), rank(y)) #&gt; [1] 0.0658 Spearman correlation can also be calculated like this: cor(x, y, method = &quot;spearman&quot;) #&gt; [1] 0.0658 There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber &amp; Elvezio M. Ronchetti. 54.3 Reversing cause and effect Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around. A form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated. Consider this quote from the article: When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse. A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school. We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model: \\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\] to the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result: library(HistData) data(&quot;GaltonFamilies&quot;) GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) %&gt;% do(tidy(lm(father ~ son, data = .))) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 34.0 4.57 7.44 4.31e-12 #&gt; 2 son 0.499 0.0648 7.70 9.47e-13 The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation. 54.4 Confounders Confounders are perhaps the most common reason that leads to associations begin misinterpreted. If \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) causes changes in both \\(X\\) and \\(Y\\). Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case. Incorrect interpretation due to confounders are ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions. 54.4.1 Example: UC Berkeley admissions Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). Here is the data: data(admissions) admissions #&gt; major gender admitted applicants #&gt; 1 A men 62 825 #&gt; 2 B men 63 560 #&gt; 3 C men 37 325 #&gt; 4 D men 33 417 #&gt; 5 E men 28 191 #&gt; 6 F men 6 373 #&gt; 7 A women 82 108 #&gt; 8 B women 68 25 #&gt; 9 C women 34 593 #&gt; 10 D women 35 375 #&gt; 11 E women 24 393 #&gt; 12 F women 7 341 The percent of men and women that were accepted was: admissions %&gt;% group_by(gender) %&gt;% summarize(percentage = round(sum(admitted*applicants)/sum(applicants),1)) #&gt; # A tibble: 2 x 2 #&gt; gender percentage #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 men 44.5 #&gt; 2 women 30.3 A statistical test clearly rejects the hypothesis that gender and admission are independent: admissions %&gt;% group_by(gender) %&gt;% summarize(total_admitted = round(sum(admitted/100*applicants)), not_admitted = sum(applicants) - sum(total_admitted)) %&gt;% select(-gender) %&gt;% do(tidy(chisq.test(.))) #&gt; # A tibble: 1 x 4 #&gt; statistic p.value parameter method #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 91.6 1.06e-21 1 Pearson&#39;s Chi-squared test with Yates&#39; con… But closer inspection shows a paradoxical result. Here are the percent admissions by major: admissions %&gt;% select(major, gender, admitted) %&gt;% spread(gender, admitted) %&gt;% mutate(women_minus_men = women - men) #&gt; major men women women_minus_men #&gt; 1 A 62 82 20 #&gt; 2 B 63 68 5 #&gt; 3 C 37 34 -3 #&gt; 4 D 33 35 2 #&gt; 5 E 28 24 -4 #&gt; 6 F 6 7 1 Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals. The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability. So let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major. A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than \\(x=0\\). However, \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)? One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants: admissions %&gt;% group_by(major) %&gt;% summarize(major_selectivity = sum(admitted*applicants)/sum(applicants), percent_women_applicants = sum(applicants*(gender==&quot;women&quot;)/sum(applicants))*100) %&gt;% ggplot(aes(major_selectivity, percent_women_applicants, label = major)) + geom_text() There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women. 54.4.2 Confounding explained graphically The following plot shows the percent of applicants that were accepted by gender: admissions %&gt;% mutate(percent_admitted = admitted*applicants/sum(applicants)) %&gt;% ggplot(aes(gender, y = percent_admitted, fill = major)) + geom_bar(stat = &quot;identity&quot;, position = &quot;stack&quot;) It also breaks down the acceptance rates by major: the size of the colored bars represents the percent of each major to which students were admitted. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors. What the plot does not show us is the percent admitted by major. 54.4.3 Average after stratifying In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away: admissions %&gt;% ggplot(aes(major, admitted, col = gender, size = applicants)) + geom_point() Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B. If we average the difference by major, we find that the percent is actually 3.5% higher for women. admissions %&gt;% group_by(gender) %&gt;% summarize(average = mean(admitted)) #&gt; # A tibble: 2 x 2 #&gt; gender average #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 men 38.2 #&gt; 2 women 41.7 54.5 Simpson’s Paradox The case we have just covered is an example of Simpson’s Paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three variables \\(X\\), \\(Y\\) and \\(Z\\). Here is a scatterplot of \\(Y\\) versus \\(X\\): You can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors), which we have not yet shown, another pattern emerges: It is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated as seen in the plot above. Exercises For the next set of exercises, we examine the data from a 2014 PNAS paper that analyzed success rates from funding agencies in the Netherlands and concluded: our results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials. A response was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded: However, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality. Who is right here? The original paper or the response? Here, you will examine the data and come to your own conclusion. The main evidence for the conclusion of the original paper comes down to a comparison of the percentages. Table S1 in the paper includes the information we need: library(dslabs) data(&quot;research_funding_rates&quot;) research_funding_rates Construct the two-by-two table used for the conclusion about differences in awards by gender. Compute the difference in percentage from the two by two table. In the previous exercise, we noticed that the success rate is lower for women. But is it significant? Compute a p-value using a Chi Square test. We see that the p-value is about 0.05. So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically they state that this could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show &quot;evidence&quot; of gender inequality. To settle this dispute, create a dataset with number of applications, awards, and success rate for each gender. Reorder the disciplines by their overall success rate. Hint: use the reorder function to reorder the disciplines in a first step, then use gather, separate and spread to create the desired table. To check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications. We definitely do not see the same level of confounding as in the UC Berkeley example. It is hard to say there is a confounder here. However, we do see that, based on the observed rates, some fields favor men and others favor women and we do see that the two fields with the largest difference favoring men are also the fields with the most applications. But, unlike the UC Berkeley example, women are not more likely to apply for the harder subjects. So perhaps some of the selection committees are biased and others are not? But, before we conclude this, we must check if these differences are any different than what we get by chance. Are any of the differences seen above statistically significant? Keep in mind that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. Perform a Chi Square test for each discipline. Hint: define a function that receives the total of a two-by-two table and returns a data frame with the p-value. Use the 0.5 correction. Then use the do function. For the Medical Sciences, there appears to be a statistically significant difference. But is this a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 might be considered an example of cherry picking. Repeat the exercise above, but instead of a p-value, compute a log odds ratio divided by their standard error. Then use qq-plot to see how much these log odds ratios deviate from the normal distribution we would expect: a standard normal distribution. "],
["introduction-to-machine-learning.html", "Chapter 55 Introduction to Machine Learning 55.1 Notation 55.2 Categorical versus continuous 55.3 An example Exercises", " Chapter 55 Introduction to Machine Learning Perhaps the most popular data science methodologies come from the field of Machine Learning. Machine learning success stories include the hand written zip code readers implemented by the postal service, speech recognition technology, such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing prices predictors, and driver-less cars. Although today Artificial Intelligence and Machine Learning are often used interchangeably, we make the following distinction: while the first artificial intelligence algorithms, such as those used by chess playing machines, implemented decision making based on programmable rules derived from theory or first principles, in Machine Learning decisions are based on algorithms built with data. 55.1 Notation In Machine Learning, data comes in the form of: the outcome we want to predict and the features that we will use to predict the outcome. We want to build an algorithm that takes feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to train an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome. Here, we will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms. Prediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, \\(Y\\) can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcome are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later. The general set-up is as follows. We have a series of features and an unknown outcome we want to predict: outcome feature_1 feature_2 feature_3 feature_4 feature_5 ? X_1 X_2 X_3 X_4 X_5 To build a model that provides a prediction for any set of values \\(X_1=x_1, X_2=x_2, \\dots X_5=x_5\\), we collect data for which we know the outcome: outcome feature_1 feature_2 feature_3 feature_4 feature_5 Y_1 X_1,1 X_1,2 X_1,3 X_1,4 X_1,5 Y_2 X_2,1 X_2,2 X_2,3 X_2,4 X_2,5 Y_3 X_3,1 X_3,2 X_3,3 X_3,4 X_3,5 Y_4 X_4,1 X_4,2 X_4,3 X_4,4 X_4,5 Y_5 X_5,1 X_5,2 X_5,3 X_5,4 X_5,5 Y_6 X_6,1 X_6,2 X_6,3 X_6,4 X_6,5 Y_7 X_7,1 X_7,2 X_7,3 X_7,4 X_7,5 Y_8 X_8,1 X_8,2 X_8,3 X_8,4 X_8,5 Y_9 X_9,1 X_9,2 X_9,3 X_9,4 X_9,5 Y_10 X_10,1 X_10,2 X_10,3 X_10,4 X_10,5 We use the notation \\(\\hat{Y}\\) to denote the prediction. We use the term actual outcome to denote what we ended up observing. So we want the prediction \\(\\hat{Y}\\) to match the actual outcome. 55.2 Categorical versus continuous The outcome \\(Y\\) can be categorical (which digit, what word, spam or not spam, pedestrian or empty road ahead) or continuous (movie ratings, housing prices, stock value, distance between driver-less car and a pedestrian). The concepts and algorithms we learn here apply to both. However, there are some differences in how we approach each case so it is important to distinguish between the two. When the outcome is categorical, we refer to the machine learning task as classification. Our predictions will be categorical just like our outcomes and they will be either correct or incorrect. When the outcome is continuous, we will refer to the task as prediction. In this case, our predictions will not be either right or wrong. Instead, we will make an error which is the difference between the prediction and the actual outcome. This terminology can become confusing since we call \\(\\hat{Y}\\) our prediction even when it is a categorical outcome. However, throughout the chapter, the context will make the meaning clear. Notice that these terms vary among course, text books, and other publications. Often prediction is used for both categorical and continuous, and regression is used for the continuous case. Here we avoid using regression to avoid confusion with our previous use of the term linear regression. In most cases, it will be clear if our outcomes are categorical or continuous so we will avoid using these terms when possible. 55.3 An example Let’s consider the zip code reader example. The first thing that happens to a letter when they are received in the post office is that they are sorted by zip code: Originally, humans had to sort these by hand. To do this, they had to read the zip codes on each letter. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. In this chapter, we will learn how to build algorithms that can read a digit. The first step in building an algorithm is to understand what are the outcomes and features. Below are three images of written digits. These have already been read by a human and assigned an outcome \\(Y\\). These are considered known and serve as the training set. The images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) to 255 (black), which we consider continuous for now. The following plot shows the individual features for each image: For each digitized image \\(i\\), we have a categorical outcome \\(Y_i\\) which can be one of 10 values: \\(0,1,2,3,4,5,6,7,8,9\\) and features \\(X_{i,1}, \\dots, X_{i,784}\\). We use bold face \\(\\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,784})\\) to distinguish the vector of predictors from the individual predictors. When referring to an arbitrary set of features, we drop the index \\(i\\) and use \\(Y\\) and \\(\\mathbf{X} = (X_{1}, \\dots, X_{784})\\). We use upper case variables because, in general, we think of the predictors as random variables. We use lower case, for example \\(\\mathbf{X} = \\mathbf{x}\\), to denote observed values. When we code, however, we stick to lowercase. The machine learning tasks is to build an algorithm that returns a prediction for any of the possible values of the features. Here, we will learn several approaches to building these algorithms. Although at this point it might seem impossible to achieve this, we will start with simple examples, and build up our knowledge until we can attack more complex ones. In fact, we start with an artificially simple example with just one predictor and then move on to a slightly more realistic example with two predictors. Once we understand these, we will attack more real world machine learning challenges involving many predictors. Exercises For each of the following, determine if the outcome is continuous or categorical: A. Digit reader B. Movie recommendations C. Spam filter D. Hospitalizations E. Siri How many features are available to us for prediction in the digits dataset? In the digit reader example, the outcomes are stored here: y &lt;- mnist$train$labels Do the following operations have a practical meaning? y[5] + y[6] y[5] &gt; y[6] Pick the best answer: A. Yes, because \\(9 + 2 = 11\\) and \\(9 &gt; 2\\). B. No, because y is not a numeric vector. C. No, because 11 is not a digit. It’s two digits D. No, because these are labels representing a category not a number. A 9 represents a type of digit not the number 9. "],
["the-confusion-matrix-prevalence-sensitivity-and-specificity.html", "Chapter 56 The confusion matrix, prevalence, sensitivity and specificity 56.1 Training and test sets 56.2 Overall accuracy 56.3 The confusion matrix 56.4 Sensitivity and specificity 56.5 Balanced accuracy and \\(F_1\\) score 56.6 Prevalence matters in practice 56.7 ROC and precision-recall curves Exercises", " Chapter 56 The confusion matrix, prevalence, sensitivity and specificity We start by briefly introducing the caret package, which has several useful functions for building and assessing machine learning methods. library(caret) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; lift Later in this chapter we provide more details on this package. In this section, we focus on describing ways in which machine learning algorithms are evaluated. For our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain machine learning step by step, this example will let us set down the first building block. Soon enough, we will be attacking more interesting challenges. For this first example, we use the height data in dslabs: data(heights) We start by defining the outcome and predictors. In this case, we have only one predictor: y &lt;- heights$sex x &lt;- heights$height This is clearly a categorical outcome since \\(Y\\) can be Male or Female and we only have one predictor: height. We know that we will not be able to predict \\(Y\\) very accurately based on \\(X\\) because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of better. 56.1 Training and test sets Ultimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome and use it to develop the algorithm as the training set, and the group for which we pretend we don’t know the outcome as the test set. A standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generates indexes for randomly splitting the data into training and test sets: set.seed(2) test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) The argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not. We can use the result of the function call to define the training and test sets like this: test_set &lt;- heights[test_index, ] train_set &lt;- heights[-test_index, ] We will now develop an algorithm using only the training set. Once we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set. This metric is usually referred to as overall accuracy. 56.2 Overall accuracy To demonstrate the use of overall accuracy, we will build two competing algorithms and compare them. Let’s start by developing the simplest possible machine algorithm: guessing the outcome. y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE) Note that we are completely ignoring the predictor and simply guessing the sex. In machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function: y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE) %&gt;% factor(levels = levels(test_set$sex)) The overall accuracy is simply defined as the overall proportion that is predicted correctly: mean(y_hat == test_set$sex) #&gt; [1] 0.524 Not surprisingly, our accuracy is about 50%. We are guessing! Can we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females: heights %&gt;% group_by(sex) %&gt;% summarize(mean(height), sd(height)) #&gt; # A tibble: 2 x 3 #&gt; sex `mean(height)` `sd(height)` #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.9 3.76 #&gt; 2 Male 69.3 3.61 But how do we make use of this insight? Let’s try another simple approach: predict Male if height is within two standard deviations from the average male: y_hat &lt;- ifelse(x &gt; 62, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) The accuracy goes up from 0.50 to about 0.80: mean(y == y_hat) #&gt; [1] 0.793 But can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in dangerously over-optimistic assessments. Here we examine the accuracy of 10 different cutoffs and pick the one yielding the best result: cutoff &lt;- seq(61, 70) accuracy &lt;- map_dbl(cutoff, function(x){ y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) mean(y_hat == train_set$sex) }) We can make a plot showing the accuracy obtained on the training set for males and females: We see that the maximum value is: max(accuracy) #&gt; [1] 0.836 which is much higher than 0.5. The cutoff resulting in this accuracy is: best_cutoff &lt;- cutoff[which.max(accuracy)] best_cutoff #&gt; [1] 64 Now we can now test this cutoff on our test set to make sure our accuracy is not overly optimistic: y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) y_hat &lt;- factor(y_hat) mean(y_hat == test_set$sex) #&gt; [1] 0.817 We see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know it is not due to cherry-picking a good result. 56.3 The confusion matrix The prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches. Given that the average female is about 65 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict Female? Generally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value. We can do this in R using the function table: table(predicted = y_hat, actual = test_set$sex) #&gt; actual #&gt; predicted Female Male #&gt; Female 50 27 #&gt; Male 69 379 If we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get: test_set %&gt;% mutate(y_hat = y_hat) %&gt;% group_by(sex) %&gt;% summarize(accuracy = mean(y_hat == sex)) #&gt; # A tibble: 2 x 2 #&gt; sex accuracy #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Female 0.420 #&gt; 2 Male 0.933 There is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females, males! How can our overall accuracy be so high then? This is because the prevalence of males in this dataset is high. These heights were collected from three data sciences courses, two of which had more males enrolled: prev &lt;- mean(y == &quot;Male&quot;) prev #&gt; [1] 0.773 So when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This can actually be a big problem in machine learning. If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original, biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm. There are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study sensitivity and specificity separately. 56.4 Sensitivity and specificity To define sensitivity and specificity, we need a binary outcome. When the outcomes are categorical, we can define these terms for a specific category. In the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit. Once we specify a category of interest, then we can talk about positive outcomes, \\(Y=1\\), and negative outcomes, \\(Y=0\\). In general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: call \\(\\hat{Y}=1\\) whenever \\(Y=1\\). Because an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine specificity, which is generally defined as the ability of an algorithm to not to predict a positive \\(\\hat{Y}=0\\) when the actual outcome is not a positive \\(Y=0\\). We can summarize in the following way: High sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\) High specificity: \\(Y=0 \\implies \\hat{Y} = 0\\) Another way to define specificity is by the proportion of positive calls that are actually positive: High specificity: \\(\\hat{Y}=1 \\implies Y=1\\). To provide a precise definition, we name the four entries of the confusion matrix: Actually Positive Actually Negative Predicted positve True positives (TP) False positives (FP) Predicted negative False negatives (FN) True negatives (TN) Sensitivity is typically quantified by \\(TP/(TP+FN)\\), or the proportion of actual positives (the first column or \\(TP+FN\\)) that are called positives TP. This quantity is referred to as the true positive rate (TPR) or recall. Specificity is typically quantified as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column or \\(FP+TN\\)) that are called negatives TN. This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives \\(TP\\). This quantity is referred to as precision and also as positive predictive value (PPV). Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing. The multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities. Measure of Name 1 Name 2 Definition Probability representation Sensitivity True Positive Rate (TPR) Recall \\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\) \\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\) Specificity True Negative Rate (TNR) 1 - False Postive Rate (1-FPR) \\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\) \\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\) Specificity Positive Predictive Value (PPV) Precision \\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\) \\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\) The caret function confusionMatrix computes all these metrics for us once we define what a positive is. The function expects factors as input and the first level is considered the “positive” outcome or \\(Y=1\\). In our example, Female is the first level because it comes before Male alphabetically: confusionMatrix(data = y_hat, reference = test_set$sex) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Female Male #&gt; Female 50 27 #&gt; Male 69 379 #&gt; #&gt; Accuracy : 0.817 #&gt; 95% CI : (0.781, 0.849) #&gt; No Information Rate : 0.773 #&gt; P-Value [Acc &gt; NIR] : 0.00835 #&gt; #&gt; Kappa : 0.404 #&gt; Mcnemar&#39;s Test P-Value : 2.86e-05 #&gt; #&gt; Sensitivity : 0.4202 #&gt; Specificity : 0.9335 #&gt; Pos Pred Value : 0.6494 #&gt; Neg Pred Value : 0.8460 #&gt; Prevalence : 0.2267 #&gt; Detection Rate : 0.0952 #&gt; Detection Prevalence : 0.1467 #&gt; Balanced Accuracy : 0.6768 #&gt; #&gt; &#39;Positive&#39; Class : Female #&gt; We can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same. 56.5 Balanced accuracy and \\(F_1\\) score Although, we usually recommend studying both specificity and sensitivity, very often it is useful to have a one number summary, for example for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average of specificity and sensitivity. In fact, the \\(F_1\\)-score, a widely used one number summary, is the harmonic average of precision and recall: \\[ \\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} + \\frac{1}{\\mbox{precision}}\\right) } \\] Because it is easier to write, you often see this harmonic average rewritten as: \\[ 2\\frac{\\mbox{precision} \\cdot \\mbox{recall}} {\\mbox{precision} + \\mbox{recall}} \\] when defining \\(F_1\\). Remember that, depending on the context, some type of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently. To do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average: \\[ \\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} + \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} } \\] The F_meas function in the caret package computes this summary with beta defaulting to 1. Let’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy: cutoff &lt;- seq(61, 70) F_1 &lt;- map_dbl(cutoff, function(x){ y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) F_meas(data = y_hat, reference = factor(train_set$sex)) }) As before, we can plot these \\(F_1\\) measures versus the cutoffs: We see that it is maximized at \\(F_1\\) value of: max(F_1) #&gt; [1] 0.614 when we use cutoff: best_cutoff &lt;- cutoff[which.max(F_1)] best_cutoff #&gt; [1] 66 A cutoff of 66 makes much more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix: y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) confusionMatrix(data = y_hat, reference = test_set$sex) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Female Male #&gt; Female 81 67 #&gt; Male 38 339 #&gt; #&gt; Accuracy : 0.8 #&gt; 95% CI : (0.763, 0.833) #&gt; No Information Rate : 0.773 #&gt; P-Value [Acc &gt; NIR] : 0.07819 #&gt; #&gt; Kappa : 0.475 #&gt; Mcnemar&#39;s Test P-Value : 0.00629 #&gt; #&gt; Sensitivity : 0.681 #&gt; Specificity : 0.835 #&gt; Pos Pred Value : 0.547 #&gt; Neg Pred Value : 0.899 #&gt; Prevalence : 0.227 #&gt; Detection Rate : 0.154 #&gt; Detection Prevalence : 0.282 #&gt; Balanced Accuracy : 0.758 #&gt; #&gt; &#39;Positive&#39; Class : Female #&gt; We now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm. It takes height as a predictor and predicts female, if you are 66 inches or shorter. 56.6 Prevalence matters in practice A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease. The doctor shares data with you and you then develop an algorithm with very high sensitivity. You explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly. You also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: \\(\\mbox{Pr}(\\hat{Y}=1)\\). The doctor is neither concerned nor impressed and explains that what is important is the precision of the test: \\(\\mbox{Pr}(Y=1 | \\hat{Y}=1)\\). Using Bayes theorem, we can connect the two measures: \\[ \\mbox{Pr}(Y \\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)}\\] The doctor knows that the prevalence of the disease is 5 in 1000 which implies that \\(\\mbox{Pr}(Y=1) \\, / \\,\\mbox{Pr}(\\hat{Y}=1) = 1/100\\) and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm. 56.7 ROC and precision-recall curves When comparing the two methods: guessing versus using a height cutoff, we looked at accuracy and \\(F_1\\). The second method clearly outperformed. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Note that guessing Male with higher probability would give us higher accuracy due to the bias in the sample: p &lt;- 0.9 y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = levels(test_set$sex)) mean(y_hat == test_set$sex) #&gt; [1] 0.718 But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this. Remember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both. A widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering where this name comes from, according to Wikipedia: The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory.[35] Following the attack on Pearl Harbor in 1941, the United States army began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals. For this purpose they measured the ability of radar receiver operators to make these important distinctions, which was called the Receiver Operating Characteristics. The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here is an ROC curve for guessing sex but using different probabilities of guessing male: probs &lt;- seq(0, 1, length.out = 10) guessing &lt;- map_df(probs, function(p){ y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Guessing&quot;, FPR = 1 - specificity(y_hat, test_set$sex), TPR = sensitivity(y_hat, test_set$sex)) }) guessing %&gt;% qplot(FPR, TPR, data =., xlab = &quot;1 - Specificity&quot;, ylab = &quot;Sensitivity&quot;) The ROC curve for guessing always looks like a straight line. A perfect algorithm would shoot straight to 1 and stay up there: perfect sensitivity for all values of specificity. So how does our second approach compare? We can construct an ROC curve for the height based approach: cutoffs &lt;- c(50, seq(60, 75), 80) height_cutoff &lt;- map_df(cutoffs, function(x){ y_hat &lt;- ifelse(test_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Height cutoff&quot;, FPR = 1-specificity(y_hat, test_set$sex), TPR = sensitivity(y_hat, test_set$sex)) }) By plotting both curves together we are able to compare sensitivity for different values of specificity: bind_rows(guessing, height_cutoff) %&gt;% ggplot(aes(FPR, TPR, color = method)) + geom_line() + geom_point() + xlab(&quot;1 - Specificity&quot;) + ylab(&quot;Sensitivity&quot;) We can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method. When making ROC curves it is often nice to add the cutoff used to the points: map_df(cutoffs, function(x){ y_hat &lt;- ifelse(test_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Height cutoff&quot;, cutoff = x, FPR = 1-specificity(y_hat, test_set$sex), TPR = sensitivity(y_hat, test_set$sex)) }) %&gt;% ggplot(aes(FPR, TPR, label = cutoff)) + geom_line() + geom_point() + geom_text(nudge_y = 0.01) ROC curves have one weakness and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall: guessing &lt;- map_df(probs, function(p){ y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Guess&quot;, recall = sensitivity(y_hat, test_set$sex), precision = precision(y_hat, test_set$sex)) }) height_cutoff &lt;- map_df(cutoffs, function(x){ y_hat &lt;- ifelse(test_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Height cutoff&quot;, recall = sensitivity(y_hat, test_set$sex), precision = precision(y_hat, test_set$sex)) }) bind_rows(guessing, height_cutoff) %&gt;% ggplot(aes(recall, precision, color = method)) + geom_line() + geom_point() From this plot we immediately see that the precision of guessing is not high. This is because the prevalence is low. If we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes: guessing &lt;- map_df(probs, function(p){ y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = c(&quot;Male&quot;, &quot;Female&quot;)) list(method = &quot;Guess&quot;, recall = sensitivity(y_hat, relevel(test_set$sex, &quot;Male&quot;, &quot;Female&quot;)), precision = precision(y_hat, relevel(test_set$sex, &quot;Male&quot;, &quot;Female&quot;))) }) height_cutoff &lt;- map_df(cutoffs, function(x){ y_hat &lt;- ifelse(test_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = c(&quot;Male&quot;, &quot;Female&quot;)) list(method = &quot;Height cutoff&quot;, recall = sensitivity(y_hat, relevel(test_set$sex, &quot;Male&quot;, &quot;Female&quot;)), precision = precision(y_hat, relevel(test_set$sex, &quot;Male&quot;, &quot;Female&quot;))) }) bind_rows(guessing, height_cutoff) %&gt;% ggplot(aes(recall, precision, color = method)) + geom_line() + geom_point() Exercises The reported_height and height datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The Biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it type, to denote the type of student: inclass or online: data(&quot;reported_heights&quot;) library(lubridate) dat &lt;- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %&gt;% filter(date_time &gt;= make_date(2016, 01, 25) &amp; date_time &lt; make_date(2016, 02, 1)) %&gt;% mutate(type = ifelse(day(date_time) == 25 &amp; hour(date_time) == 8 &amp; between(minute(date_time), 15, 30), &quot;inclass&quot;, &quot;online&quot;)) %&gt;% select(sex, type) y &lt;- factor(dat$sex, c(&quot;Female&quot;, &quot;Male&quot;)) x &lt;- dat$type Show summary statistics that indicate that the type is predictive of sex. Instead of using height to predict sex, use the type variable. Show the confusion matrix. Use the confusionMatrix function in the caret package to report accuracy. Now use the sensitivity and specificity functions to report specificity and sensitivity. What is the prevalence (% of females) in the dat dataset defined above? "],
["conditional-probabilities-and-expectations.html", "Chapter 57 Conditional probabilities and expectations 57.1 Conditional probabilities 57.2 Conditional expectations 57.3 The loss function 57.4 Conditional expectation minimizes squared loss function Exercises", " Chapter 57 Conditional probabilities and expectations In machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and your bank at times thinks your card was stolen when it was not. The most common reason for not being able to build prefect algorithms is that it is impossible. To see this, note that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes. Because our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height \\(x\\), you will have both males and females that are \\(x\\) inches tall. However, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data. 57.1 Conditional probabilities We use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_n\\) for covariates \\(X_1, \\dots, X_p\\). This does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability. In particular, we denote the conditional probabilities for each class \\(k\\): \\[ \\mbox{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K \\] To avoid writing out all the predictors, we will use the following bold letters: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)\\). We will also use the following notation for the conditional probability of being class \\(k\\). \\[ p_k(\\mathbf{x}) = \\mbox{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K \\] Note: We will be using the \\(p(x)\\) notation to represent conditional probabilities as functions of the predictors. Do not confuse it with the \\(p\\) that represents the number of predictors. Knowing these probabilities can guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(x), p_2(x), \\dots p_K(x)\\). In mathematical notation, we write it like this: \\[\\hat{Y} = \\max_k p_k(\\mathbf{x})\\] In machine learning we refer to this as Bayes’ Rule. But keep in mind that this is a theoretical rule since in practice we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\). In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our estimate \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor: \\[\\hat{Y} = \\max_k \\hat{p}_k(\\mathbf{x})\\] So what we will predict depends on two things 1) how close \\[\\max_k p_k(\\mathbf{x})\\] is to 1 and 2) how close our estimate \\(\\hat{p}_k(\\mathbf{x})\\) is to \\(p_k(\\mathbf{x})\\). We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities. The first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, digit readers for example, in others our success is restricted by the randomness of the process, movie recommendations for example. Before we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed above, sensitivity and specificity may differ in importance. But, even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired. 57.2 Conditional expectations For binary data, you can think of the probability \\(\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\). Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations. Because the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones: \\[ \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}). \\] As a result, we often only use the expectation to denote both the conditional probability and conditional expectation. Just like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors. 57.3 The loss function Before we start describing approaches to optimizing the way we build algorithms for continuous data, we first need to define what we mean when we say one approach is better than another. Specifically, we need to quantify what we mean by “better”. With binary outcomes, we have already described how sensitivity, specificity, accuracy and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes. The general approach to defining “best” in machine learning is to define a loss function. The most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply: \\[ (\\hat{y} - y)^2 \\] Because we often have a test set with many observations, say \\(N\\), we use the mean squared error (MSE): \\[ \\mbox{MSE} = \\frac{1}{N} \\mbox{RSS} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\] In practice, we often report the root mean squared error because it is in the same units as the outcomes: \\[ \\mbox{RMSE} = \\sqrt{\\mbox{MSE}} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2} \\] But doing the math is often easier with the MSE and is therefore more commonly used in text books, since these usually describe theoretical properties of algorithms. If the outcomes are binary, both RMSE and MSE are equivalent to accuracy since \\((\\hat{y} - y)^2\\) is 1 if the prediction was correct and 0 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible. Because our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this: \\[ \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\} \\] This is a theoretical concept because in practice we only have one dataset to work with. But, in theory, we think of having a very, very large number, call it \\(B\\), of random samples, apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as: \\[ \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2 \\] with \\(y_{i}^b\\) denoting the \\(i\\)th observation in the \\(b\\)-th random sample and \\(\\hat{y}_i^b\\) the resulting prediction obtained from applying the exact same algorithm to the \\(b\\)-th random sample. But again, in practice, we only observe one random sample so the expected MSE is only theoretical. However, in a later section, we describe an approach to estimating the MSE that tries to mimic this theoretical quantity. Before we continue, note that the there are loss functions other that the squared loss. For example, the Mean Absolute Error uses absolute values instead of squaring the errors: \\[ \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N |\\hat{Y}_i - Y_i|\\right\\} \\] However, in this book we focus on minimizing square loss since it is the most widely used. 57.4 Conditional expectation minimizes squared loss function So why do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimized the MSE. Specifically, of all possible predictors \\(\\hat{Y}\\), \\[ \\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2 \\mid \\mathbf{X}=\\mathbf{x} \\} \\] Due to this property, a succinct description of the main task of Machine Learning is that we use data to estimate: \\[ f(\\mathbf{x}) \\equiv \\mbox{E}( Y \\mid \\mathbf{X}=\\mathbf{x} ) \\] for any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)\\). Of course this is easier said than done, since this function can take any shape and \\(p\\) can be very large. Consider a case in which we only have one predictor \\(x\\). The expectation \\(\\mbox{E}\\{ Y \\mid X=x \\}\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)! The main way in which competing Machine Learning algorithms differ is in the approach to estimating this expectation. Exercises Compute conditional probabilities for being Male for the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability \\(P(x) = \\mbox{Pr}(\\mbox{Male} | \\mbox{height}=x)\\) for each \\(x\\). In the plot we just made, we see high variability for low values of height. This is because we have few data points. This time use the quantile \\(0.1,0.2,\\dots,0.9\\) and cut function to assure each group has the same number of points. Hint: for any numeric vector x, you can create groups based on quantiles like this: cut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE) Generate data from a bivariate normal distribution using the MASS package like this: Sigma &lt;- 9*matrix(c(1,0.5,0.5,1), 2, 2) dat &lt;- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) %&gt;% data.frame() %&gt;% setNames(c(&quot;x&quot;, &quot;y&quot;)) You can make a quick plot of the data like this: plot(dat) Use an approach similar to the previous exercise to estimate the conditional expectations and make a plot. "],
["logistic-regression.html", "Chapter 58 Logistic Regression 58.1 Linear regression for prediction 58.2 The predict function Exercises 58.3 Logistic regression 58.4 Generalized Linear Models Exercises", " Chapter 58 Logistic Regression 58.1 Linear regression for prediction Linear regression can be considered a machine learning algorithm. As we will see, it is too rigid to be useful in general, but for some challenges it works rather well. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. To quickly make the connection between regression and machine learning, we will reformulate Galton’s study with heights: a continuous outcome. library(HistData) galton_heights &lt;- GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) Suppose you are tasked with building a machine learning algorithm that predicts the son’s height \\(Y\\) using the father’s height \\(X\\). Let’s generate testing and training sets: library(caret) y &lt;- galton_heights$son test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) train_set &lt;- galton_heights %&gt;% slice(-test_index) test_set &lt;- galton_heights %&gt;% slice(test_index) In this case, if we were just ignoring the father’s height and guessing the son’s height, we would guess the average height of sons. avg &lt;- mean(train_set$son) avg #&gt; [1] 70.5 Our squared loss is: mean((avg - test_set$son)^2) #&gt; [1] 6.45 Can we do better? In the regression chapter, we learned that if the pair \\((X,Y)\\) follow a bivariate normal distribution, the conditional expectation (what we want to estimate) is equivalent to the regression line: \\[ f(x) = \\mbox{E}( Y \\mid X= x ) = \\beta_0 + \\beta_1 x \\] We also introduced least squares as a method for estimating the slope \\(\\beta_0\\) and intercept \\(\\beta_1\\): fit &lt;- lm(son ~ father, data = train_set) fit$coef #&gt; (Intercept) father #&gt; 35.645 0.504 This gives us an estimate of the conditional expectation: \\[ \\hat{f}(x) = 38 + 0.47 x \\] We can see that this does indeed provide an improvement over our guessing approach. y_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$father mean((y_hat - test_set$son)^2) #&gt; [1] 4.73 58.2 The predict function The predict function is very useful for machine learning applications. This function takes a fitted object from functions such as lm or glm (we learn about glm soon) and a data frame with the new predictors for which to predict. So in our current example, we would use predict like this: y_hat &lt;- predict(fit, test_set) Using predict, we can get the same results as we did previously: y_hat &lt;- predict(fit, test_set) mean((y_hat - test_set$son)^2) #&gt; [1] 4.73 Predict does not always return objects of the same types; it depends on what type of object is sent to it. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used. The predict is actually a special type of function in R (called a generic function) that calls other functions depending on what kind of object it receives. So if predict receives an object coming out of the lm function, it will call predict.lm. If it receives an object coming out of glm, it calls predict.glm. These two functions are similar but different. You can learn more about the differences by reading the help files: ?predict.lm and ?predict.glm There are many other versions of predict and many machine learning algorithms have one. Exercises Create a dataset using the following code. n &lt;- 100 Sigma &lt;- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) dat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %&gt;% data.frame() %&gt;% setNames(c(&quot;x&quot;, &quot;y&quot;)) Use the caret package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: use the code shown in the previous chapter inside a call to replicate. Now we will repeat the above, but using a larger datasets. Repeat exercise 1 but for datasets with n &lt;- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the sapply or map functions. Describe what you observe with the RMSE as the size of the dataset becomes larger. A. On average, the RMSE does not change much as n gets larger, while the variability of RMSE does decrease. B. Because of the law of large numbers, the RMSE decreases: more data, more precise estimates. C. n = 10000 is not sufficiently large. To see a decrease in RMSE, we need to make it larger. D. The RMSE is not a random variable. Now repeat exercise 1, but this time make the correlation between x and y larger by changing Sigma like this: n &lt;- 100 Sigma &lt;- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2) dat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %&gt;% data.frame() %&gt;% setNames(c(&quot;x&quot;, &quot;y&quot;)) Repeat the exercise and note what happens to the RMSE now. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1. A. It is just luck. If we do it again, it will be larger. B. The central limit theorem tell us the RMSE is normal. C. When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. This correlation has a much bigger effect on RMSE than n. Large n simply provide us more precise estimates of the linear model coefficients. D. These are both examples of regression so the RMSE has to be the same. Create a dataset using the following code. n &lt;- 1000 Sigma &lt;- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0, 0.75, 0, 1.0), 3, 3) dat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %&gt;% data.frame() %&gt;% setNames(c(&quot;y&quot;, &quot;x_1&quot;, &quot;x_2&quot;)) Note that y is correlated with both x_1 and x_2, but the two predictors are independent of each other. cor(dat) Use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2 Train a linear model and report the RMSE. Repeat exercise 6 but now create an example in which x_1 and x_2 are highly correlated n &lt;- 1000 Sigma &lt;- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3) dat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %&gt;% data.frame() %&gt;% setNames(c(&quot;y&quot;, &quot;x_1&quot;, &quot;x_2&quot;)) Use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2 Train a linear model and report the RMSE. Compare the results in 6 and 7 and choose the statement you agree with: A. Adding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor. B. Adding extra predictors improves predictions equally in both exercises. C. Adding extra predictors results in over fitting. D. Unless we include all predictors, we have no predicting power. 58.3 Logistic regression The regression approach can be extended to categorical data. In this chapter we first illustrate how, for binary data, one can simply assign numeric values of 0 and 1 to the outcomes \\(y\\), and apply regression as if the data were continuous. We will then point out a limitation with this approach and introduce logistic regression as a solution. Logistic regression is specific case of a set of generalized linear models. To illustrate this, we will apply it to our previous predicting sex example: If we define the outcome \\(Y\\) as 1 for females and 0 for males, and \\(X\\) as the height, in this case we are interested in the conditional probability: \\[ \\mbox{Pr}( Y = 1 \\mid X = x) \\] As an example, let’s provide a prediction for a student that is 66 inches tall. What is the conditional probability of being female if you are 66 inches tall? In our dataset, we can estimate this by rounding to the nearest inch and computing: train_set %&gt;% filter(round(height)==66) %&gt;% summarize(mean(sex==&quot;Female&quot;)) #&gt; mean(sex == &quot;Female&quot;) #&gt; 1 0.242 We will define \\(Y=1\\) for females and \\(Y=0\\) for males. To construct a prediction algorithm, we want to estimate the proportion of the population that is female for any given height \\(X=x\\), which we write as the conditional probability described above: \\(\\mbox{Pr}( Y = 1 | X=x)\\). Let’s see what this looks like for several values of \\(x\\) (we will remove values of \\(x\\) with few data points): heights %&gt;% mutate(x = round(height)) %&gt;% group_by(x) %&gt;% filter(n() &gt;= 10) %&gt;% summarize(prop = mean(sex == &quot;Female&quot;)) %&gt;% ggplot(aes(x, prop)) + geom_point() Since the results from the plot above look close to linear, and it is the only approach we currently know, we will try regression. We assume that: \\[p(x) = \\mbox{Pr}( Y = 1 | X=x) = \\beta_0 + \\beta_1 x\\] Note: because \\(p_0(x) = 1 - p_1(x)\\), we will only estimate \\(p_1(x)\\) and drop the index. If we convert the factors to 0s and 1s, we can we can estimate \\(\\beta_0\\) and \\(\\beta_1\\) with least squares. lm_fit &lt;- mutate(train_set, y = as.numeric(sex == &quot;Female&quot;)) %&gt;% lm(y ~ height, data = .) Once we have estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can obtain an actual prediction. Our estimate of the conditional probability \\(p(x)\\) is: \\[ \\hat{p}(x) = \\hat{\\beta}_0+ \\hat{\\beta}_1 x \\] To form a prediction, we define a decision rule: predict female if \\(\\hat{p}(x) &gt; 0.5\\). We can compare our predictions to the outcomes using: p_hat &lt;- predict(lm_fit, test_set) y_hat &lt;- ifelse(p_hat &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) %&gt;% factor() confusionMatrix(y_hat, test_set$sex) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Female Male #&gt; Female 20 15 #&gt; Male 98 393 #&gt; #&gt; Accuracy : 0.785 #&gt; 95% CI : (0.748, 0.82) #&gt; No Information Rate : 0.776 #&gt; P-Value [Acc &gt; NIR] : 0.322 #&gt; #&gt; Kappa : 0.177 #&gt; Mcnemar&#39;s Test P-Value : 1.22e-14 #&gt; #&gt; Sensitivity : 0.1695 #&gt; Specificity : 0.9632 #&gt; Pos Pred Value : 0.5714 #&gt; Neg Pred Value : 0.8004 #&gt; Prevalence : 0.2243 #&gt; Detection Rate : 0.0380 #&gt; Detection Prevalence : 0.0665 #&gt; Balanced Accuracy : 0.5664 #&gt; #&gt; &#39;Positive&#39; Class : Female #&gt; We see this method does substantially better than guessing. 58.4 Generalized Linear Models The function \\(\\beta_0 + \\beta_1 x\\) can take any value including negatives and values larger than 1. In fact, the estimate \\(\\hat{p}(x)\\) computed in the linear regression section does indeed become negative at around 76 inches. heights %&gt;% mutate(x = round(height)) %&gt;% group_by(x) %&gt;% filter(n() &gt;= 10) %&gt;% summarize(prop = mean(sex == &quot;Female&quot;)) %&gt;% ggplot(aes(x, prop)) + geom_point() + geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2]) The range is: range(p_hat) #&gt; [1] -0.398 1.123 But we are estimating a probability: \\(\\mbox{Pr}( Y = 1 \\mid X = x)\\) which is constrained between 0 and 1. The idea of Generalized Linear Models (GLM) is to 1) define a distribution of \\(Y\\) that is consistent with it’s possible outcomes and 2) find a function \\(g\\) so that \\(g(\\mbox{Pr}( Y = 1 \\mid X = x))\\) can be modeled as a linear combination of predictors. Logistic regression is the most commonly used GLM. It is an extension of linear regression that assures that the estimate of \\(\\mbox{Pr}( Y = 1 \\mid X = x)\\) is between 0 and 1. This approach makes use of the logistics transformation introduced in the data visualization chapter. \\[ g(p) = \\log \\frac{p}{1-p}\\] This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely something will happen compared to not happening. So \\(p=0.5\\) means the odds are 1 to 1, thus the odds are 1. If \\(p=0.75\\), the odds are 3 to 1. A nice characteristic of this transformation is that it transforms probabilities to be symmetric around 0. Here is a plot of \\(g(p)\\) versus \\(p\\): With logistic regression, we model the conditional probability directly with: \\[ g\\left\\{ \\mbox{Pr}(Y = 1 \\mid X=x) \\right\\} = \\beta_0 + \\beta_1 x \\] With this model, we can no longer use least squares. Instead we compute the maximum likelihood estimate (MLE). You can learn more about this concept in a statistical theory text. In R, we can fit the logistic regression model with the function glm: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the family parameter: glm_fit &lt;- train_set %&gt;% mutate(y = as.numeric(sex == &quot;Female&quot;)) %&gt;% glm(y ~ height, data=., family = &quot;binomial&quot;) We can obtain prediction using the predict function: p_hat_logit &lt;- predict(glm_fit, newdata = test_set, type = &quot;response&quot;) When using predict with a glm object, we have to specify that we want type=&quot;response&quot; if we want the conditional probabilities since the default is to return the logistic transformed values. This model fits the data slightly better than the line: Because we have an estimate \\(\\hat{p}(x)\\), we can obtain predictions: y_hat_logit &lt;- ifelse(p_hat_logit &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) %&gt;% factor confusionMatrix(y_hat_logit, test_set$sex) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Female Male #&gt; Female 31 19 #&gt; Male 87 389 #&gt; #&gt; Accuracy : 0.798 #&gt; 95% CI : (0.762, 0.832) #&gt; No Information Rate : 0.776 #&gt; P-Value [Acc &gt; NIR] : 0.114 #&gt; #&gt; Kappa : 0.272 #&gt; Mcnemar&#39;s Test P-Value : 7.64e-11 #&gt; #&gt; Sensitivity : 0.2627 #&gt; Specificity : 0.9534 #&gt; Pos Pred Value : 0.6200 #&gt; Neg Pred Value : 0.8172 #&gt; Prevalence : 0.2243 #&gt; Detection Rate : 0.0589 #&gt; Detection Prevalence : 0.0951 #&gt; Balanced Accuracy : 0.6081 #&gt; #&gt; &#39;Positive&#39; Class : Female #&gt; The resulting predictions are similar. This is because the two estimates of \\(p(x)\\) are larger than 1/2 in about the same region of x: data.frame(x = seq(min(tmp$x), max(tmp$x))) %&gt;% mutate(logistic = plogis(glm_fit$coef[1] + glm_fit$coef[2]*x), regression = lm_fit$coef[1] + lm_fit$coef[2]*x) %&gt;% gather(method, p_x, -x) %&gt;% ggplot(aes(x, p_x, color = method)) + geom_line() + geom_hline(yintercept = 0.5, lty = 5) Both linear and logistic regressions provide an estimate for the conditional expectation: \\[ \\mbox{E}(Y \\mid X=x) \\] which in the case of binary data is equivalent to the conditional probability: \\[ \\mbox{Pr}(Y = 1 \\mid X = x) \\] Once we move on to more complex examples, we will see that linear regression and generalized linear regression are limited and not flexible enough to be useful. The techniques we learn are essentially approaches to estimating the conditional probability in a way that is more flexible. Exercises Define the following dataset: make_data &lt;- function(n = 1000, p = 0.5, mu_0 = 0, mu_1 = 2, sigma_0 = 1, sigma_1 = 1){ y &lt;- rbinom(n, 1, p) f_0 &lt;- rnorm(n, mu_0, sigma_0) f_1 &lt;- rnorm(n, mu_1, sigma_1) x &lt;- ifelse(y == 1, f_1, f_0) test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) list(train = data.frame(x = x, y = as.factor(y)) %&gt;% slice(-test_index), test = data.frame(x = x, y = as.factor(y)) %&gt;% slice(test_index)) } dat &lt;- make_data() Note that we have defined a variable x that is predictive of a binary outcome y. dat$train %&gt;% ggplot(aes(x, color = y)) + geom_density() Compare the accuracy of linear regression and logistic regression. Repeat exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer. Generate 25 different datasets changing the difference between the two class: delta &lt;- seq(0, 3, len = 25). Plot accuracy versus delta. "],
["case-study-is-it-a-2-or-a-7.html", "Chapter 59 Case study: is it a 2 or a 7?", " Chapter 59 Case study: is it a 2 or a 7? In the two simple examples above, we only had one predictor. We actually do not consider these machine learning challenges, which are characterized by cases with many predictors. Let’s go back to the digits example in which we had 784 predictors. For illustrative purposes, we will start by simplifying this problem to one with two predictors and two classes. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the predictors. We are not quite ready to build algorithms with 784 predictors so we will extract two simple predictors from the 784: the proportion of dark pixels that are in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)). We then select a random sample of 1,000 digits, 500 in the training set and 500 in the test set and provide them here: data(&quot;mnist_27&quot;) We can explore this data by plotting the two predictors and by using colors to denote the labels: mnist_27$train %&gt;% ggplot(aes(x_1, x_2, color = y)) + geom_point() We can immediately see some patterns. For example, if \\(X_1\\) (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of \\(X_1\\), the 2s appear to be in the mid range values of \\(X_2\\). These are the images of the digits with the largest and smallest values for \\(X_1\\): And here are the original images corresponding to the largest and smallest value of \\(x_2\\): We can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging. So let’s try building a machine learning algorithm. We haven’t really learned any algorithms yet, so let’s start with logistic regression. The model is simply: \\[ p(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) = g^{-1}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) \\] with \\(g^{-1}\\) the inverse of the logistic function: \\(g^{-1}(x) = \\exp(x)/\\{1+\\exp(x)\\}\\). We fit it like this: fit &lt;- glm(y ~ x_1 + x_2, data=mnist_27$train, family=&quot;binomial&quot;) We can now build a decision rule based on the estimate of \\(\\hat{p}(x_1, x_2)\\): p_hat &lt;- predict(fit, newdata = mnist_27$test) y_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2)) library(caret) confusionMatrix(data = y_hat, reference = mnist_27$test$y) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 2 7 #&gt; 2 92 34 #&gt; 7 14 60 #&gt; #&gt; Accuracy : 0.76 #&gt; 95% CI : (0.695, 0.817) #&gt; No Information Rate : 0.53 #&gt; P-Value [Acc &gt; NIR] : 1.67e-11 #&gt; #&gt; Kappa : 0.512 #&gt; Mcnemar&#39;s Test P-Value : 0.0061 #&gt; #&gt; Sensitivity : 0.868 #&gt; Specificity : 0.638 #&gt; Pos Pred Value : 0.730 #&gt; Neg Pred Value : 0.811 #&gt; Prevalence : 0.530 #&gt; Detection Rate : 0.460 #&gt; Detection Prevalence : 0.630 #&gt; Balanced Accuracy : 0.753 #&gt; #&gt; &#39;Positive&#39; Class : 2 #&gt; We get an accuracy of 0.79! Not bad for our first try. But can we do better? Because we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(x_1, x_2)\\). Keep in mind that this is something we don’t have access to in practice, but we include it in this example because it lets us compare \\(\\hat{p}(x_1, x_2)\\) to the true \\(p(x_1, x_2)\\), which teaches us the limitations of different algorithms. Let’s do that here. We can access and plot \\(p(x_1,x_2)\\) like this: mnist_27$true_p %&gt;% ggplot(aes(x_1, x_2, fill=p)) + geom_raster() We will choose better colors and draw a curve that separates pairs \\((x_1,x_2)\\) for which \\(p(x_1,x_2) &gt; 0.5\\) and cases for which \\(p(x_1,x_2) &lt; 0.5\\): mnist_27$true_p %&gt;% ggplot(aes(x_1, x_2, z=p, fill=p)) + geom_raster() + scale_fill_gradientn(colors=c(&quot;#F8766D&quot;,&quot;white&quot;,&quot;#00BFC4&quot;)) + stat_contour(breaks=c(0.5),color=&quot;black&quot;) So above you see a plot of the true \\(p(x,y)\\). To start understanding the limitations of logistic regression here, first, note that with logistic regression \\(\\hat{p}(x,y)\\) has to be a plane and, as a result, the boundary defined by the decision rule is given by: \\[ \\hat{p}(x,y) = 0.5\\] which implies the boundary can’t be anything other than a straight line: \\[ g^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2) = 0.5 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = g(0.5) = 0 \\implies x_2 = -\\hat{\\beta}_0/\\hat{\\beta}_2 + -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1 \\] This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(x_1,x_2)\\). Here is a visual representation of \\(\\hat{p}(x_1, x_2)\\): p_hat &lt;- predict(fit, newdata = mnist_27$true_p) mnist_27$true_p %&gt;% mutate(p_hat = p_hat) %&gt;% ggplot(aes(x_1, x_2, z=p_hat, fill=p_hat)) + geom_raster() + scale_fill_gradientn(colors=c(&quot;#F8766D&quot;,&quot;white&quot;,&quot;#00BFC4&quot;)) + stat_contour(breaks=c(0.5),color=&quot;black&quot;) We can see where the mistakes were made mainly come from low values \\(x_1\\) that have either high or low value of \\(x_2\\). Logistic regression can’t catch this. p_hat &lt;- predict(fit, newdata = mnist_27$true_p) mnist_27$true_p %&gt;% mutate(p_hat = p_hat) %&gt;% ggplot() + stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color=&quot;black&quot;) + geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test) We need something more flexible. A method that permits estimates with shapes other than a plane. We are going to learn a few new algorithms based on different ideas and concepts. But what they all have in common is that they permit more flexible approaches. We will start by describing nearest neighbor and kernel approaches. To introduce the concepts behinds these approaches, we will again start with a simple one dimensional example and describe the concept of smoothing. "],
["smoothing.html", "Chapter 60 Smoothing 60.1 Bin smoothing 60.2 Kernels 60.3 Local weighted regression (loess) 60.4 Fitting parabolas 60.5 Beware of ggplot defaults Exercises", " Chapter 60 Smoothing Before continuing with machine learning algorithms, we introduce the important concept of smoothing. Smoothing is a very powerful technique used all across data analysis. Other names given to this technique are curve fitting and low pass filtering. It is designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown. The smoothing name comes from the fact that to accomplish this feat, we assume that the trend is smooth, as in a smooth surface. In contrast, the noise, or deviation from the trend, is unpredictably wobbly: Part of what we explain in this section are the assumptions that permit us to extract the trend from the noise. To understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty. To explain these concepts, we will focus first on a problem with just one predictor. Specifically, we try to estimate the time trend in the popular vote polls (difference between Obama and McCain) from 2008. data(&quot;polls_2008&quot;) qplot(day, margin, data = polls_2008) For the purposes of this example, do not think of it as a forecasting problem. Instead, we are simply interested in learning the shape of the trend after the election is over. We assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\). A mathematical model for the observed poll margin \\(Y_i\\) is: \\[ Y_i = f(x_i) + \\varepsilon_i \\] To think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\). If we knew the conditional expectation \\(f(x) = \\mbox{E}(Y \\mid X=x)\\), we would use it. But since we don’t know this conditional expectation, we have to estimate it. Let’s use regression, since it is the only method we have learned up to now. resid &lt;- ifelse(lm(margin~day, data = polls_2008)$resid &gt; 0, &quot;+&quot;, &quot;-&quot;) polls_2008 %&gt;% mutate(resid = resid) %&gt;% ggplot(aes(day, margin)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;) + geom_point(aes(color = resid), size = 3) The line we see does not appear to describe the trend very well. For example, on September 4 (day -62) the Republican Convention was held and gave John McCain a boost in the polls, which can be clearly seen in the data. However, the regression line does not capture this. To see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed. We therefore need an alternative, more flexible approach. 60.1 Bin smoothing The general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant. We can make this assumption because we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of time. An example of this idea is to assume, for the poll_2008 data, that public opinion remained approximately the same within a week’s time. With this assumption in place, we have several data points with the same expected value. So if we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\). This assumption implies that: \\[ E[Y_i | X_i = x_i ] \\approx \\mu \\mbox{ if } |x_i - x_0| \\leq 3.5 \\] In smoothing we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span. Having a name is important because later we will see that we try to optimize this parameter. This assumption implies that a good estimate for \\(f(x)\\) is the average of the \\(Y_i\\) values in the window. If we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is: \\[ \\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0} Y_i \\] The idea behind bin smoothing is to make this calculation with each value of \\(x\\) as the center. So in the poll example, for each day, we would compute the average of the values within a week with that day in the center. Here are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\). The blue line is the resulting average. By computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the -155 up to 0. At each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point: The final result looks like this: span &lt;- 7 fit &lt;- with(polls_2008, ksmooth(day, margin, x.points = day, kernel=&quot;box&quot;, bandwidth = span)) polls_2008 %&gt;% mutate(smooth = fit$y) %&gt;% ggplot(aes(day, margin)) + geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + geom_line(aes(day, smooth), color=&quot;red&quot;) 60.2 Kernels The final result from the bin smoother is quite wiggly. One reason for this is that each time the window moves, two points change. We can attenuate this somewhat by taking weighted averages that give the center point more weight than far away points, with the two points at the edges that will change receiving very little weight. You can think of the bin smoother approach as a weighted average: \\[ \\hat{f}(x_0) = \\sum_{i=1}^N w_0(x_i) Y_i \\] in which each point receives a weight of either \\(0\\) or \\(1/N_0\\), with \\(N_0\\) the number of points in the week. In the code above, we used the argument kernel=box in our call to the function ksmooth. This is because the weight function looks like a box: The ksmooth function provides a “smoother” option which uses the normal density to assign weights: In this animation, we see that points on the edge get less weight (the size of the point is proportional to its weight): The final result looks like this: span &lt;- 7 fit &lt;- with(polls_2008, ksmooth(day, margin, x.points = day, kernel=&quot;normal&quot;, bandwidth = span)) polls_2008 %&gt;% mutate(smooth = fit$y) %&gt;% ggplot(aes(day, margin)) + geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + geom_line(aes(day, smooth), color=&quot;red&quot;) Notice that the final estimate now looks smoother. There are several functions in R that implement bin smoothers. One example is ksmooth, shown above. In practice, however, we typically prefer methods that use slightly more complex models than fitting a constant. The final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as loess, which we explain next, improve on this. 60.3 Local weighted regression (loess) A limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\). Here we describe how local weighted regression (loess) permits us to consider larger window sizes. To do this, we will use a mathematical result, referred to as Taylor’s Theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line. To see why this makes sense, consider the curved edges gardeners make using straight edges: (Source: Number 10) So instead of assuming the function is approximately constant in a window, we assume the function is locally linear. We can consider larger window sizes with the linear assumption than with a constant. So instead of the one week window, we consider a larger one in which the trend is approximately linear. We start with a 3 week window and later consider and evaluate other options: \\[ E[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{ if } |x_i - x_0| \\leq 21 \\] For every point \\(x_0\\), loess defines a window and fits a line within that window. Here is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\): The fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\). The following animation demonstrates the idea: The final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters: total_days &lt;- diff(range(polls_2008$day)) span &lt;- 21/total_days fit &lt;- loess(margin ~ day, degree=1, span = span, data=polls_2008) polls_2008 %&gt;% mutate(smooth = fit$fitted) %&gt;% ggplot(aes(day, margin)) + geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + geom_line(aes(day, smooth), color=&quot;red&quot;) Different spans give us different estimates. We can see how different window sizes lead to different estimates: Here are the final estimates: There are three other differences between loess and the typical bin smoother. The first is that rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5*N closest points to \\(x\\) for the fit. When fitting a line locally, loess uses weighted approach. Basically, instead of using least squares, we minimize a weighted version: \\[ \\sum_{i=1}^N w_0(x_i) \\left[Y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2 \\] However, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight: \\[ W(u)= \\left( 1 - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } W(u) = 0 \\mbox{ if } |u| &gt; 1 \\] To define the weights, we denote \\(2h\\) as the window size and define: \\[ w_0(x_i) = W\\left(\\frac{x_i - x_0}{h}\\right) \\] This kernel differs from the Gaussian kernel in that more points get values closer to the max: The third difference is that loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument family=&quot;symmetric&quot;. 60.4 Fitting parabolas Taylor’s Theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola. The theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines. This means we can make our windows even larger and fit parabolas instead of lines. \\[ E[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{ if } |x_i - x_0| \\leq h \\] This is actually the default procedure of the function loess. You may have noticed that when we showed the code for using loess, we saw that we set degree = 1. This tells loess to fit polynomials of degree 1, a fancy name for lines. If you read the help page for loess, you will see that the argument degree defaults to 2. So, by default, loess fits parabolas not lines. Here is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid): total_days &lt;- diff(range(polls_2008$day)) span &lt;- 28/total_days fit_1 &lt;- loess(margin ~ day, degree=1, span = span, data=polls_2008) fit_2 &lt;- loess(margin ~ day, span = span, data=polls_2008) polls_2008 %&gt;% mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) %&gt;% ggplot(aes(day, margin)) + geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + geom_line(aes(day, smooth_1), color=&quot;red&quot;, lty = 2) + geom_line(aes(day, smooth_2), color=&quot;orange&quot;, lty = 1) The degree = 2 gives us more wiggly results. We actually prefer degree = 1 as it is less prone to this kind of noise. 60.5 Beware of ggplot defaults ggplot uses loess in its geom_smooth function. polls_2008 %&gt;% ggplot(aes(day, margin)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; But be careful with default behavior as they are rarely optimal. However, you can conveniently change them: polls_2008 %&gt;% ggplot(aes(day, margin)) + geom_point() + geom_smooth(color=&quot;red&quot;, span = 0.15, method.args = list(degree=1)) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercises In the wrangling part of this book, we used the code below to obtain mortality counts for Puerto Rico for 2015-2018. library(tidyverse) library(purrr) library(pdftools) fn &lt;- system.file(&quot;extdata&quot;, &quot;RD-Mortality-Report_2015-18-180531.pdf&quot;, package=&quot;dslabs&quot;) dat &lt;- map_df(str_split(pdf_text(fn), &quot;\\n&quot;), function(s){ s &lt;- str_trim(s) header_index &lt;- str_which(s, &quot;2015&quot;)[1] tmp &lt;- str_split(s[header_index], &quot;\\\\s+&quot;, simplify = TRUE) month &lt;- tmp[1] header &lt;- tmp[-1] tail_index &lt;- str_which(s, &quot;Total&quot;) n &lt;- str_count(s, &quot;\\\\d+&quot;) out &lt;- c(1:header_index, which(n==1), which(n&gt;=28), tail_index:length(s)) s[-out] %&gt;% str_remove_all(&quot;[^\\\\d\\\\s]&quot;) %&gt;% str_trim() %&gt;% str_split_fixed(&quot;\\\\s+&quot;, n = 6) %&gt;% .[,1:5] %&gt;% as_data_frame() %&gt;% setNames(c(&quot;day&quot;, header)) %&gt;% mutate(month = month, day = as.numeric(day)) %&gt;% gather(year, deaths, -c(day, month)) %&gt;% mutate(deaths = as.numeric(deaths)) }) %&gt;% mutate(month = recode(month, &quot;JAN&quot; = 1, &quot;FEB&quot; = 2, &quot;MAR&quot; = 3, &quot;APR&quot; = 4, &quot;MAY&quot; = 5, &quot;JUN&quot; = 6, &quot;JUL&quot; = 7, &quot;AGO&quot; = 8, &quot;SEP&quot; = 9, &quot; OCT&quot; = 10, &quot;NOV&quot; = 11, &quot;DEC&quot; = 12)) %&gt;% mutate(date = make_date(year, month, day)) %&gt;% filter(date &lt;= &quot;2018-05-01&quot;) Use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long. Plot the smooth estimates against day of the year, all on the same plot but with different colors. Suppose we want to predict 2s and 7s in our mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power. In fact, if we fit a regular logistic regression, the coefficient for x_2 is not significant! library(broom) mnist_27$train %&gt;% glm(y ~ x_2, family = &quot;binomial&quot;, data = .) %&gt;% tidy() Plotting a scatterplot here is not useful since y is binary: qplot(x_2, y, data = mnist_27$train) Fit a loess line to the data above and plot the results. Notice that there is predictive power, except the conditional probability is not linear. "],
["matrix-algebra.html", "Chapter 61 Matrix algebra 61.1 Notation 61.2 Matrix operations 61.3 Matrix algebra operations Exercises", " Chapter 61 Matrix algebra In machine learning, situations in which all predictors are numeric, or can be converted to numerics in a meaningful way, are common. The digits data set is an example: every pixel records a number between 0 and 255. Let’s load the data: if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist() In these cases, it is often convenient to save the predictors in a matrix and the outcome in a vector rather than using a data frame. You can see that the predictors are saved as a matrix: class(mnist$train$images) #&gt; [1] &quot;matrix&quot; This matrix represents 60,000 digits so for the examples in this chapter, we will take a more manageable subset. We will take the first 1,000 predictors x and labels y: x &lt;- mnist$train$images[1:1000,] y &lt;- mnist$train$labels[1:1000] The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called linear algebra. In fact, linear algebra and matrix notation are key elements of the language used in academic papers describing machine learning techniques. We will not cover linear algebra in detail here, but will demonstrate how to use matrices in R so that you can apply the linear algebra techniques already implemented in R base or other packages. To motivate the use of matrices, we will pose five questions/challenges: Do some digits require more ink than others? Study the distribution of the total pixel darkness and how it varies by digits. Are some pixels uninformative? Study the variation of each pixel and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification. Can we remove smudges? First, look at the distribution of all pixel values. Use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0. Binarize the data. First, look at the distribution of all pixel values. Use this to pick a cutoff to distinguish between writing and no writing. Then, convert all entries into either 1 or 0 respectively. Scale each of the predictors in each entry to have the same average and standard deviation. To complete these, we will have to perform mathematical operations involving several variables. The tidyverse is not developed to perform these types of mathematical operations. For this task, it is convenient to use matrices. Before we do this, we will introduce matrix notation and basic R code to define and operate on matrices. 61.1 Notation In matrix algebra, we have three main types of objects: scalars, vectors and matrices. Let’s start by defining and showing the notation for matrices A scalar is just one number. For example \\(a = 1\\). To denote scalars in matrix notation, we usually use a lower case letter and do not bold. Vectors are like the numeric vectors we define in R: they include several scalar entries. For example, the column containing the first pixel: length(x[,1]) #&gt; [1] 1000 entries. In matrix algebra, we use the following notation: \\[ \\mathbf{X} = \\begin{pmatrix} x_1\\\\\\ x_2\\\\\\ \\vdots\\\\\\ x_N \\end{pmatrix} \\] Similarly, we can use math notation to represent different features mathematically, by adding an index: \\[ \\mathbf{X}_1 = \\begin{pmatrix} x_{1,1}\\\\ \\vdots\\\\ x_{N,1} \\end{pmatrix} \\mbox{ and } \\mathbf{X}_2 = \\begin{pmatrix} x_{1,2}\\\\ \\vdots\\\\ x_{N,2} \\end{pmatrix} \\] If we are writing out a column, such as \\(\\mathbf{X}_1\\), in a sentence we often use the notation: \\(\\mathbf{X}_1 = ( x_{1,1}, \\dots x_{N,1})^\\top\\) with the \\(^\\top\\) the transpose operation that converts columns into rows and rows into columns. A matrix can be defined as a series of vectors of the same size joined together as columns: x_1 &lt;- 1:5 x_2 &lt;- 6:10 cbind(x_1, x_2) #&gt; x_1 x_2 #&gt; [1,] 1 6 #&gt; [2,] 2 7 #&gt; [3,] 3 8 #&gt; [4,] 4 9 #&gt; [5,] 5 10 Mathematically, we represent them with bold upper case letters: \\[ \\mathbf{X} = [ \\mathbf{X}_1 \\mathbf{X}_2 ] = \\begin{pmatrix} x_{1,1}&amp;x_{1,2}\\\\ \\vdots\\\\ x_{N,1}&amp;x_{N,2} \\end{pmatrix} \\] The dimension of a matrix is often an important characteristic needed to assure that certain operations can be performed. The dimension is a two number summary defined as the number of rows \\(\\times\\) the number of columns. In R, we can extract the dimension of a matrix with the function dim: dim(x) #&gt; [1] 1000 784 Vectors can be thought of as \\(N\\times 1\\) matrices. However, in R, a vector does not have dimensions: dim(x_1) #&gt; NULL Yet we explicitly convert a vector into a matrix using the function as.matrix: dim(as.matrix(x_1)) #&gt; [1] 5 1 We can use this notation to denote an arbitrary number of predictors with the following \\(N\\times p\\) matrix, for example, with \\(p=784\\): \\[ \\mathbf{X} = \\begin{pmatrix} x_{1,1}&amp;\\dots &amp; x_{1,p} \\\\ x_{2,1}&amp;\\dots &amp; x_{2,p} \\\\ &amp; \\vdots &amp; \\\\ x_{N,1}&amp;\\dots &amp; x_{N,p} \\end{pmatrix} \\] We stored this in x: dim(x) #&gt; [1] 1000 784 61.2 Matrix operations We will learn several useful operations related to matrix algebra. We use the three motivating examples listed above. 61.2.1 Converting a vector to a matrix It is often useful to convert a vector to a matrix. For example, because the variables are pixels on a grid, we can convert the rows of pixel intensities into a matrix representing this grid. We can convert a vector into a matrix with the matrix function and specifying the number of rows and columns that the resulting matrix should have. The matrix is filled in by column: the first column is filled first, then the second second and so on. This example helps illustrate: my_vector &lt;- 1:15 mat &lt;- matrix(my_vector, 5, 3) mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 6 11 #&gt; [2,] 2 7 12 #&gt; [3,] 3 8 13 #&gt; [4,] 4 9 14 #&gt; [5,] 5 10 15 We can fill by row by using the byrow argument. So, for example, to transpose the matrix mat, we can use: mat_t &lt;- matrix(my_vector, 3, 5, byrow = TRUE) mat_t #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 2 3 4 5 #&gt; [2,] 6 7 8 9 10 #&gt; [3,] 11 12 13 14 15 When we turn the columns into rows, we refer to the operations as transposing the matrix. The function t can be used to directly transpose a matrix: identical(t(mat), mat_t) #&gt; [1] TRUE Remember that the matrix function recycles values in the vector without warning if the product of columns and rows does not match the length of the vector: matrix(my_vector, 5, 5) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 6 11 1 6 #&gt; [2,] 2 7 12 2 7 #&gt; [3,] 3 8 13 3 8 #&gt; [4,] 4 9 14 4 9 #&gt; [5,] 5 10 15 5 10 To put the pixel intensities of our, say, 3rd entry, which is a 4 into grid, we can use: grid &lt;- matrix(x[3,], 28, 28) To confirm that in fact we have done this correctly, we can use the function image which shows an image of its third argument: image(1:28, 1:28, grid) The top of this plot is pixel 1, which is shown at the bottom so the image is flipped. To flip it back we can use: image(1:28, 1:28, grid[,28:1]) 61.2.2 Row and column summaries For the first task, related to total pixel darkness, we want to sum the values of each row and then visualize how these values vary by digit. The function rowSums takes a matrix as input and computes the desired values: sums &lt;- rowSums(x) We can also compute the averages with rowMeans if we want the values to remain between 0 and 255: avg &lt;- rowMeans(x) Once we have this, we can simply generate a boxplot: data.frame(labels = as.factor(y), row_averages = avg) %&gt;% ggplot(aes(labels, row_averages)) + geom_boxplot() From this plot we see that, not surprisingly, 1s use less ink than the other digits. We can compute the column sums and averages using the function colSums and colMeans respectively. The matrixStats package adds functions that performs operations on each row or column very efficiently, including the functions rowSds and colSds. 61.2.3 apply The functions just described are performing an operation similar to what sapply and the purrr function map do: apply the same function to a part of your object. In this case, the function is applied to either each row or each column. The apply function lets you apply any function, not just sum or mean, to a matrix. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function. So, for example, rowMeans can be written as: avgs &lt;- apply(x, 1, mean) But notice that just like with sapply and map, we can perform any function. So if we wanted the standard deviation for each column, we could write: sds &lt;- apply(x, 2, sd) What you pay for in this flexibility is that these are not as fast as dedicated functions such as rowMeans. 61.2.4 Filtering columns based on summaries We now turn to task 2: studying the variation of each pixel and removing columns associated with pixels that don’t change much and thus do not inform the classification. Although a simplistic approach, we will quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the colSds function from the matrixStats package: library(matrixStats) sds &lt;- colSds(x) A quick look at the distribution of these values shows that some pixels have very low entry to entry variability: qplot(sds, bins = &quot;30&quot;, color = I(&quot;black&quot;)) This makes sense since we don’t write in some parts of the box. Here is the variance plotted by location: image(1:28, 1:28, matrix(sds, 28, 28)[, 28:1]) We see that there is little variation in the corners. So we could remove features that have no variation since these can’t help us predict. In the R chapter, we described the operations used to extract columns: x[ ,c(351,352)] and rows: x[c(2,3),] We can also use logical indexes to determine which columns or rows to keep. So if we wanted to remove uninformative predictors from our matrix, we could write this one line of code: new_x &lt;- x[ ,colSds(x) &gt; 60] dim(new_x) #&gt; [1] 1000 314 Only the columns for which the standard deviation is above 60 are kept, which removes over half the predictors. Here we add an important warning related to subsetting matrices: if you select one column or one row, the result is no longer a matrix but a vector. class(x[,1]) #&gt; [1] &quot;integer&quot; dim(x[1,]) #&gt; NULL However, we can preserve the matrix class by using the argument drop: class(x[ , 1, drop=FALSE]) #&gt; [1] &quot;matrix&quot; dim(x[, 1, drop=FALSE]) #&gt; [1] 1000 1 61.2.5 Indexing with matrices We can quickly make a histogram of all the values in our dataset. We saw how we can turn vectors into matrices. We can also undo this and turn matrices into vectors. The operation will happen by row: mat &lt;- matrix(1:15, 5, 3) mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 6 11 #&gt; [2,] 2 7 12 #&gt; [3,] 3 8 13 #&gt; [4,] 4 9 14 #&gt; [5,] 5 10 15 as.vector(mat) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 To see a histogram of all our predictor data, we can use: qplot(as.vector(x), bins = 30, color = I(&quot;black&quot;)) We notice a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 25 are smudges, we can quickly make them zero using: new_x &lt;- x new_x[new_x &lt; 50] &lt;- 0 To see what this does, we look at a smaller matrix: mat &lt;- matrix(1:15, 5, 3) mat[mat &lt; 3] &lt;- 0 mat #&gt; [,1] [,2] [,3] #&gt; [1,] 0 6 11 #&gt; [2,] 0 7 12 #&gt; [3,] 3 8 13 #&gt; [4,] 4 9 14 #&gt; [5,] 5 10 15 We can also use logical operations with matrix logical: mat &lt;- matrix(1:15, 5, 3) mat[mat &gt; 6 &amp; mat &lt;12] &lt;- 0 mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 6 0 #&gt; [2,] 2 0 12 #&gt; [3,] 3 0 13 #&gt; [4,] 4 0 14 #&gt; [5,] 5 0 15 61.2.6 Binarizing the data The histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations: bin_x &lt;- x bin_x[bin_x &lt; 255/2] &lt;- 0 bin_x[bin_x &gt; 255/2] &lt;- 1 We can also convert to a matrix of logicals and then coerce to numbers like this: bin_X &lt;- (x &gt; 255/2)*1 We can see that least the entry we looked at before does not change much: 61.2.7 Vectorization for matrices In R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. So using mathematical notation, we would write it as follows: \\[ \\begin{pmatrix} X_{1,1}&amp;\\dots &amp; X_{1,p} \\\\ X_{2,1}&amp;\\dots &amp; X_{2,p} \\\\ &amp; \\vdots &amp; \\\\ X_{N,1}&amp;\\dots &amp; X_{N,p} \\end{pmatrix} - \\begin{pmatrix} a_1\\\\\\ a_2\\\\\\ \\vdots\\\\\\ a_N \\end{pmatrix} = \\begin{pmatrix} X_{1,1}-a_1&amp;\\dots &amp; X_{1,p} -a_1\\\\ X_{2,1}-a_2&amp;\\dots &amp; X_{2,p} -a_2\\\\ &amp; \\vdots &amp; \\\\ X_{N,1}-a_n&amp;\\dots &amp; X_{N,p} -a_n \\end{pmatrix} - \\] The same holds true for other arithmetic operations. This implies that we can scale each row of a matrix like this: (x - rowMeans(x)) / rowSds(x) If you want to scale each column, be careful since this approach does not work for columns. To perform a similar operation, we convert the columns to rows using the transpose t, proceed as above, and then transpose back: t(t(X) - colMeans(X)) We can also use a function called sweep that works similarly to apply. It takes each entry of a vector and subtracts it from the corresponding row or column. X_mean_0 &lt;- sweep(x, 2, colMeas(x)) The function sweep actually has another argument that lets you define the arithmetic operation. So to divide by the standard deviation, we do the following: x_mean_0 &lt;- sweep(x, 2, colMeans(x)) x_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = &quot;/&quot;) 61.3 Matrix algebra operations Finally, although we do not cover matrix algebra operations such as matrix multiplication, we share here the relevant commands for those that know the mathematics and want to learn the code: Matrix multiplication is done with %*% so the cross product, for example, is: t(x) %*% x We can compute the cross product directly with the function with that name: crossprod(x) To compute the inverse of a function, we use solve. Here it is applied to the cross product: solve(crossprod(x)) The QR decomposition is readily available by using the qr function: qr(x) Exercises Create a 100 by 10 matrix of randomly generated normal numbers. Put the result in x Apply the three R functions that give you the dimension of x, the number of rows of x and the number of columns of x respectively. Add the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x. Add the scalar 1 to column 1, the scalar 2 to column 2, and son on, to the matrix x. Hint: use sweep with FUN = &quot;+&quot;. Compute the average of each row of x. Compute the average of each column of x. For each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make boxplot by digit class. Hint: use logical operators and rowMeans "],
["distance.html", "Chapter 62 Distance 62.1 Euclidean distance 62.2 Distance in higher dimensions 62.3 Example 62.4 Distance between predictors Exercises", " Chapter 62 Distance The concept of distance is quite intuitive. For example, when we cluster animals into subgroups (reptiles, amphibians, mammals), we are implicitly defining a distance that permits us to say what animals are “close” to each other. Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance. Many clustering and machine learning techniques rely on being able to define distance between observations, using features or predictors. 62.1 Euclidean distance As a review, let’s define the distance between two points, \\(A\\) and \\(B\\), on a Cartesian plane. The euclidean distance between \\(A\\) and \\(B\\) is simply: \\[ \\mbox{dist}(A,B) = \\sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2} \\] This definition applies to the case of one dimension in which the distance between two numbers is simply the absolute value of their difference. So if our two one-dimensional numbers are \\(A\\) and \\(B\\), the distance is: \\[ \\mbox{dist}(A,B) = \\sqrt{ (A - B)^2 } = | A - B | \\] 62.2 Distance in higher dimensions Earlier we introduced a training dataset with feature matrix measurements for 784 features. For illustrative purposes, we will look at a random sample of 2s and 7s. set.seed(0) if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist() ind &lt;- which(mnist$train$labels %in% c(2,7)) %&gt;% sample(500) x &lt;- mnist$train$images[ind,] y &lt;- mnist$train$labels[ind] The predictors are in x and the labels in y. For the purposes of, for example, smoothing, we are interested in describing distance between observation; in this case, digits. Later, for the purposes of selecting features, we might also be interested in finding pixels that behave similarly across samples. To define distance, we need to know what points are since mathematical distance is computed between points. With high dimensional data, points are no longer on the Cartesian plane. Instead, points are in higher dimensions. We can no longer visualize them and need to think abstractly. For example, predictors \\(\\mathbf{X}_i\\) are defined as a point in 784 dimensional space: \\(\\mathbf{X}_i = (x_{i,1},\\dots,x_{i,784})^\\top\\). Once we define points this way, the Euclidean distance is defined very similarly as it was for two dimensions. For example, the distance between the predictors for two observations, say observations \\(i=1\\) and \\(i=2\\) is: \\[ \\mbox{dist}(1,2) = \\sqrt{ \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 } \\] This is just one non-negative number just as it is for two dimensions. 62.3 Example The labels for the first three observations are: y[1:3] #&gt; [1] 7 7 2 The vector of predictors for each of these observations are: x_1 &lt;- x[1,] x_2 &lt;- x[2,] x_3 &lt;- x[3,] The first two numbers are seven and the third one a 2. We expect the distances between the same number: sqrt(sum((x_1-x_2)^2)) #&gt; [1] 2080 to be smaller than between different numbers: sqrt(sum((x_1-x_3)^2)) #&gt; [1] 2252 sqrt(sum((x_2-x_3)^2)) #&gt; [1] 2643 As expected, the 7s are closer to each other. A faster way to compute this is using matrix algebra: sqrt(crossprod(x_1-x_2)) #&gt; [,1] #&gt; [1,] 2080 sqrt(crossprod(x_1-x_3)) #&gt; [,1] #&gt; [1,] 2252 sqrt(crossprod(x_2-x_3)) #&gt; [,1] #&gt; [1,] 2643 We can also compute all the distances at once relatively quickly using the function dist, which computes the distance between each row and produces an object of class dist: d &lt;- dist(x) class(d) #&gt; [1] &quot;dist&quot; There are several machine learning related functions in R that take objects of class dist as input. To access the entries using row and column indices, we need to coerce it into a matrix. We can see the distance we calculated above like this: as.matrix(d)[1:3,1:3] #&gt; 1 2 3 #&gt; 1 0 2080 2252 #&gt; 2 2080 0 2643 #&gt; 3 2252 2643 0 We can quickly see an image of these distances using this code: image(as.matrix(d)) If we order this distance by the labels, we can see that, in general, the twos are closer to each other and the sevens are closer to each other: image(as.matrix(d)[order(y), order(y)]) One thing we notice here is that there appears to be more uniformity in how the sevens are drawn, since they appear to be closer (more red). 62.4 Distance between predictors We can also compute distances between predictors. If \\(N\\) is the number of observations, the distance between two predictors, say 1 and 2, is: \\[ \\mbox{dist}(1,2) = \\sqrt{ \\sum_{i=1}^{N} (x_{i,1}-x_{i,2})^2 } \\] To compute the distance between all pairs of the 784 predictors, we can transpose the matrix first and then use dist: d &lt;- dist(t(x)) dim(as.matrix(d)) #&gt; [1] 784 784 An interesting thing to note here is that if we pick a predictor (a pixel), we can see which pixels are close. That is, they either have ink together or don’t. The distance with the pixel 492 are: d_492 &lt;- as.matrix(d)[492,] We can now see the spatial: image(1:28, 1:28, matrix(d_492, 28, 28)) Not surprisingly, points physically nearby are mathematically closer. Exercises Load the following dataset: data(&quot;tissue_gene_expression&quot;) This dataset includes a matrix x dim(tissue_gene_expression$x) with the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in y table(tissue_gene_expression$y) Compute the distance between each observation and store it in an object d Compare the distance between the first two observations, both cerebellums, the 39th and 40th, both colons, and the 73rd and 74th, both endometriums. See if the tissue of the same type are closer to each other. We see that indeed observations of the same tissue type are closer to each other in the six tissue examples we just examined. Make a plot of all the distances using the image function to see if this pattern is general. Hint: Convert d to a matrix first. "],
["nearest-neighbors.html", "Chapter 63 Nearest neighbors 63.1 Over training 63.2 Over-smoothing 63.3 Picking the \\(k\\) in kNN Exercises", " Chapter 63 Nearest neighbors Let’s get back to our digits data with the two predictors. data(&quot;mnist_27&quot;) mnist_27$test%&gt;% ggplot(aes(x_1, x_2, color = y)) + geom_point() To see how this relates to smoothing, we can think of the conditional probability: \\[ p(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2). \\] The 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(x_1, x_2)\\) are not that close to 0 or 1. So we need to estimate \\(p(x_1, x_2)\\). How do we do this? We can try smoothing. K-nearest neighbors (kNN) is similar to bin smoothing, but it is easier to adapt to multiple dimensions. We first define the distance between all observations based on the features. Basically, for any point \\((x_1,x_2)\\) for which we want an estimate of \\(p(x_1, x_2)\\), we look for the \\(k\\) nearest points and then take an average of these 0s and 1s associated points. We refer to the set of points used to compute the average and the neighborhood. Due to the connection we described earlier between conditional expectations and conditional probabilities, this gives us an \\(\\hat{p}(x_1,x_2)\\), just like the bin smoother gave us an estimate of a trend. We can now control flexibility of our estimate through \\(k\\): larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and more wiggly estimates. Let’s use our logistic regression as the standard we need to beat. library(caret) fit_glm &lt;- glm(y ~ x_1 + x_2, data=mnist_27$train, family=&quot;binomial&quot;) p_hat_logistic &lt;- predict(fit_glm, mnist_27$test) y_hat_logistic &lt;- factor(ifelse(p_hat_logistic &gt; 0.5, 7, 2)) confusionMatrix(data = y_hat_logistic, reference = mnist_27$test$y)$overall[1] #&gt; Accuracy #&gt; 0.76 Now, lets compare to kNN. We will use the knn3 function from the caret package. Looking at the help file of this package, we can see that we can call it in one of two ways. In the first, we specify a formula and a data frame. The data frame contains all the data to be used. The formula has the from outcome ~ predictor_1 + predictor_2 + predictor_3 and so on. Therefore, we would type y ~ x_1 + x_2. But if we are going to use all the predictors, we can use the . like this y ~ .. The final call looks like this: knn_fit &lt;- knn3(y ~ ., data = mnist_27$train) The second way to call this function is with the first argument being the matrix of predictors and the second a vector of outcomes. So the code would look like this: x &lt;- as.matrix(mnist_27$train[,2:3]) y &lt;- mnist_27$train$y knn_fit &lt;- knn3(x, y) For this function, we also need to pick a parameter: the number of neighbors to include. Let’s start with the default \\(k=5\\). knn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5) In this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance. The predict function for knn produces a probability for each class. So we keep the probability of being a 7 as the estimate \\(\\hat{p}(x_1, x_2)\\) y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;) confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.815 This already improves over the logistics model. To see why this is case, we will plot \\(\\hat{p}(x_1, x_2)\\). In the estimate, we see some islands of blue in the red area. Intuitively, this does not make much sense. This is due to what we call over training. Note that we have higher accuracy in the train set compared to the test set: y_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = &quot;class&quot;) confusionMatrix(data = y_hat_knn, reference = mnist_27$train$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.882 y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;) confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.815 63.1 Over training Over-training is at its worst when we set a \\(k=1\\). With \\(k=1\\) the estimate for each \\((x_1, x_2)\\) in the training set is obtained with just the \\(y\\) corresponding to that point. So, in this case, if the \\((x_1, x_2)\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself (if the predictors are not unique and have different outcomes, then we can’t predict perfectly). Here we fit a kNN model with \\(k=1\\): knn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1) y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = &quot;class&quot;) confusionMatrix(data=y_hat_knn_1, reference=mnist_27$train$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.995 However, the test set accuracy is actually worse than logistics regression: y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = &quot;class&quot;) confusionMatrix(data=y_hat_knn_1, reference=mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.735 We can see the over-fitting problem in this figure. The black curves denote the decision rule boundaries. The estimate \\(\\hat{p}(x_1, x_2)\\) follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points \\((x_1, x_2)\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions. 63.2 Over-smoothing Although not as badly as with previous examples, we saw that with \\(k=5\\) we also over-trained. Hence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k=401\\). knn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401) y_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = &quot;class&quot;) confusionMatrix(data=y_hat_knn_401, reference=mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.79 This turns out to be similar to logistic regression: This size of \\(k\\) is so large that it does not permit enough flexibility. We call this over smoothing. 63.3 Picking the \\(k\\) in kNN So how do we pick \\(k\\)? Let’s repeat what we did above but for different values of \\(k\\): ks &lt;- seq(3, 251, 2) Now we use the map_df function to repeat the above for each one. For comparative purposes, we will compute the accuracy by using both the training set (incorrect) and the test set (correct): library(purrr) accuracy &lt;- map_df(ks, function(k){ fit &lt;- knn3(y ~ ., data = mnist_27$train, k = k) y_hat &lt;- predict(fit, mnist_27$train, type = &quot;class&quot;) cm_train &lt;- confusionMatrix(data = y_hat, reference = mnist_27$train$y) train_error &lt;- cm_train$overall[&quot;Accuracy&quot;] y_hat &lt;- predict(fit, mnist_27$test, type = &quot;class&quot;) cm_test &lt;- confusionMatrix(data = y_hat, reference = mnist_27$test$y) test_error &lt;- cm_test$overall[&quot;Accuracy&quot;] list(train = train_error, test = test_error) }) We can now plot the accuracy against the value of \\(k\\): First, note that the accuracy versus \\(k\\) plot is quite jagged. We do not expect this because small changes in \\(k\\) should not affect the algorithm’s performance too much. The jaggedness is explained by the fact that the accuracy is computed on this sample and therefore is a random variable. This demonstrates why we prefer to minimize the expectation loss rather than the loss we observe with one dataset. We will soon learn a better way of estimating this expected loss. Despite the noise present in the plot above, we still see a general pattern. Low values of \\(k\\) give low test set accuracy but high train set accuracy, which is evidence of over-training. Large values of \\(k\\) result in low accuracy, which is evidence of over-smoothing. The maximum is achieved somewhere between 25 and 41 with a maximum accuracy of 0.85. In fact, the resulting estimate with \\(k=41\\) looks quite similar to the true conditional probability: p1 &lt;- plot_cond_prob() + ggtitle(&quot;True conditional probability&quot;) knn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 41) p2 &lt;- plot_cond_prob(predict(knn_fit, newdata = mnist_27$true_p)[,2]) + ggtitle(&quot;kNN-41 estimate&quot;) grid.arrange(p1, p2, nrow=1) The final accuracy for this value of \\(k\\) is: max(accuracy$test) #&gt; [1] 0.86 So is this what we should expect if we apply this algorithm in the real world? The answer is no because we broke a golden rule of machine learning: we selected the \\(k\\) using the test set. So how do we select the \\(k\\) in the real world? In the next section, we introduce the important concept of cross validation which provides a way to estimate the expected loss for any given method using only the training set. Exercises Earlier, we used logistic regression to predict sex from height. Use kNN to do the same. Use the code described in this chapter to select the \\(F_1\\) measure and plot it against \\(k\\). Compare to the \\(F_1\\) of about 0.6 we obtained with regression. Here we will use the same gene expression example used in the Distance Chapter exercises. You can load it like this: data(&quot;tissue_gene_expression&quot;) Split the data in training and test sets, then see what accuracy you obtain. Try it for \\(k = 1, 3, \\dots, 11\\). "],
["the-caret-package.html", "Chapter 64 The caret package 64.1 The train functon 64.2 Cross validation 64.3 Example: fitting with loess", " Chapter 64 The caret package We have already learned about regression, logistic regression, and kNN as machine learning algorithms. In later sections, we learn several others. And this is just a small subset of all the algorithms out there. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The caret package tries to consolidate these differences and provide consistency. It currently includes 237 different methods which are summarized here. Keep in mind that caret does not include the packages and, to implement a package through caret, you still need to install the library. The required package for each method is included here. The caret package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this incredibly helpful package. We will use the 2 or 7 example to illustrate: data(&quot;mnist_27&quot;) 64.1 The train functon The train function lets us train different algorithms using similar syntax. So, for example, we can type: library(caret) train_glm &lt;- train(y ~ ., method = &quot;glm&quot;, data = mnist_27$train) train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train) To make predictions, we can use the output of this function directly without needing to look at the specifics of predict.glm and predict.knn. Instead, we can learn how to obtain predictions from predict.train. So the code looks the same for both methods: y_hat_glm &lt;- predict(train_glm, mnist_27$test, type = &quot;raw&quot;) y_hat_knn &lt;- predict(train_knn, mnist_27$test, type = &quot;raw&quot;) This permits us to quickly compare the algorithms. For example, we can compare the accuracy like this: confusionMatrix(y_hat_glm, mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.75 confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.84 64.2 Cross validation When an algorithm includes a tuning parameter, train automatically uses cross validation to decide among a few default values. To find out what parameter or parameters are optimized, you can read this or study the output of: getModelInfo(&quot;knn&quot;) We can also use a quick look up like this: modelLookup(&quot;knn&quot;) #&gt; model parameter label forReg forClass probModel #&gt; 1 knn k #Neighbors TRUE TRUE TRUE If we run it with default values: train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train) you can quickly see the results of the cross-validation using the ggplot function. The argument highlight highlights the max: ggplot(train_knn, highlight = TRUE) By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. For the kNN method, the default is to try \\(k=5,7,9\\). To change this we use the tuneGrid parameters. The grid of values must be supplied by a data frame with the parameter names as specified in the modelLookup output. Here, we present an example where we try out 30 values between 9 and 67. To do this with caret, we need to define a column named k, so we use this: data.frame(k = seq(9, 67, 2)). Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples. Since we are fitting \\(30 \\times 25 = 750\\) kNN models, running this code will take several seconds: train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k = seq(9, 71, 2))) ggplot(train_knn, highlight = TRUE) To access the parameter that maximized the accuracy, you can use this: train_knn$bestTune #&gt; k #&gt; 11 29 and the best performing model like this: train_knn$finalModel #&gt; 29-nearest neighbor model #&gt; Training set outcome distribution: #&gt; #&gt; 2 7 #&gt; 379 421 The function predict will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set: confusionMatrix(predict(train_knn, mnist_27$test, type = &quot;raw&quot;), mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.84 If we want to change of how we perform cross validation, we can use the trainControl function. We can make the code above go a bit faster by using, for example, 10-fold cross validation. This means we have 10 samples using 10% of the observations each. We accomplish this using the following code: control &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = .9) train_knn_cv &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k = seq(9, 71, 2)), trControl = control) ggplot(train_knn_cv, highlight = TRUE) We notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy. We can also use the standard deviation bars obtained from the cross validation samples: train_knn$results %&gt;% ggplot(aes(x = k, y = Accuracy)) + geom_line() + geom_point() + geom_errorbar(aes(x = k, ymin = Accuracy - AccuracySD, ymax = Accuracy + AccuracySD)) 64.3 Example: fitting with loess The best fitting kNN model approximates the true conditional probability: However, we do see that the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models we see that we can use the gamLoess method. Here, we see that we need to install the gam package if we have not done so already: install.packages(&quot;gam&quot;) Then we see that we have two parameters to optimize: modelLookup(&quot;gamLoess&quot;) #&gt; model parameter label forReg forClass probModel #&gt; 1 gamLoess span Span TRUE TRUE TRUE #&gt; 2 gamLoess degree Degree TRUE TRUE TRUE We will stick to a degree of 1. But to try out different values for the span, we still have to include a column in the table with the name degree so we can do this: grid &lt;- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1) We will use the default cross validation control parameters. train_loess &lt;- train(y ~ ., method = &quot;gamLoess&quot;, tuneGrid=grid, data = mnist_27$train) ggplot(train_loess, highlight = TRUE) We can see that the method performs similar to kNN: confusionMatrix(data =predict(train_loess, mnist_27$test), reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.85 and produces a smoother estimate of the conditional probability: plot_cond_prob(predict(train_loess, mnist_27$true_p, type = &quot;prob&quot;)[,2]) "],
["cross-validation-1.html", "Chapter 65 Cross validation 65.1 K-fold cross validation Exercises 65.2 Bootstrap Exercises", " Chapter 65 Cross validation In a previous chapter, we described that a common goal of machine learning is to find an algorithm that produces predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the MSE: \\[ \\mbox{MSE} = \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\} \\] When all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this: \\[ \\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\] These two are often referred to as the true error and apparent error respectively. There are two important characteristics of the apparent error we should always keep in mind: Because our data is random, the apparent error is a random variable. For example, the dataset we have may be a random sample from a larger population. So an algorithm having a lower apparent error than another algorithm, may be due to luck. If we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We saw an extreme example of this with k nearest neighbor. Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to \\(B\\) new random samples of the data, none of them used to train the algorithm. As shown in a previous chapter, we think of the true error as: \\[ \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2 \\] with \\(B\\) a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because we only have available one set of outcomes: \\(y_1, \\dots, y_n\\). Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have. To do this, we have to generate a series of different random samples. There are several approaches we can use. But the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error. 65.1 K-fold cross validation The first one we describe is K-fold cross validation. Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow). But we don’t get to see these independent datasets. So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes. We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss. Typical choices are to use 10%-20% of the data for testing. Let’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, nothing! Now this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors \\(k\\) in k-nearest neighbors. Here, we will refer to the set of parameters as \\(\\lambda\\). We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful. For each set of algorithm parameters being considered, we we want an estimate of the MSE and then we will chose the parameters with the smallest MSE. Cross validation provides this estimate. First, before we start the cross validation procedure, it is important to fix all the algorithm parameters. Although we will train the algorithm on the set of training sets, the parameters \\(\\lambda\\) will be the same across all training sets. We will use \\(\\hat{y}_i(\\lambda)\\) to denote the predictors obtained when we use parameters \\(\\lambda\\). So, if we are going to imitate this definition: \\[ \\mbox{MSE}(\\lambda) = \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2 \\] we want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation, we do it \\(K\\) times. In the cartoons, we are showing an example that uses \\(K=5\\). We will eventually end up with \\(K\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M=N/K\\) (we round if \\(M\\) is not a round number) observations at random and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b=1\\). We call this the validation set: Now we can fit the model in the training set, then compute the apparent error on the independent set: \\[ \\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i=1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2 \\] Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take \\(K\\) samples, not just one. In K-cross validation, we randomly split the observations into \\(K\\) non-overlapping sets: Now we repeat the calculation above for each of these sets \\(b=1,\\dots,K\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_K(\\lambda)\\). Then, for our final estimate, we compute the average: \\[ \\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b=1}^K \\hat{\\mbox{MSE}}_b(\\lambda) \\] and obtain an estimate of our loss. A final step would be to select the \\(\\lambda\\) that minimizes the MSE. We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on: We can do cross validation again: and obtain a final estimate of our expected loss. However, note that this means that our entire compute time gets multiplied by \\(K\\). You will soon learn that performing this task takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set. Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the parameters. Now how do we pick the cross validation \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original dataset. However, larger values of \\(K\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of \\(K=5\\) and \\(K=10\\) are popular. One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick \\(K\\) sets of some size at random. One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the Bootstrap. In fact, this is the default approach in the caret package. Below we include an explanation of how it works in general. Exercises Generate a set of random predictors and outcomes like this: set.seed(1996) n &lt;- 1000 p &lt;- 10000 x &lt;- matrix(rnorm(n*p), n, p) colnames(x) &lt;- paste(&quot;x&quot;, 1:ncol(x), sep = &quot;_&quot;) y &lt;- rbinom(n, 1, 0.5) %&gt;% factor() x_subset &lt;- x[ ,sample(p, 100)] Because x and y are completely independent, you should not be able to predict y using x with accuracy larger than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model. Hint: use the caret train function. The results component of the output of train shows you the accuracy. Ignore the warnings. Now, instead of a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the \\(y=1\\) group to those in the \\(y=0\\) group, for each predictor, using a t-test. You can perform this step like this: devtools::install_bioc(&quot;genefilter&quot;) install.packages(&quot;genefilter&quot;) library(genefilter) tt &lt;- colttests(x, y) Create a vector of the p-values and call it pvals. Create an index ind with the column numbers of the predictors that were “statistically significantly” associated with y. Use a p-value cutoff of 0.01 to define “statistically significant”. How many predictors survive this cutoff? Re-run the cross-validation but after redefining x_subset to be the subset of x defined by the columns showing “statistically significant” association with y. What is the accuracy now? Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies. In exercise 3 and 4, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then. What is it? A. The function train estimates accuracy on the same data it uses to train the algorithm. B. We are over-fitting the model by including 100 predictors. C. We used the entire dataset to select the columns used in the model. This step needs to be included as part of the algorithm. The cross-validation was done after this selection. D. The high accuracy is just due to random variability. Advanced. Re-do the cross validation but this time include the selection step in the cross-validation. The accuracy should now be close to 50%. Use the train function to predict tissue from gene expression in the tissue_gene_expression dataset. Use kNN. What k works best? 65.2 Bootstrap Suppose the income distribution of your population is as follows: hist(log10(income)) The population median is: m &lt;- median(income) m #&gt; [1] 45384 Suppose we don’t have access to the entire population, but want to estimate the median \\(m\\). We take a sample of 250 and estimate the population median \\(m\\) with the sample median \\(M\\): set.seed(1) N &lt;- 250 X &lt;- sample(income, N) M &lt;- median(X) M #&gt; [1] 48503 Can we construct a confidence interval? What is the distribution of \\(M\\) ? From a Monte Carlo simulation, we see that the distribution of \\(M\\) is approximately normal with the following expected value and standard error: B &lt;- 10^5 Ms &lt;- replicate(B, { X &lt;- sample(income, N) M &lt;- median(X) }) par(mfrow=c(1,2)) hist(Ms) qqnorm(Ms) qqline(Ms) mean(Ms) #&gt; [1] 45517 sd(Ms) #&gt; [1] 3670 The problem here is that, as we have already described, in practice we do not have access to the distribution. In the past, we have used the central limit theorem. But the CLT we studied applies to averages and here we are interested in the median. The Bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample (with replacement) datasets, of the same sample size as the original dataset. Then we compute the summary statistic, in this case median, on this bootstrap sample. Theory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution: B &lt;- 10^5 M_stars &lt;- replicate(B, { X_star &lt;- sample(X, N, replace = TRUE) M_star &lt;- median(X_star) }) Now we can check how close it is to the actual distribution: qqplot(Ms, M_stars) abline(0,1) We see it is not perfect, but it provides a decent approximation: quantile(Ms, c(0.05, 0.95)) #&gt; 5% 95% #&gt; 39727 51778 quantile(M_stars, c(0.05, 0.95)) #&gt; 5% 95% #&gt; 40471 53080 This is much better than what we get if we mindlessly use the CLT: median(X) + 1.96 * sd(X)/sqrt(N) * c(-1,1) #&gt; [1] 36801 60205 If we know the distribution is normal, we can use the bootstrap to estimate the mean: mean(Ms) + 1.96*sd(Ms)*c(-1,1) #&gt; [1] 38325 52710 mean(M_stars) + 1.96*sd(M_stars)*c(-1,1) #&gt; [1] 40170 55350 Note that we can use ideas similar to those used in the Bootstrap in cross validaion: instead of dividing the data into equal partitions, we simply Boostrap many times. Exercises The createResample function can be used to create bootstrap samples. So, for example, we can create 10 bootstrap samples for the mnist_27 dataset like this: set.seed(1995) indexes &lt;- createResample(mnist_27$train$y, 10) How many times do 3, 4 and 7 appear in the first re-sampled index? We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the re-sampled indexes. Generate a random dataset like this: y &lt;- rnorm(100, 0, 1) Estimate the 75th quantiles, which we know is: qnorm(0.75) with the sample quantile: quantile(y, 0.75) Run a Monte Carlo simulation to learn the expected value and standard error of this random variable. In practice, we can’t run a Monte Carlo simulation because we don’t know if rnorm is being used to simulate the data. Use the bootstrap to estimate the standard error using just the initial sample y. Use 10 bootstrap samples. Redo exercise 4, but with 10,000 bootstrap samples. "],
["generative-models.html", "Chapter 66 Generative models 66.1 Naive Bayes 66.2 Controlling prevalence 66.3 Quadratic Discriminant Analysis 66.4 Linear discriminant analysis 66.5 Connection distance 66.6 Case study: more than three classes Exercises", " Chapter 66 Generative models We have described how, when using squared loss, the conditional expectation/probabilities provide the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability: \\[ p(\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}) \\] We have described several approaches to estimating \\(p(\\mathbf{x})\\). In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as discriminative approaches. However, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful. Methods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models (we model how the entire data, \\(\\mathbf{X}\\) and \\(Y\\), are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe to more specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA). 66.1 Naive Bayes Recall that we can rewrite \\(p(\\mathbf{x})\\) like this: \\[ p(\\mathbf{x}) = \\mbox{Pr}(Y=1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y=1}(\\mathbf{x}) \\mbox{Pr}(Y=1)} { f_{\\mathbf{X}|Y=0}(\\mathbf{x})\\mbox{Pr}(Y=0) + f_{\\mathbf{X}|Y=1}(\\mathbf{x})\\mbox{Pr}(Y=1) } \\] with \\(f_{\\mathbf{X}|Y=1}\\) and \\(f_{\\mathbf{X}|Y=0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y=1\\) and \\(Y=0\\). The formula implies that if we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule. However, this is a big if. As we go forward, we will encounter examples in which \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them. Let’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height. library(caret) data(&quot;heights&quot;) y &lt;- heights$height set.seed(2) test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) train_set &lt;- heights %&gt;% slice(-test_index) test_set &lt;- heights %&gt;% slice(test_index) In this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes \\(Y=1\\) (female) and \\(Y=0\\) (Male). This implies that we can approximate the conditional distributions \\(f_{X|Y=1}\\) and \\(f_{X|Y=0}\\) by simply estimating averages and standard deviations from the data: params &lt;- train_set %&gt;% group_by(sex) %&gt;% summarize(avg = mean(height), sd = sd(height)) params #&gt; # A tibble: 2 x 3 #&gt; sex avg sd #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.5 4.02 #&gt; 2 Male 69.3 3.52 The prevalence, which we will denote with \\(\\pi = \\mbox{Pr}(Y=1)\\), can be estimated from the data with: pi &lt;- train_set %&gt;% summarize(pi=mean(sex==&quot;Female&quot;)) %&gt;% .$pi pi #&gt; [1] 0.229 Now we can use our estimates of average and standard deviation to get an actual rule: x &lt;- test_set$height f0 &lt;- dnorm(x, params$avg[2], params$sd[2]) f1 &lt;- dnorm(x, params$avg[1], params$sd[1]) p_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi)) Our Naive Bayes estimate \\(\\hat{p}(x)\\) looks a lot like our logistic regression estimate: In fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as this one. We can see that they are similar empirically by comparing the two resulting curves. 66.2 Controlling prevalence One useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated \\(f_{X|Y=1}\\), \\(f_{X|Y=0}\\) and \\(\\pi\\). If we use hats to denote the estimates, we can write \\(\\hat{p}(x)\\) as: \\[ \\hat{p}(x)= \\frac{\\hat{f}_{X|Y=1}(x) \\hat{\\pi}} { \\hat{f}_{X|Y=0}(x)(1-\\hat{\\pi}) + \\hat{f}_{X|Y=1}(x)\\hat{\\pi} } \\] As we discussed earlier, our sample has a much lower prevalence, 0.229, than the general population. So if we use the rule \\(\\hat{p}(x)&gt;0.5\\) to predict females, our accuracy will be affected due to the low sensitivity: y_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex)) #&gt; [1] 0.263 Again, this is because the algorithm gives more weight to specificity to account for the low prevalence: specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex)) #&gt; [1] 0.953 This is due mainly to the fact that \\(\\hat{\\pi}\\) is substantially less than 0.5, so we tend to predict Male more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity. The Naive Bayes approach gives us a direct way to correct this since we can simply force \\(\\hat{pi}\\) to be, for example, \\(\\pi\\). So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change \\(\\hat{\\pi}\\): p_hat_bayes_unbiased &lt;- f1*0.5 / (f1*0.5 + f0*(1-0.5)) y_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased&gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) Note the difference in sensitivity with a better balance: sensitivity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex)) #&gt; [1] 0.712 specificity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex)) #&gt; [1] 0.821 The new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights: qplot(x, p_hat_bayes_unbiased, geom = &quot;line&quot;) + geom_hline(yintercept = 0.5, lty = 2) + geom_vline(xintercept = 67, lty = 2) 66.3 Quadratic Discriminant Analysis Quadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y=1}(x)\\) and \\(p_{\\mathbf{X}|Y=0}(\\mathbf{x})\\) are multivariate normal. The simple example we described above is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example. data(&quot;mnist_27&quot;) In this case, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y=1\\) and \\(Y=0\\). Once we have these, we can approximate the distributions \\(f_{X_1,X_2|Y=1}\\) and \\(f_{X_1, X_2|Y=0}\\). We can easily estimate parameters from the data: params &lt;- mnist_27$train %&gt;% group_by(y) %&gt;% summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), sd_1= sd(x_1), sd_2 = sd(x_2), r = cor(x_1,x_2)) params #&gt; # A tibble: 2 x 6 #&gt; y avg_1 avg_2 sd_1 sd_2 r #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 0.129 0.283 0.0702 0.0578 0.401 #&gt; 2 7 0.234 0.288 0.0719 0.105 0.455 Here we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points): mnist_27$train %&gt;% mutate(y = factor(y)) %&gt;% ggplot(aes(x_1, x_2, fill = y, color=y)) + geom_point(show.legend = FALSE) + stat_ellipse(type=&quot;norm&quot;, lwd = 1.5) This defines the following estimate of \\(f(x_1, x_2)\\). We can use the caret package to fit the model and obtain predictors. library(caret) train_qda &lt;- train(y ~ ., method = &quot;qda&quot;, data = mnist_27$train) We see that we obtain relatively good accuracy: y_hat &lt;- predict(train_qda, mnist_27$test) confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.82 The estimated conditional probability looks relatively good, although it does not fit as well as the kernel smoothers: One reason QDA does not work as well as the kernel methods is perhaps because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off (notice the slight curvature): mnist_27$train %&gt;% mutate(y = factor(y)) %&gt;% ggplot(aes(x_1, x_2, fill = y, color=y)) + geom_point(show.legend = FALSE) + stat_ellipse(type=&quot;norm&quot;) + facet_wrap(~y) QDA worked well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs and 2 correlations. How many parameters would we have if instead of 2 predictors, we had 10? The main problem comes from estimating correlations for 10 of predictors. With 10, we have 45 correlations for each class. In general, the formula is \\(K\\times p(p-1)/2\\) which gets big fast. Once the number of parameters approaches the size of our data, the method becomes unpractical due to overfitting. 66.4 Linear discriminant analysis A relatively simple solution to the problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate. In this case, we would compute just one pair of standard deviations and one correlation, so the parameters would look something like this: params &lt;- mnist_27$train %&gt;% group_by(y) %&gt;% summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), sd_1= sd(x_1), sd_2 = sd(x_2), r = cor(x_1,x_2)) params &lt;-params %&gt;% mutate(sd_1 = mean(sd_1), sd_2=mean(sd_2), r=mean(r)) params #&gt; # A tibble: 2 x 6 #&gt; y avg_1 avg_2 sd_1 sd_2 r #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 0.129 0.283 0.0710 0.0813 0.428 #&gt; 2 7 0.234 0.288 0.0710 0.0813 0.428 The distributions now look like this: Now the size of the ellipses as well as the angle are the same. This is because they have the same standard deviations and correlations. When we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method linear discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function. In this case, the lack of flexibility does not permit us to capture the nonlinearity in the true conditional probability function. We can fit the model using caret: train_lda &lt;- train(y ~ ., method = &quot;lda&quot;, data = mnist_27$train) y_hat &lt;- predict(train_lda, mnist_27$test) confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.75 66.5 Connection distance The normal density is: \\[ p(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\} \\] If we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get: \\[ - \\frac{(x-\\mu)^2}{\\sigma^2} \\] which is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations. 66.6 Case study: more than three classes We will briefly give a slightly more complex example: one with 3 classes instead of 2. We first create a dataset similar to the 2 or 7 dataset, except now we have 1s, 2s and 7s. if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist() set.seed(3456) index_127 &lt;- sample(which(mnist$train$labels %in% c(1,2,7)), 2000) y &lt;- mnist$train$labels[index_127] x &lt;- mnist$train$images[index_127,] index_train &lt;- createDataPartition(y, p=0.8, list = FALSE) ## get the quandrants #temporary object to help figure out the quandrants row_column &lt;- expand.grid(row=1:28, col=1:28) upper_left_ind &lt;- which(row_column$col &lt;= 14 &amp; row_column$row &lt;= 14) lower_right_ind &lt;- which(row_column$col &gt; 14 &amp; row_column$row &gt; 14) #binarize the values. Above 200 is ink, below is no ink x &lt;- x &gt; 200 #cbind proportion of pixels in upper right quandrant and ##proportion of pixes in lower rigth quandrant x &lt;- cbind(rowSums(x[ ,upper_left_ind])/rowSums(x), rowSums(x[ ,lower_right_ind])/rowSums(x)) train_set &lt;- data.frame(y = factor(y[index_train]), x_1 = x[index_train,1], x_2 = x[index_train,2]) test_set &lt;- data.frame(y = factor(y[-index_train]), x_1 = x[-index_train,1], x_2 = x[-index_train,2]) Here is the training data: train_set %&gt;% ggplot(aes(x_1, x_2, color=y)) + geom_point() We use the caret package to train the QDA model: train_qda &lt;- train(y ~ ., method = &quot;qda&quot;, data = train_set) Now we estimate three conditional probabilities (although they have to add to 1): predict(train_qda, test_set, type = &quot;prob&quot;) %&gt;% head() #&gt; 1 2 7 #&gt; 1 0.2223 0.660 0.1180 #&gt; 2 0.1926 0.454 0.3539 #&gt; 3 0.6275 0.322 0.0505 #&gt; 4 0.0462 0.101 0.8529 #&gt; 5 0.2167 0.623 0.1604 #&gt; 6 0.1267 0.335 0.5383 And our predictions are one of the three classes: predict(train_qda, test_set) #&gt; [1] 2 2 1 7 2 7 2 1 7 1 7 7 2 7 7 1 1 7 1 7 7 1 7 7 7 7 1 1 1 2 1 7 2 7 1 #&gt; [36] 7 2 7 2 7 7 7 7 1 7 7 1 2 2 7 1 7 2 2 1 7 7 2 2 1 7 1 1 1 2 2 1 1 1 2 #&gt; [71] 7 1 7 7 2 1 7 1 7 2 7 1 2 2 1 2 2 2 1 1 1 7 1 2 1 7 2 2 2 7 1 1 1 7 1 #&gt; [106] 7 7 7 2 7 1 7 7 2 1 2 1 1 2 1 2 2 1 2 1 7 1 1 2 1 7 2 7 1 2 1 2 1 2 1 #&gt; [141] 1 1 7 2 2 7 1 1 2 2 2 7 7 1 7 1 7 7 2 1 2 7 7 1 7 7 7 1 7 7 2 2 1 2 2 #&gt; [176] 1 2 2 2 1 2 7 2 7 7 7 1 7 7 2 2 1 2 1 2 7 1 7 7 1 1 1 7 1 1 7 2 1 7 7 #&gt; [211] 2 2 1 7 2 2 2 2 7 2 2 1 2 2 1 1 1 7 1 7 1 7 7 7 2 7 1 1 2 2 1 7 1 7 7 #&gt; [246] 7 1 1 7 1 1 7 2 1 2 1 2 1 7 1 7 1 2 7 7 7 7 7 2 1 7 1 7 2 1 7 7 1 7 7 #&gt; [281] 7 2 7 1 2 7 2 2 7 2 2 7 2 1 2 1 1 1 7 1 1 7 7 1 7 2 1 7 7 7 7 1 2 2 7 #&gt; [316] 7 1 1 7 1 2 1 2 1 7 7 1 1 1 7 1 2 7 7 1 1 7 2 7 7 7 1 7 7 7 7 7 2 2 1 #&gt; [351] 7 7 2 1 2 2 7 7 1 7 7 1 1 7 7 1 1 1 2 1 7 2 7 2 7 1 2 2 1 1 7 2 7 2 1 #&gt; [386] 2 7 7 7 2 7 1 1 7 1 7 2 7 7 #&gt; Levels: 1 2 7 So the confusion matrix has a 3 by 3 table: confusionMatrix(predict(train_qda, test_set), test_set$y) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 1 2 7 #&gt; 1 111 17 7 #&gt; 2 14 80 17 #&gt; 7 19 25 109 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.752 #&gt; 95% CI : (0.706, 0.794) #&gt; No Information Rate : 0.361 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.627 #&gt; Mcnemar&#39;s Test P-Value : 0.0615 #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: 1 Class: 2 Class: 7 #&gt; Sensitivity 0.771 0.656 0.820 #&gt; Specificity 0.906 0.888 0.835 #&gt; Pos Pred Value 0.822 0.721 0.712 #&gt; Neg Pred Value 0.875 0.854 0.902 #&gt; Prevalence 0.361 0.306 0.333 #&gt; Detection Rate 0.278 0.201 0.273 #&gt; Detection Prevalence 0.338 0.278 0.383 #&gt; Balanced Accuracy 0.838 0.772 0.827 The actuary is: confusionMatrix(predict(train_qda, test_set), test_set$y)$overal[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.752 For sensitivity and specificity, we have a pair of values for each class. To define these terms, we need a binary outcome. We therefore have three columns: one for each class as the positives and the other two as the negatives. We can visualize what parts of the region are called 1, 2 and 7: GS &lt;- 150 new_x &lt;- expand.grid(x_1 = seq(min(train_set$x_1), max(train_set$x_1), len=GS), x_2 = seq(min(train_set$x_2), max(train_set$x_2), len=GS)) new_x %&gt;% mutate(y_hat = predict(train_qda, new_x)) %&gt;% ggplot(aes(x_1, x_2, color = y_hat, z = as.numeric(y_hat))) + geom_point(size = 0.5, pch = 16) + stat_contour(breaks=c(1.5, 2.5),color=&quot;black&quot;) + guides(colour = guide_legend(override.aes = list(size=2))) Here is what it looks like for LDA: train_lda &lt;- train(y ~ ., method = &quot;lda&quot;, data = train_set) confusionMatrix(predict(train_lda, test_set), test_set$y)$overal[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.664 The accuracy is much worse because the model is more rigid: The results for kNN are much better: train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(15, 51, 2)), data = train_set) confusionMatrix(predict(train_knn, test_set), test_set$y)$overal[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.769 with much better accuracy now. new_x %&gt;% mutate(y_hat = predict(train_knn, new_x)) %&gt;% ggplot(aes(x_1, x_2, color = y_hat, z = as.numeric(y_hat))) + geom_point(size = 0.5, pch = 16) + stat_contour(breaks=c(1.5, 2.5),color=&quot;black&quot;) + guides(colour = guide_legend(override.aes = list(size=2))) Note that the limitations of LDA are to the lack of fit of the normal assumption: train_set %&gt;% mutate(y = factor(y)) %&gt;% ggplot(aes(x_1, x_2, fill = y, color=y)) + geom_point(show.legend = FALSE) + stat_ellipse(type=&quot;norm&quot;) Generative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class. Exercises We are going to apply LDA and QDA to the tissue_gene_expression dataset. We will start with simple examples based on this dataset and then develop a realistic example. Create a dataset with just the cerebellums and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns. set.seed(1993) data(&quot;tissue_gene_expression&quot;) ind &lt;- which(tissue_gene_expression$y %in% c(&quot;cerebellum&quot;, &quot;hippocampus&quot;)) y &lt;- droplevels(tissue_gene_expression$y[ind]) x &lt;- tissue_gene_expression$x[ind, ] x &lt;- x[, sample(ncol(x), 10)] Use the train function to estimate the accuracy of LDA. In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimate means of both distribution. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm. Repeat exercises 1 with QDA. Does it have a higher accuracy than LDA? Are the same predictors (genes) driving the algorithm? One thing we see in the previous plot is that the value of predictors correlate in both groups: some predictors are low in both groups while others high in both groups. The mean value of each predictor or colMeans(x) is not informative or useful for prediction and often for interpretation purposes it is useful to center or scale each column. This can be achieved with the preProcessing argument in train. Re-run LDA with preProcessing = &quot;scale&quot;. Note that accuracy does not change but see how it is easier to identify the predictors that differ more between groups in the plot made in figure 3. In the previous exercises we saw that both approaches worked well. Plot the predictor values for the two genes with the largest differences between the two groups in a scatter plot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches. Color the points by the outcome. Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types. set.seed(1993) data(&quot;tissue_gene_expression&quot;) y &lt;- tissue_gene_expression$y x &lt;- tissue_gene_expression$x x &lt;- x[, sample(ncol(x), 10)] What accuracy do you get with LDA? We see that the results are slightly worse. Use the confusionMatrix function to learn what type of errors we are making. Plot an image of the centers of the seven 10-dimensional normal distributions "],
["trees-and-random-forests.html", "Chapter 67 Trees and Random Forests 67.1 The curse of dimensionality 67.2 Classification and Regression Trees (CART) 67.3 Regression tree 67.4 Classification (decision) trees 67.5 Random Forests Exercises", " Chapter 67 Trees and Random Forests 67.1 The curse of dimensionality We described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. For example, with the digits example \\(p=784\\), so we would have over 600,000 parameters with LDA and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space. A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility. For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10-th of data. Then it’s easy to see that our windows have to be of size 0.1: Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point: If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{10} \\approx .316\\): Using the same logic, if we want to include 10% of the data in a three dimensional space, then the side of each cube is \\(\\sqrt[3]{10} \\approx 4.64\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 (including all the data and no longer smoothing) quickly: p &lt;- 1:100 qplot(p, .1^(1/p), ylim = c(0,1)) So by the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset. Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on Regression and Decision Trees and their extension to Random Forests. 67.2 Classification and Regression Trees (CART) To motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids: data(&quot;olive&quot;) head(olive) #&gt; region area palmitic palmitoleic stearic oleic linoleic #&gt; 1 Southern Italy North-Apulia 10.75 0.75 2.26 78.2 6.72 #&gt; 2 Southern Italy North-Apulia 10.88 0.73 2.24 77.1 7.81 #&gt; 3 Southern Italy North-Apulia 9.11 0.54 2.46 81.1 5.49 #&gt; 4 Southern Italy North-Apulia 9.66 0.57 2.40 79.5 6.19 #&gt; 5 Southern Italy North-Apulia 10.51 0.67 2.59 77.7 6.72 #&gt; 6 Southern Italy North-Apulia 9.11 0.49 2.68 79.2 6.78 #&gt; linolenic arachidic eicosenoic #&gt; 1 0.36 0.60 0.29 #&gt; 2 0.31 0.61 0.29 #&gt; 3 0.31 0.63 0.29 #&gt; 4 0.50 0.78 0.35 #&gt; 5 0.50 0.80 0.46 #&gt; 6 0.51 0.70 0.44 names(olive) #&gt; [1] &quot;region&quot; &quot;area&quot; &quot;palmitic&quot; &quot;palmitoleic&quot; &quot;stearic&quot; #&gt; [6] &quot;oleic&quot; &quot;linoleic&quot; &quot;linolenic&quot; &quot;arachidic&quot; &quot;eicosenoic&quot; For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors. table(olive$region) #&gt; #&gt; Northern Italy Sardinia Southern Italy #&gt; 151 98 323 We remove the area column because we won’t use it as a predictor. olive &lt;- select(olive, -area) Let’s very quickly try to predict the region using kNN: library(caret) fit &lt;- train(region ~ ., method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(1, 15, 2)), data = olive) ggplot(fit) We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region: olive %&gt;% gather(fatty_acid, percentage, -region) %&gt;% ggplot(aes(region, percentage, fill = region)) + geom_boxplot() + facet_wrap(~fatty_acid, scales = &quot;free&quot;) We see that eicosenoic is only present in Southern Italy and that linolenic separates Northern Italy from Sardinia. This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for these two predictors: p &lt;- olive %&gt;% ggplot(aes(eicosenoic, linoleic, color = region)) + geom_point() p We can, by eye, construct a prediction rule that partitions the predictor space like this: p + geom_vline(xintercept = 0.065, lty = 2) + geom_segment(x = -0.2, y = 10.535, xend = 0.065, yend = 10.535, color = &quot;black&quot;, lty = 2) Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linolenic is larger than \\(10.535\\), predict Sardinia and if lower, predict Northern Italy. We can draw this decision tree like this: Decision trees like this are often used in practice. For example, to decide if a person is at risk of having a heart attack, doctors use the following: (Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184) The general idea of the methods we are describing is to define an algorithm that uses data to create these trees. Regression and decision trees operate by predicting an outcome variable \\(Y\\) by partitioning predictor space. 67.3 Regression tree When the outcome is continuous, we call this method regression trees. We will use a continuous case, the 2008 poll data introduced earlier, to describe the basic idea of how we build these algorithms. We will try to estimate the conditional expectation \\(f(x) = \\mbox{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day. data(&quot;polls_2008&quot;) qplot(day, margin, data = polls_2008) The general idea here is to build a decision tree and, at end of each node, we will have a different prediction \\(\\hat{Y}\\). The regression tree model then: Partitions feature space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For every observation that falls within region \\(x \\in R_j\\), predict with the average of the training observations \\(Y_i\\) in the region: \\(x_i \\in R_j\\). But how do we decide on the partition \\(R_1, R_2, \\ldots, R_M\\) and how do we choose \\(M\\)? Regression trees create partitions recursively. So suppose we already have a partition. We then have to decide what predictor \\(j\\) to use to make the next partition and where to make it. Imagine then that we already have a partition so that every observation \\(i\\) is in exactly one of these partitions. For each of these partitions, we will divide further using the algorithm described below. Find predictor \\(j\\) and value \\(s\\) that define two new partitions \\(R_1(j,s)\\) and \\(R_2(j,s)\\) that split our observations into: \\[ R_1(j,s) = \\{\\mathbf{X} \\mid X_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{\\mathbf{X} \\mid X_j \\geq s\\} \\] and then define \\(\\bar{y}_{R_1}\\) and \\(\\bar{y}_{R_2}\\) as the averages of the observations in both those partitions. Now we find the predictor \\(j\\) and split location \\(s\\) that minimizes the residual sum of square (RSS): \\[ \\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2 \\] This is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the rpart function in the rpart package. fit &lt;- rpart(margin ~ ., data = polls_2008) Here. there is only one predictor. So we do not have to decide which predictor \\(j\\) to split by. We simply have to decide what value \\(s\\) we use to split. We can visually see where the splits were made: plot(fit, margin = 0.1) text(fit, cex = 0.75) The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5 respectively, and so on. We end up with 8 partitions. The final estimate \\(\\hat{f}(x)\\) looks like this: polls_2008 %&gt;% mutate(y_hat = predict(fit)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) Now why did the algorithm stop partitioning at 8? There are some details of the algorithm we did not explain above. Complexity parameter: Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the complexity parameter (cp). The RSS must improve by a factor of cp for the new partition to be added. minsplit: The algorithm sets a minimum number of observations to be partitioned. The default is 20. minbucket: The algorithm also sets a minimum on the number of observations in each partition. If the optimal split results in a partition with less observations than this minimum, it is not considered. The default is round(minsplit/3) As expected, if we set cp = 0 and minsplit=2, then our prediction is our original data: fit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0, minsplit = 2)) polls_2008 %&gt;% mutate(y_hat = predict(fit)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) Note that we can prune tree by snipping of partitions that do not meet a cp criterion: pruned_fit &lt;- prune(fit, cp = 0.01) polls_2008 %&gt;% mutate(y_hat = predict(pruned_fit)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) So how do we pick cp? We can use cross validation just like with any tuning parameter. library(caret) train_rpart &lt;- train(margin ~ ., method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)), data = polls_2008) ggplot(train_rpart) To see the resulting tree, we access the finalModel and plot it: plot(train_rpart$finalModel, margin = 0.1) text(train_rpart$finalModel, cex = 0.75) And because we only have one predictor, we can actually plot \\(\\hat{f}(x)\\): polls_2008 %&gt;% mutate(y_hat = predict(train_rpart)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) 67.4 Classification (decision) trees Classification, or decision trees, are used in classification problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with categorical data. The first difference is that rather than taking the average in each partition, we predict with whatever class is the most common among the training set observations within the partition. The second is that we can no longer use RSS to decide on the partition. While we could use a naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the Gini Index and Entropy. If we define \\(\\hat{p}_{m,k}\\) as the proportion of observations in partition \\(m\\) that are of class \\(k\\), then the Gini Index is defined as: \\[ \\mbox{Gini} = \\sum_{k=1}^K \\hat{p}_{m,k}(1-\\hat{p}_{m,k}) \\] and Entropy is defined as: \\[ \\mbox{Entropy} = -\\sum_{k=1}^K \\hat{p}_{m,k}\\log(\\hat{p}_{m,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0 \\] Both of these seek to partition observations into subsets that have the same class. Note that if a partition has only one class, say the first one, then \\(\\hat{p}{m,1} = 1, \\hat{p}{m,2} = 0, \\dots, \\hat{p}{m,K} = 0\\) and both the Gini Index and Entropy are 0. Let us look at how a classification tree performs on the digits example we examined before: We can see that the accuracy is better than regression, but is not as good as the kernel methods: confusionMatrix(predict(train_rpart, mnist_27$test), mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.82 The plot of the estimate conditional probability shows us the limitations of classification trees: With decision trees, the boundary can’t be smooth. Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear model. They are easy to visualize (if small enough). Finally, they (maybe) model human decision processes and don’t require that dummy predictors for categorical variables be used. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. It may not always be the best performing method since it is not very flexible and is highly unstable to changes in training data. Random Forests, explained next, improve on several of these shortcomings. 67.5 Random Forests Random Forests are a very popular approach that address the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this. The first trick is bootstrap aggregation or bagging. The general scheme for bagging is as follows: 1. Build many decision trees \\(T_1, T_2, \\dots, T_B\\) using the training set. We later explain how we assure they are different. 2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\). 3. For continuous outcomes, predict with \\(\\hat{y} = \\frac{1}{B} \\sum_{j=1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_T\\)). So how do we get many decision trees from a single training set? For this, we use the bootstrap. To create \\(T_j, \\, j=1,\\ldots,B\\) from training set of size \\(N\\): Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. Build a decision tree from bootstrap training set. Here is the Random Forest estimate of the 2008 polls data: library(randomForest) fit &lt;- randomForest(margin~., data = polls_2008) We can see the algorithm improves as we added more trees: plot(fit) In this case, we see that by the time we get to 200 trees, the algorithm is not changing much. But note that more complex problems will require more trees to converge. polls_2008 %&gt;% mutate(y_hat = predict(fit, newdata = polls_2008)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_line(aes(day, y_hat), col=&quot;red&quot;) The averaging is what permits estimates that are not step functions. The following animation helps illustrate this procedure. In the animated figure you see each of \\(b=1,\\dots,50\\) bootstrap samples appear in order. For each one we see the tree that is fitted, and in blue we see the result of bagging the trees up to that point. Here is the Random Forest fit for our digits example based on two predictors: library(randomForest) train_rf &lt;- randomForest(y ~ ., data=mnist_27$train) confusionMatrix(predict(train_rf, mnist_27$test), mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.79 Here is what the conditional probabilities look like: plot_cond_prob(predict(train_rf, mnist_27$true_p, type = &quot;prob&quot;)[,2]) We can train the parameters of the Random Forest. Below, we use the caret package to optimize over the minimum node size: fit &lt;- train(y ~ ., method = &quot;Rborist&quot;, tuneGrid = data.frame(predFixed = 2, minNode = c(3, 50)), data = mnist_27$train) confusionMatrix(predict(fit, mnist_27$test), mnist_27$test$y)$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.795 We can control the “smoothness” of the Random Forest estimate in several ways. One is to limit the size of each node. We can require the number of points per node to be larger: The second Random Forests feature is to use a random selection of features to split when deciding on partitions. Specifically, when building each tree \\(T_j\\), at each recursive partition only consider a randomly selected subset of predictors to check for best split. A different random choice of predictors is made for each tree. This reduces correlation between trees in forest, thereby improving prediction accuracy. The argument for this tuning parameter in randomForrest is mtry. A disadvantage of Random Forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure variable importance from the Random Forest. This measure counts how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book. The caret package includes the function varImp that extracts variable importnance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section. Exercises Create a simple dataset where the outcomes grows 0.75 units on average for every increase in a predictor. n &lt;- 1000 sigma &lt;- 0.25 x &lt;- rnorm(n, 0, 1) y &lt;- 0.75 * x + rnorm(n, 0, sigma) dat &lt;- data.frame(x = x, y = y) Use rpart to fit a regression tree and save the result to fit. Plot the final tree so that you can see where the partitions occurred. Make a scatter plot of y versus x along with the predicted values based on the fit. Now run Random Forests instead of a regression tree using randomForest from the randomForest package, and remake the scatterplot with the prediction line. Use the function plot to see if the Random Forest has converged or if we need more trees. It seems that the default values for the Random Forest result in an estimate that is too flexible (unsmooth). Re-run the Random Forest but this time with nodesize set at 50 and maxnodes set at 25. Remake the plot. We see that this yields smoother results. Let’s use the train function to help us pick these values. From the caret description of methods we see that we can’t tune the maxnodes parameter or the nodesize argument with randomForests. So we will use the Rborist package and tune the minNode argument. Use the train function to try values minNode &lt;- seq(5, 250, 25). See which value minimizes the estimated RMSE. Make a scatterplot along with the prediction from the best fitted model. Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.05, 0.01). Plot the accuracies to report the results of the best model. Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta? Notice that placentas are called endometriums more often than placenta. Note also that the number of placentas is just six, and that, by default, rpart requires 20 observations before splitting a node. So it is difficult to have a node in which placentas are the majority. Rerun the above analysis but this time permit rpart to split any node by using the argument control = rpart.control(minsplit = 0). Does the accuracy increase? Look at the confusion matrix again. Plot the tree from the best fitting model. We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a Random Forest. Use the train function and the rf method to train a Random Forest. Try out values of mtry ranging from, at least, seq(50, 200, 25). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. This will take several seconds to run. If you want to test it out, try using smaller values with ntree. Set the seed to 1990. Use the function varImp on the output of train and save it to an object called imp. The rpart model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this: ind &lt;- !(fit_rpart$finalModel$frame$var == &quot;&lt;leaf&gt;&quot;) tree_terms &lt;- fit_rpart$finalModel$frame$var[ind] %&gt;% unique() %&gt;% as.character() tree_terms What is the variable importance in the Random Forest call for these predictors? Where do they rank? Advanced: Extract the top 50 predictors based on importance, take a subset of x with just these predictors and apply the function heatmap to see how these genes behave across the tissues. We will introduce the heatmap function in the Cluster Chapter. In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset. "],
["case-study-mnist.html", "Chapter 68 Case study: MNIST 68.1 Preprocessing 68.2 kNN 68.3 Variable importance 68.4 Visual assessments", " Chapter 68 Case study: MNIST Now that we have learned several methods and explored them with illustrative examples, we are going to try them out on a real example: the MNIST digits. We can load this data using the following dslabs package: mnist &lt;- read_mnist() The dataset includes two components, a training set and test set: names(mnist) #&gt; [1] &quot;train&quot; &quot;test&quot; Each of these components includes a matrix with features in the columns: dim(mnist$train$images) #&gt; [1] 60000 784 and vector with the classes as integers: class(mnist$train$labels) #&gt; [1] &quot;integer&quot; table(mnist$train$labels) #&gt; #&gt; 0 1 2 3 4 5 6 7 8 9 #&gt; 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949 Because we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset. We will sample 10,000 random rows from the training set and 1,000 random rows from the test set: set.seed(123) index &lt;- sample(nrow(mnist$train$images), 10000) x &lt;- mnist$train$images[index,] y &lt;- factor(mnist$train$labels[index]) index &lt;- sample(nrow(mnist$train$images), 1000) x_test &lt;- mnist$train$images[index,] y_test &lt;- factor(mnist$train$labels[index]) 68.1 Preprocessing In machine learning, we often transform predictors before running the machine algorithm. We also remove predictors that are clearly not useful. We call these steps preprocessing. Examples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation. We show an example below. We can run the nearZero function to see that several features do not vary much from observation to observation. We can see that there are a large number of features with 0 variability: library(matrixStats) sds &lt;- colSds(x) qplot(sds, bins = 256, color = I(&quot;black&quot;)) This is expected because there are parts of the image that rarely contain writing (dark pixels). The caret packages includes a function that recommends features to be removed due to near zero variance: library(caret) nzv &lt;- nearZeroVar(x) We can see the columns that are removed: image(matrix(1:784 %in% nzv, 28, 28)) So we end up keeping these many columns: col_index &lt;- setdiff(1:ncol(x), nzv) length(col_index) #&gt; [1] 252 Now we are ready to fit some models. Before we start, we need to add column names to the feature matrices as these are required by caret: colnames(x) &lt;- 1:ncol(mnist$train$images) colnames(x_test) &lt;- colnames(mnist$train$images) 68.2 kNN Let’s start with kNN. The first step is to optimize for \\(k\\). Keep in mind that when we run the algorithm, we will have to compute a distance between each observation in the test set and each observation in the training set. These are a lot of computations. We will therefore use k-fold cross validation to improve speed. If we run the following code, the computing time on a standard laptop will be several minutes. control &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = .9) train_knn &lt;- train(x[,col_index], y, method = &quot;knn&quot;, tuneGrid = data.frame(k = c(3,5,7)), trControl = control) ggplot(train_knn) In general, it is a good idea to try a test run with a subset of the data to get an idea of timing before we start running code that might take hours to complete. You can do this as follows: n &lt;- 1000 b &lt;- 2 index &lt;- sample(nrow(x), n) control &lt;- trainControl(method = &quot;cv&quot;, number = b, p = .9) train_knn &lt;- train(x[index ,col_index], y[index,] method = &quot;knn&quot;, tuneGrid = data.frame(k = c(3,5,7)), trControl = control) Then increase n and b to get an idea of how long it takes a function of these values. Once we optimize our algorithm, we can fit it to the entire dataset: fit_knn&lt;- knn3(x[ ,col_index], y, k = 5) The accuracy is almost 0.95! y_hat_knn &lt;- predict(fit_knn, x_test[, col_index], type=&quot;class&quot;) cm &lt;- confusionMatrix(y_hat_knn, factor(y_test)) cm$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.948 We now achieve an accuracy of about 0.95. From the specificity and sensitivity, we also see that 8s are the hardest to detect and the most commonly incorrectly predicted digit is 7. cm$byClass[,1:2] #&gt; Sensitivity Specificity #&gt; Class: 0 1.000 0.996 #&gt; Class: 1 0.991 0.990 #&gt; Class: 2 0.883 0.998 #&gt; Class: 3 0.955 0.996 #&gt; Class: 4 0.928 0.996 #&gt; Class: 5 0.969 0.991 #&gt; Class: 6 0.990 0.999 #&gt; Class: 7 0.958 0.988 #&gt; Class: 8 0.876 1.000 #&gt; Class: 9 0.933 0.990 Now let’s see if we can do even better with Random Forest. With Random Forest, computation time is a challenge. For each Forest, we need to build hundreds of trees. We also have several parameters we can tune. We use the Random Forest implementation in the Rborist package which is faster than the one in the randomForest package Because with Random Forest, the fitting is the slowest part of the procedure rather than the predicting (as with kNN), we will use only 5 fold cross validation. We will also reduce the number of trees that are fit since we are not yet building our final model. Finally, to compute on a smaller dataset, we will take a random sample of the observations when constructing each tree. We can change this number with the nSamp argument. library(Rborist) #&gt; Loading required package: Rcpp #&gt; Rborist 0.1-8 #&gt; Type RboristNews() to see new features/changes/bug fixes. control &lt;- trainControl(method=&quot;cv&quot;, number = 5, p = 0.8) grid &lt;- expand.grid(minNode = c(1) , predFixed = c(10, 15, 35)) train_rf &lt;- train(x[ , col_index], y, method = &quot;Rborist&quot;, nTree = 50, trControl = control, tuneGrid = grid, nSamp = 5000) ggplot(train_rf) train_rf$bestTune #&gt; predFixed minNode #&gt; 2 15 1 Now that we have optimized our tree, we are ready to fit our final model: fit_rf &lt;- Rborist(x[, col_index], y, nTree = 1000, minNode = train_rf$bestTune$minNode, predFixed = train_rf$bestTune$predFixed) y_hat_rf &lt;- factor(levels(y)[predict(fit_rf, x_test[ ,col_index])$yPred]) cm &lt;- confusionMatrix(y_hat_rf, y_test) cm$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.953 Here are some examples of the original images and our calls: With some further tuning, we can get even higher accuracy. 68.3 Variable importance Unfortunately, the Rborist implementation of Random Forest does not yet support importance calculations. So we demonstrate with a quick fit using the randomForest package. library(randomForest) rf &lt;- randomForest(x, y, ntree = 50) The following function computes the importance of each feature: imp &lt;- importance(rf) We can see which features are most being used by plotting an image: image(matrix(imp, 28, 28)) 68.4 Visual assessments An important part of data science is visualizing results to determine why we are failing. How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. We can compare what we get with kNN to Random Forest. Here is kNN: And here is Random Forest: "],
["ensembles.html", "Chapter 69 Ensembles Exercises", " Chapter 69 Ensembles The idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate. In Machine Learning, one can usually greatly improve the final results by combining the results of different algorithms. Here is a simple example where we compute new class probabilities by taking the average of Random Forest and kNN. We can see that the accuracy improves to 0.96: p_rf &lt;- predict(fit_rf, x_test[,col_index])$census p_rf&lt;- p_rf / rowSums(p_rf) p_knn &lt;- predict(fit_knn, x_test[,col_index]) p &lt;- (p_rf + p_knn)/2 y_pred &lt;- factor(apply(p, 1, which.max)-1) confusionMatrix(y_pred, y_test) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 2 3 4 5 6 7 8 9 #&gt; 0 100 0 2 0 1 0 0 0 0 0 #&gt; 1 0 106 2 0 2 0 0 1 2 0 #&gt; 2 0 0 93 2 0 0 0 0 1 0 #&gt; 3 0 0 0 85 0 1 0 0 3 0 #&gt; 4 0 0 1 0 104 0 1 0 0 1 #&gt; 5 0 0 0 1 0 95 0 0 1 2 #&gt; 6 0 0 0 0 0 1 95 0 1 0 #&gt; 7 0 0 5 0 1 0 0 94 0 3 #&gt; 8 0 0 0 0 0 0 0 0 89 0 #&gt; 9 0 1 0 0 3 1 0 1 0 98 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.959 #&gt; 95% CI : (0.945, 0.97) #&gt; No Information Rate : 0.111 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.954 #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 #&gt; Sensitivity 1.000 0.991 0.903 0.966 0.937 0.969 #&gt; Specificity 0.997 0.992 0.997 0.996 0.997 0.996 #&gt; Pos Pred Value 0.971 0.938 0.969 0.955 0.972 0.960 #&gt; Neg Pred Value 1.000 0.999 0.989 0.997 0.992 0.997 #&gt; Prevalence 0.100 0.107 0.103 0.088 0.111 0.098 #&gt; Detection Rate 0.100 0.106 0.093 0.085 0.104 0.095 #&gt; Detection Prevalence 0.103 0.113 0.096 0.089 0.107 0.099 #&gt; Balanced Accuracy 0.998 0.991 0.950 0.981 0.967 0.982 #&gt; Class: 6 Class: 7 Class: 8 Class: 9 #&gt; Sensitivity 0.990 0.979 0.918 0.942 #&gt; Specificity 0.998 0.990 1.000 0.993 #&gt; Pos Pred Value 0.979 0.913 1.000 0.942 #&gt; Neg Pred Value 0.999 0.998 0.991 0.993 #&gt; Prevalence 0.096 0.096 0.097 0.104 #&gt; Detection Rate 0.095 0.094 0.089 0.098 #&gt; Detection Prevalence 0.097 0.103 0.089 0.104 #&gt; Balanced Accuracy 0.994 0.985 0.959 0.968 For these exercises we are going to build several machine learning models for the mnist_27 and then build an ensemble. Exercises Use the training set to build a model with several of the models available from the caret package. For example, you can try these: models &lt;- c(&quot;glm&quot;, &quot;lda&quot;, &quot;naive_bayes&quot;, &quot;svmLinear&quot;, &quot;gamboost&quot;, &quot;gamLoess&quot;, &quot;qda&quot;, &quot;knn&quot;, &quot;kknn&quot;, &quot;loclda&quot;, &quot;gam&quot;, &quot;rf&quot;, &quot;ranger&quot;, &quot;wsrf&quot;, &quot;Rborist&quot;, &quot;avNNet&quot;, &quot;mlp&quot;, &quot;monmlp&quot;, &quot;adaboost&quot;, &quot;gbm&quot;, &quot;svmRadial&quot;, &quot;svmRadialCost&quot;, &quot;svmRadialSigma&quot;) We have not explained many of these but apply them anyway using train with all the default parameters .You might need to install some packages. Keep in mind that you will likely get some warnings. Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models). Now compute accuracy for each model on the test set. Now build an ensemble prediction by majority vote and compute the accuracy of the ensemble. Earlier we computed the accuracy of each method on the training set and noticed they varied. How many of the individual methods do better than the ensemble? Which methods are they? It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object. Now let’s only consider the methods with an estimated accuracy of 0.8 when constructing the ensemble. What is the accuracy now? Advanced: If two methods give results that are the same, ensembling them will not change the results at all. For each pair of metrics compare the percent of time they call the same thing. Then use the heatmap function to visualize the results. Hint: use the method = &quot;binary&quot; argument in the dist function. Advanced: Note that each method can also produce an estimated conditional probability. Instead of majority vote we can take the average of these estimated conditional probabilities. For most methods, we can the use the type = &quot;prob&quot; in the train function. However, some of the methods require you to use the argument trControl=trainControl(classProbs=TRUE) when calling train. Also these methods do not work if classes have numbers as names. Hint: change the levels like this: dat$train$y &lt;- recode_factor(dat$train$y, &quot;2&quot;=&quot;two&quot;, &quot;7&quot;=&quot;seven&quot;) dat$test$y &lt;- recode_factor(dat$test$y, &quot;2&quot;=&quot;two&quot;, &quot;7&quot;=&quot;seven&quot;) "],
["dimension-reduction.html", "Chapter 70 Dimension Reduction 70.1 Preserving distance 70.2 Linear transformations (advnced) 70.3 Orthogogal transformations (advaced) 70.4 Principal Component Analysis 70.5 Iris Example 70.6 MNIST Exercises", " Chapter 70 Dimension Reduction A typical machine learning challenge will include a large number of predictors. This makes the important step of data visualization somewhat challenging. We have shown methods for visualizing univariate and paired data, but plots that reveal relationships between many variables are more complicated in higher dimensionality. For example, to compare each of the 784 features in our predicting digits example, we would have to create, for example, 306,936 scatter plots. Creating one single scatter-plot of the data is impossible due to the high dimensionality. Here we describe a powerful techniques useful for exploratory data analysis, among other things, generally referred to as dimension reduction. The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations. With fewer dimensions, visualization then becomes more feasible. The technique behind it all, the singular value decomposition, is also useful in other contexts. Principal component analysis (PCA) is the approach we will be showing. Before applying PCA to high dimensional dataset, we will motivate the ideas behind with a simple example. 70.1 Preserving distance We consider an example with twin heights. Some pairs are adults the others are children. Here we simulate 100 two dimensional points that represent the number of standard deviations each individual is from the mean height. Each point is a pair of twins. We use the mvrnorm function from the MASS package to simulate bivariate normal data. set.seed(1) library(MASS) n &lt;- 100 x &lt;- rbind(mvrnorm(n/2, c(69,69), matrix(c(9, 9*0.9, 9*0.92, 9*1),2,2)), mvrnorm(n/2, c(55,55), matrix(c(9, 9*0.9, 9*0.92, 9*1),2,2))) A scatter-plot quickly reveals that the correlation is high and that there are two groups of twins: To help with the illustration, think of this as dataset with two features, the two heights, and \\(N\\) observations. For illustrative purposes, we will act as if visualizing two dimensions is too challenging. We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data. For example, that the observations cluster in to two groups: adults and children. Let’s consider a specific challenge: we want a one dimensional summary of our predictors from which we can approximate the distance between any two observations. In the figure below we show the distance between observation 1 and 2, and observation 1 and 51: 1 and 2 are closer: We can compute these distances using dist: d &lt;- dist(x) as.matrix(d)[1,2] #&gt; [1] 3.48 as.matrix(d)[2,51] #&gt; [1] 23.1 This distance is based on two dimension and we need a distance approximation based on just one. Let’s start with the naive approach of simply removing one of the two dimensions. Let’s compare the actual distances to the distance computed with just the first dimensions. z &lt;- x[,1] Here are the approximate distances versus the original distances: plot(dist(x), dist(z)) abline(0,1, col = &quot;red&quot;) The plot looks about the same if we use the second dimension. We obtain a general underestimation. This is actually to be expected because we are adding more positive quantities in the actual distance. If instead we use and average like this: \\[\\sqrt{ \\frac{1}{2} \\sum_{j=1}^2 (X_{i,j}-X_{i,j})^2 }\\] then the underestimation goes away plot(dist(x), dist(z)*sqrt(2)) abline(0, 1, col = &quot;red&quot;) This actually works pretty well and we get an typical difference of: sd(dist(x) - dist(z)*sqrt(2)) #&gt; [1] 1.26 Now, can we pick a one dimensional summary that makes this approximation even better? If we look back at the previous scatter plot, and visualize a line between any pair of points, the length of this line is the distance between the two points. These lines tend to go along the direction of the diagonal. Notice that if we instead plot the difference versus the average: z &lt;- cbind((x[,2] + x[,1])/2, x[,2] - x[,1]) we can see how the distance between points in mostly explained by the first dimension: the average. rafalib::mypar() plot(z, xlim=lim, ylim = lim - mean(lim)) lines(z[c(1,2),], col = &quot;blue&quot;, lwd = 2) lines(z[c(2,51),], col = &quot;red&quot;, lwd = 2) points(z[c(1,2,51),], pch = 16) This means that we can ignore the second dimension and not lose too much information. If the line is completely flat, we lose no information at all. Using the first dimension of this transformed matrix we obtain an even better approximation: plot(dist(x), dist(z[,1])*sqrt(2)) abline(0,1) with the typical difference improved by about 35%: sd(dist(x) - dist(z[,1])*sqrt(2)) #&gt; [1] 0.362 Later we learn that z[,1] is the first principal component of the matrix x. 70.2 Linear transformations (advnced) Note that each row of \\(X\\) was transformed using a linear transformation. For any row \\(i\\), the first entry was \\[Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}\\] with \\(a_{1,1} = 0.5\\) and \\(a_{2,1} = 0.5\\) The second entry was also a linear transformation \\[Z_{i,2} = a_{1,2} X_{i,1} + a_{2,2} X_{i,2}\\] with \\(a_{1,2} = 1\\) and \\(a_{2,2} = -1\\) We can also use linear transformation to get \\(X\\) back from \\(Z\\): \\[X_{i,1} = b_{1,1} Z_{i,1} + b_{2,1} Z_{i,2}\\] with \\(b_{1,2} = 1\\) and \\(b_{2,1} = 0.5\\) and \\[X_{i,2} = b_{2,1} Z_{i,1} + b_{2,2} Z_{i,2}\\] with \\(b_{2,1} = 1\\) and \\(a_{1,2} = -0.5\\) If you are familiar with linear algebra we can write the operation we just performed like this: \\[ Z = X A \\mbox{ with } A = \\, \\begin{pmatrix} 1/2&amp;1\\\\ 1/2&amp;-1\\\\ \\end{pmatrix} \\] And that we can transform back by simply multiplying by \\(A^{-1}\\) as follows: \\[ X = Z A^{-1} \\mbox{ with } A^{-1} = \\, \\begin{pmatrix} 1&amp;1\\\\ 1/21&amp;-1/2\\\\ \\end{pmatrix} \\implies \\] Dimension reduction can often be described as applying a transformation \\(A\\) to a matrix \\(X\\) with many columns that moves the information contained in \\(X\\) to the first few columns of \\(Z=AX\\) then keeping just these few and thus reducing the dimension of the vectors contained in the rows. 70.3 Orthogogal transformations (advaced) Note that we redefined distance above to account for the difference in dimensions. We can actually guarantee that the distance scales remain the same if we re-scale the columns of \\(A\\) to assure that the sum of squares are 1: \\[a_{1,1}^2 + a_{2,1}^2 = 1\\mbox{ and } a_{1,2}^2 + a_{2,2}^2=1\\] and the correlation of the columns is 0: \\[ a_{1,1} a_{1,2} + a_{2,1} a_{2,2} = 0 \\] Remember that if the columns are centered to have average 0, then the sum of squares is equivalent to the variance or standard deviation squared. In the particular example we have been working with, to achieve orthogonality, we multiply the first set of coefficients (first column of \\(A\\)) by \\(\\sqrt{2}\\) and the second by \\(1\\sqrt{2}\\) then we get the same exact distance if we use both dimensions z[,1] &lt;- (x[,1] + x[,2])/sqrt(2) z[,2] &lt;- (x[,2] - x[,1])/sqrt(2) This gives us a transformation that preserves the distance between any two points: plot(dist(x), dist(z)) abline(0, 1, col = &quot;red&quot;) and a great approximation if we use just the first dimension: sd(dist(x) - dist(z[,1])) #&gt; [1] 0.362 In this case \\(Z\\) is called an orthogonal rotation of \\(X\\): it preserves the distances between points. Note that by using the transformation above we can summarize the distance between any two pair of twins with just on dimension. For example, one-dimensional data exploration of the first dimension of \\(Z\\) clearly shows the two groups: qplot(z[,1], bins = 20, color = I(&quot;black&quot;)) We reduced the number of dimensions from two to one with very little loss of information. The reason we were able to do this is because the columns of \\(X\\) were very correlated: cor(x[,1], x[,2]) #&gt; [1] 0.986 and the transformation produced uncorrelated columns with “independent” information in each column: cor(z[,1], z[,2]) #&gt; [1] 0.0506 One way this insight may be useful in a machine learning application is that we can reduce the complexity of a model by using just \\(Z_1\\) rather than both \\(X_1\\) and \\(X_2\\). It is actually common to obtain data with several highly correlated predictors. In these cases PCA, which we describe next, can be quite useful for reducing the complexity of the model being fit. 70.4 Principal Component Analysis In the computation above the total variability in our data can be defined as the sum of squares of the columns. We assume the column are centered so we have: \\[ v_1 = \\frac{1}{N}\\sum_{i=1}^N X_{i,1}^2 \\mbox{ and } v_2 = \\frac{1}{N}\\sum_{i=1}^N X_{i,2}^2 \\] Which we can compute using: colMeans(x^2) #&gt; [1] 3887 3904 We can show, mathematically, that if we apply an orthogonal transformation as above, then the total variation remains the same: sum(colMeans(x^2)) #&gt; [1] 7791 sum(colMeans(z^2)) #&gt; [1] 7791 However, while the variability in the two columns of X is about the same, in the transformed version \\(Z\\) 99% of the variability is included in just the first dimensions: v &lt;- colMeans(z^2) v/sum(v) #&gt; [1] 0.999888 0.000112 The first principal component (PC) of a matrix \\(X\\) is the linear orthogonal transformation of \\(X\\), that maximizes this variability. The function prcomp provides this info: pca &lt;- prcomp(x) pca$rotation #&gt; PC1 PC2 #&gt; [1,] -0.704 -0.710 #&gt; [2,] -0.710 0.704 Note that the first PC is almost the same as that provided by the \\((X_1 + X_2)/\\sqrt(2)\\) we used earlier (except perhaps for a sign change that is arbitrary). The function PCA returns both the rotation needed to transform \\(X\\) so that the variability of the columns is decreasing from most variable to least as well as the resulting new matrix. By default the columns of \\(X\\) are first centered. So, using the matrix multiplication shown above, we have that the following are the same: a &lt;- sweep(x, 2, colMeans(x)) b &lt;- pca$x %*% t(pca$rotation) max(abs(a - b)) #&gt; [1] 5.33e-15 The rotation is orthogonal which means that the inverse is it’s transpose. So we also have that these two are identical: a &lt;- sweep(x, 2, colMeans(x)) %*% pca$rotation b &lt;- pca$x max(abs(a - b)) #&gt; [1] 0 We can visualize these to see how the first component summaries the data. In the plot below red represents high values and blue negative values. Later we learn why we call these weights and patterns): It turns out that we can find this linear transformation not just for two dimensions but for matrices of any dimension \\(p\\). For a multidimensional matrix with \\(X\\) with say, \\(p\\) columns, we can find a transformation that creates \\(Z\\) that preserves distance between rows, but with the variance of the columns in decreasing order. The second column is the second principal component, the third column is the third principal component etc… As in our example, if past \\(k\\) these variances are very small, it means these dimensions have little to contribute to the distance and we can approximate distance between any two points with just \\(k\\) dimensions. 70.5 Iris Example The Iris data is a widely used example. It includes four botanical measurements related to three flower species: names(iris) #&gt; [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; #&gt; [5] &quot;Species&quot; If you print iris$Species you will see that the data is ordered by the species. Let’s compute the distance between each observation. You can clearly see the three species with one species very different from the other two: x &lt;- iris[,1:4] %&gt;% as.matrix() d &lt;- dist(x) image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, &quot;RdBu&quot;))) Our predictors here have four dimensions but three are very correlated: cor(x) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Sepal.Length 1.000 -0.118 0.872 0.818 #&gt; Sepal.Width -0.118 1.000 -0.428 -0.366 #&gt; Petal.Length 0.872 -0.428 1.000 0.963 #&gt; Petal.Width 0.818 -0.366 0.963 1.000 If we apply PCA we should be able to approximate this distance with just two dimensions. Using the summary function we can see the variability explained by each PC: pca &lt;- prcomp(x) summary(pca) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 2.056 0.4926 0.2797 0.15439 #&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521 #&gt; Cumulative Proportion 0.925 0.9777 0.9948 1.00000 The first two dimensions account for 97%. So we should be able to approximate the distance very well. We can visualize the results of PCA: And see that the first pattern is Sepal Length, Petal Length and Petal Width (red) in one direction and Sepal Width (blue) in the other. The second patter is the Sepal Length and Petal Width in one direction (blue) and Petal Length and Petal Width in the other (red). You can see from the weights that the first PC1 drives most the variability and it clearly separates the first third (setosa) from the second two thirds (versicolor and virginica). If you look at the second column of the weights you notice that it somewhat separates versicolor (red) from virgnica (blue). We can see this better by plotting the first two PCs with color representing the species: data.frame(pca$x[,1:2], Species=iris$Species) %&gt;% ggplot(aes(PC1,PC2, fill = Species))+ geom_point(cex=3, pch=21) + coord_fixed(ratio = 1) We see that the first two dimensions preserve the distance: d_approx &lt;- dist(pca$x[,1:2]) plot(d, d_approx) abline(0, 1, col=&quot;red&quot;) This example is more realistic than the first since we showed how we can visualize the data, using two dimensions, when the data was four dimensional. 70.6 MNIST We have been using the written digits example. This dataset had 784 features. Is there any room for data reduction? Can we create simple machine learning algorithms with using fewer features? Let’s load the data: if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist() Because the pixels are so small we expect those to be close to each other on the grid to be correlated, meaning that dimension reduction should be possible. Let’s try PCA. This will take a few seconds as it is a rather large matrix. col_means &lt;- colMeans(mnist$test$images) pca &lt;- prcomp(mnist$train$images) And explore the variance of the PCs: plot(pca$sdev) We can see that the first few PCs already explain a large percent of the variability: summary(pca)$importance[,1:5] %&gt;% knitr::kable() PC1 PC2 PC3 PC4 PC5 Standard deviation 576.823 493.238 459.899 429.856 408.567 Proportion of Variance 0.097 0.071 0.062 0.054 0.049 Cumulative Proportion 0.097 0.168 0.230 0.284 0.332 And just by looking at the first two PCs we see information about the class. Here is a random sample of 2,000 digits: data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], label=factor(mnist$train$label)) %&gt;% sample_n(2000) %&gt;% ggplot(aes(PC1, PC2, fill=label))+ geom_point(cex=3, pch=21) We can also see the linear combinations on the grid to get an idea of what is getting weighted: tmp &lt;- lapply( c(1:4,781:784), function(i){ expand.grid(Row=1:28, Column=1:28) %&gt;% mutate(id=i, label=paste0(&quot;PC&quot;,i), value = pca$rotation[,i]) }) tmp &lt;- Reduce(rbind, tmp) tmp %&gt;% filter(id&lt;5) %&gt;% ggplot(aes(Row, Column, fill=value)) + geom_raster() + scale_y_reverse() + scale_fill_gradientn(colors = brewer.pal(9, &quot;RdBu&quot;)) + facet_wrap(~label, nrow = 1) The higher PCs appear related to unimportant variability in the corners: tmp %&gt;% filter(id&gt;5) %&gt;% ggplot(aes(Row, Column, fill=value)) + geom_raster() + scale_y_reverse() + scale_fill_gradientn(colors = brewer.pal(9, &quot;RdBu&quot;)) + facet_wrap(~label, nrow = 1) Now let’s apply the transformation we learned with the training data to the test data, reduce the dimension and run knn on just a small number of dimensions. We try 36 dimensions since this explains about 80% of the data First fit the model: library(caret) K &lt;- 36 x_train &lt;- pca$x[,1:K] y &lt;- factor(mnist$train$labels) fit &lt;- knn3(x_train, y) Now transform the test set: x_test &lt;- sweep(mnist$test$images, 2, col_means) %*% pca$rotation x_test &lt;- x_test[,1:K] And we are ready to predict and see how we do: y_hat &lt;- predict(fit, x_test, type = &quot;class&quot;) confusionMatrix(y_hat, factor(mnist$test$labels))$overall #&gt; Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull #&gt; 0.974 0.972 0.971 0.977 0.114 #&gt; AccuracyPValue McnemarPValue #&gt; 0.000 NaN With just 36 dimensions we get an accuracy well above 0.95. Exercises We want to explore the tissue_gene_expression predictors by plotting them. data(&quot;tissue_gene_expression&quot;) dim(tissue_gene_expression$x) We want to get an idea of which observations are close to each other, but the predictor are 500 dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type. The predictors for each observation are measured on the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation. We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center. For the first 10 PCs, make a boxplot showing the values for each tissue. Plot the percent variance explained by PC number. Hint: use the summary function. "],
["recommendation-systems.html", "Chapter 71 Recommendation systems 71.1 Movielens data 71.2 Recommendation systems as a machine learning challenge 71.3 Modeling movie effects Exercises", " Chapter 71 Recommendation systems Recommendation systems use ratings that users have given items to make specific recommendations to users. Companies like Amazon, that sell many products to many customers and permit these customers to rate their products, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user. Netflix uses a recommendation system to predict how many stars a user will give a specific movie. One star suggests it is not a good movie, whereas five stars suggests it is an excellent movie. Here, we provide the basics of how these recommendations are predicted, motivated by some of the approaches taken by the winners of the Netflix challenges. On October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced. You can read a good summary of how the winning algorithm was put together here and a more detailed explanation here. We will now show you some of the data analysis strategies used by the winning team. 71.1 Movielens data The Netflix data is not publicly available, but the GroupLens research lab generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the dslabs package: library(dslabs) data(&quot;movielens&quot;) We can see this table is in tidy format with thousands of rows: head(movielens) #&gt; movieId title year #&gt; 1 31 Dangerous Minds 1995 #&gt; 2 1029 Dumbo 1941 #&gt; 3 1061 Sleepers 1996 #&gt; 4 1129 Escape from New York 1981 #&gt; 5 1172 Cinema Paradiso (Nuovo cinema Paradiso) 1989 #&gt; 6 1263 Deer Hunter, The 1978 #&gt; genres userId rating timestamp #&gt; 1 Drama 1 2.5 1260759144 #&gt; 2 Animation|Children|Drama|Musical 1 3.0 1260759179 #&gt; 3 Thriller 1 3.0 1260759182 #&gt; 4 Action|Adventure|Sci-Fi|Thriller 1 2.0 1260759185 #&gt; 5 Drama 1 4.0 1260759205 #&gt; 6 Drama|War 1 2.0 1260759151 Each row represents a rating given by one user to one movie. We can see the number of unique users that provided ratings and for how many unique movies they provided them for: movielens %&gt;% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId)) #&gt; n_users n_movies #&gt; 1 671 9066 If we multiply those two numbers, we get a number larger than 5 million, yet our data table has about 100,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The gather function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let’s show the matrix for a few users: userId Forrest Gump Pulp Fiction Shawshank Redemption Silence of the Lambs Star Wars 13 5.0 3.5 4.5 NA NA 15 1.0 5.0 2.0 5.0 5.0 16 NA NA 4.0 NA NA 17 2.5 5.0 5.0 4.5 3.5 18 NA NA NA NA 3.0 19 5.0 5.0 4.0 3.0 4.0 20 2.0 0.5 4.5 0.5 1.5 You can think of the task of a recommendation system as filling in the NAs in the table above. To see how sparse the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating. This machine learning challenge is more complicated than what we have studied up to now because each outcome \\(Y\\) has a different set of predictors. To see this, note that if we are predicting the rating for movie \\(i\\) by user \\(u\\), in principle, all other ratings related to movie \\(i\\) and by user \\(u\\) may used as predictors, but different users rate a different number of movies and different movies. Furthermore, we may be able to use information from other movies that we have determined are similar to movie \\(i\\) or from users determined to be similar to user \\(u\\). So, in essence, the entire matrix can be used as predictors for each cell. Let’s look at some of the general properties of the data to better understand the challenges. The first thing we notice is that some movies get rated more than others. Here is the distribution: movielens %&gt;% count(movieId) %&gt;% ggplot(aes(n)) + geom_histogram(bins = 30, color = &quot;black&quot;) + scale_x_log10() + ggtitle(&quot;Movies&quot;) This should not surprise us given that there are blockbuster movies watched by millions and artsy, independent movies watched by just a few. Our second observation is that some users are more active than others at rating movies: movielens %&gt;% count(userId) %&gt;% ggplot(aes(n)) + geom_histogram(bins = 30, color = &quot;black&quot;) + scale_x_log10() + ggtitle(&quot;Users&quot;) 71.2 Recommendation systems as a machine learning challenge To see how this is a type of machine learning, notice that we need to build an algorithm with data we have collected that will then be applied outside our control, as users look for movie recommendations. So let’s create a test set to assess the accuracy of the models we implement. library(caret) set.seed(755) test_index &lt;- createDataPartition(y = movielens$rating, times = 1, p = 0.2, list = FALSE) train_set &lt;- movielens[-test_index,] test_set &lt;- movielens[test_index,] To make sure we don’t include users and movies in the test set that do not appear in the training set, we remove these entries using the semi_join function: test_set &lt;- test_set %&gt;% semi_join(train_set, by = &quot;movieId&quot;) %&gt;% semi_join(train_set, by = &quot;userId&quot;) 71.2.1 Loss function The Netflix challenge used the typical error loss. They decided on a winner based on the residual mean squared error (RMSE) on a test set. We define \\(y_{u,i}\\) as the rating for movie \\(i\\) by user \\(u\\) and denote our prediction with \\(\\hat{y}_{i,u}\\). The RMSE is then defined as: \\[ \\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{u,i}^{} \\left( \\hat{y}_{u,i} - y_{u,i} \\right)^2 } \\] with \\(N\\) being the number of user/movie combinations and the sum occurring over all these combinations. Remember that we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good. Let’s write a function that computes the RMSE for vectors of ratings and their corresponding predictors: RMSE &lt;- function(true_ratings, predicted_ratings){ sqrt(mean((true_ratings - predicted_ratings)^2)) } 71.2.2 A first model Let’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. So what number should this prediction be? We can use a model based approach. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this: \\[ Y_{u,i} = \\mu + \\varepsilon_{u,i} \\] with \\(\\varepsilon_{i}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the “true” rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings: mu_hat &lt;- mean(train_set$rating) mu_hat #&gt; [1] 3.54 If we predict all unknown ratings with \\(\\hat{\\mu}\\) or mu above, we obtain the following RMSE: naive_rmse &lt;- RMSE(test_set$rating, mu_hat) naive_rmse #&gt; [1] 1.05 Keep in mind that if you plug in any other number, you get a higher RMSE. For example: predictions &lt;- rep(2.5, nrow(test_set)) RMSE(test_set$rating, predictions) #&gt; [1] 1.49 From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better! As we go along, we will be comparing different approaches. Let’s start by creating a results table with this naive approach: rmse_results &lt;- data_frame(method = &quot;Just the average&quot;, RMSE = naive_rmse) 71.3 Modeling movie effects We know from experience that some movies are just generally rated higher than others. So our intuition that different movies are rated differently is confirmed by data. We can augment our previous model by adding the term \\(b_i\\) to represent average ranking for movie \\(i\\): \\[ Y_{u,i} = \\mu + b_i + \\varepsilon_{u,i} \\] In statistics, we usually call the \\(b\\)s as effects. However, in the Netflix challenge papers, they refer to them as “bias”, thus the \\(b\\) notation. We can again use least squared to estimate the \\(b_i\\) in the following way: fit &lt;- lm(rating ~ as.factor(userId), data = movielens) Because there are thousands of \\(b_i\\), each movie gets one, the lm() function will be very slow here. We therefore don’t recommend running the code above. But in this particular situation, we know that the least square estimate \\(\\hat{b}_i\\) is just the average of \\(Y_{u,i} - \\hat{\\mu}\\) for each movie \\(i\\). So we can compute them this way (we will drop the hat notation in the code to represent estimates going forward): mu &lt;- mean(train_set$rating) movie_avgs &lt;- train_set %&gt;% group_by(movieId) %&gt;% summarize(b_i = mean(rating - mu)) We can see that these estimates vary substantially: movie_avgs %&gt;% qplot(b_i, geom =&quot;histogram&quot;, bins = 10, data = ., color = I(&quot;black&quot;)) Remember \\(\\hat{\\mu}=3.5\\) so a \\(b_i = 1.5\\) implies a perfect five star rating. Let’s see how much our prediction improves once we predict using \\(\\hat{y}_{u,i} = \\hat{\\mu} + \\hat{b}_i\\): predicted_ratings &lt;- mu + test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% .$b_i model_1_rmse &lt;- RMSE(predicted_ratings, test_set$rating) rmse_results &lt;- bind_rows(rmse_results, data_frame(method=&quot;Movie Effect Model&quot;, RMSE = model_1_rmse )) rmse_results %&gt;% knitr::kable() method RMSE Just the average 1.048 Movie Effect Model 0.986 We already see an improvement. But can we make it better? 71.3.1 User effects Let’s compute the average rating for user \\(u\\), for those that have rated over 100 movies. train_set %&gt;% group_by(userId) %&gt;% summarize(b_u = mean(rating)) %&gt;% filter(n()&gt;=100) %&gt;% ggplot(aes(b_u)) + geom_histogram(bins = 30, color = &quot;black&quot;) Notice that there is substantial variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be: \\[ Y_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i} \\] where \\(b_u\\) is a user-specific effect. So now if a cranky user (negative \\(b_u\\)) rates a great movie (positive \\(b_i\\)), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5. To fit this model, we could again use lm: lm(rating ~ as.factor(movieId) + as.factor(userId)) but, for the reasons described earlier, we won’t. Instead, we will compute an approximation but computing \\(\\hat{\\mu}\\) and \\(\\hat{b}_i\\) and estimating \\(\\hat{b}_u\\) as the average of \\(y_{u,i} - \\hat{\\mu} - \\hat{b}_i\\) user_avgs &lt;- test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% group_by(userId) %&gt;% summarize(b_u = mean(rating - mu - b_i)) We can now construct predictors and see how much the RMSE improves: predicted_ratings &lt;- test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% left_join(user_avgs, by=&#39;userId&#39;) %&gt;% mutate(pred = mu + b_i + b_u) %&gt;% .$pred model_2_rmse &lt;- RMSE(predicted_ratings, test_set$rating) rmse_results &lt;- bind_rows(rmse_results, data_frame(method=&quot;Movie + User Effects Model&quot;, RMSE = model_2_rmse )) rmse_results %&gt;% knitr::kable() method RMSE Just the average 1.048 Movie Effect Model 0.986 Movie + User Effects Model 0.885 Exercises Load the movielens data. data(&quot;movielens&quot;) Compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts. We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it. Among movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use? A. Fill in the missing values with average rating of all movies. B. Fill in the missing values with 0. C. Fill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set. D. None of the above. The movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: use the as_datetime function in the lubridate package. Compute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by. The plot shows some evidence of a time effect. If we define \\(d_{u,i}\\) as the day for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate: A. \\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\). B. \\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\). C. \\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta_i + \\varepsilon_{u,i}\\). D. \\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\). The movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots. The plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate: A. \\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\). B. \\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\). C. \\(Y_{u,i} = \\mu + b_i + b_u + \\sum{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\). D. \\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\). "],
["regularization.html", "Chapter 72 Regularization 72.1 Motivation 72.2 Penalized Least Squares 72.3 Choosing the penalty terms Exercises", " Chapter 72 Regularization 72.1 Motivation Despite the large movie to movie variation our improvement in RMSE was only about 5%. Let’s explore where we made mistakes in our first model (using only movies). Here are the 10 largest mistakes: test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% mutate(residual = rating - (mu + b_i)) %&gt;% arrange(desc(abs(residual))) %&gt;% select(title, residual) %&gt;% slice(1:10) %&gt;% knitr::kable() title residual Day of the Beast, The (Día de la Bestia, El) 4.50 Horror Express -4.00 No Holds Barred 4.00 Dear Zachary: A Letter to a Son About His Father -4.00 Faust -4.00 Hear My Song -4.00 Confessions of a Shopaholic -4.00 Twilight Saga: Breaking Dawn - Part 1, The -4.00 Taxi Driver -3.81 Taxi Driver -3.81 These all seem like obscure movies. Many of them have large predictions. Let’s look at the top 10 worst and best movies based on \\(\\hat{b}_i\\). First, let’s create a database that connects movieId to movie title: movie_titles &lt;- movielens %&gt;% select(movieId, title) %&gt;% distinct() Here are the 10 best movies according to our estimate: movie_avgs %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(desc(b_i)) %&gt;% select(title, b_i) %&gt;% slice(1:10) %&gt;% knitr::kable() title b_i Lamerica 1.46 Love &amp; Human Remains 1.46 Enfer, L’ 1.46 Picture Bride (Bijo photo) 1.46 Red Firecracker, Green Firecracker (Pao Da Shuang Deng) 1.46 Faces 1.46 Maya Lin: A Strong Clear Vision 1.46 Heavy 1.46 Gate of Heavenly Peace, The 1.46 Death in the Garden (Mort en ce jardin, La) 1.46 And here are the 10 worst: movie_avgs %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(b_i) %&gt;% select(title, b_i) %&gt;% slice(1:10) %&gt;% knitr::kable() title b_i Santa with Muscles -3.04 BAP*S -3.04 3 Ninjas: High Noon On Mega Mountain -3.04 Barney’s Great Adventure -3.04 Merry War, A -3.04 Day of the Beast, The (Día de la Bestia, El) -3.04 Children of the Corn III -3.04 Whiteboyz -3.04 Catfish in Black Bean Sauce -3.04 Watcher, The -3.04 They all seem to be quite obscure. Let’s look at how often they are rated. train_set %&gt;% count(movieId) %&gt;% left_join(movie_avgs) %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(desc(b_i)) %&gt;% select(title, b_i, n) %&gt;% slice(1:10) %&gt;% knitr::kable() #&gt; Joining, by = &quot;movieId&quot; title b_i n Lamerica 1.46 1 Love &amp; Human Remains 1.46 3 Enfer, L’ 1.46 1 Picture Bride (Bijo photo) 1.46 1 Red Firecracker, Green Firecracker (Pao Da Shuang Deng) 1.46 3 Faces 1.46 1 Maya Lin: A Strong Clear Vision 1.46 2 Heavy 1.46 1 Gate of Heavenly Peace, The 1.46 1 Death in the Garden (Mort en ce jardin, La) 1.46 1 train_set %&gt;% count(movieId) %&gt;% left_join(movie_avgs) %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(b_i) %&gt;% select(title, b_i, n) %&gt;% slice(1:10) %&gt;% knitr::kable() #&gt; Joining, by = &quot;movieId&quot; title b_i n Santa with Muscles -3.04 1 BAP*S -3.04 1 3 Ninjas: High Noon On Mega Mountain -3.04 1 Barney’s Great Adventure -3.04 1 Merry War, A -3.04 1 Day of the Beast, The (Día de la Bestia, El) -3.04 1 Children of the Corn III -3.04 1 Whiteboyz -3.04 1 Catfish in Black Bean Sauce -3.04 1 Watcher, The -3.04 1 The supposed “best” and “worst” movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of \\(b_i\\), negative or positive, are more likely. These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure. In previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization. Regularization permits us to penalize large estimates that come from small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions. The general idea is to add a penalty for large values of \\(b_i\\) to the sum of squares equation that we minimize. So having many large \\(b_i\\) makes it harder to minimize. 72.2 Penalized Least Squares One way to think about this is that if we were to fit an effect to every rating, we could, of course, make the sum of squares equation by simply making each \\(b\\) match its respective rating \\(Y\\). This would yield an unstable estimate that changes drastically with new instances of \\(Y\\). Remember that \\(Y\\) is a random variable. By penalizing the equation, we optimize to be bigger when the estimated \\(b\\) are far from 0; we then shrink the estimate towards 0. This is similar to the Bayesian approach we saw earlier. To estimate the \\(b_i\\) we now minimize this equation: \\[\\frac{1}{N} \\sum_{u,i} \\left(y_{u,i} - \\mu - b_i\\right)^2 + \\lambda \\sum_{i} b_i^2\\] The first term is just least squares and the second is a penalty that gets larger when many \\(b_i\\) are large. Using calculus we can actually show that the values of \\(b_i\\) that minimize this equation are: \\[ \\hat{b}_i(\\lambda) = \\frac{1}{\\lambda + n_i} \\sum_{u=1}^{n_i} \\left(Y_{u,i} - \\hat{\\mu}\\right) \\] where \\(n_i\\) is the number of ratings made for movie \\(i\\). This approach will have our desired effect: when \\(n_i\\) is very large, which will give us a stable estimate, then \\(\\lambda\\) is effectively ignored since \\(n_i+\\lambda \\approx n_i\\). However, when \\(n_i\\) is small, then the estimate \\(\\hat{b}_i(\\lambda)\\) is shrunken towards 0. The larger \\(\\lambda\\), the more we shrink. Let’s compute these regularized estimates of \\(b_i\\) using \\(\\lambda=3\\). Later, we will see why we picked 3. lambda &lt;- 3 mu &lt;- mean(train_set$rating) movie_reg_avgs &lt;- train_set %&gt;% group_by(movieId) %&gt;% summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) To see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates. data_frame(original = movie_avgs$b_i, regularlized = movie_reg_avgs$b_i, n = movie_reg_avgs$n_i) %&gt;% ggplot(aes(original, regularlized, size=sqrt(n))) + geom_point(shape=1, alpha=0.5) Let’s look at the top 10 best movies based on \\(\\hat{b}_i(\\lambda)\\): train_set %&gt;% count(movieId) %&gt;% left_join(movie_reg_avgs) %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(desc(b_i)) %&gt;% select(title, b_i, n) %&gt;% slice(1:10) %&gt;% knitr::kable() #&gt; Joining, by = &quot;movieId&quot; title b_i n All About Eve 0.927 26 Shawshank Redemption, The 0.921 240 Godfather, The 0.897 153 Godfather: Part II, The 0.871 100 Maltese Falcon, The 0.860 47 Best Years of Our Lives, The 0.859 11 On the Waterfront 0.847 23 Face in the Crowd, A 0.833 4 African Queen, The 0.832 36 All Quiet on the Western Front 0.824 11 These make much more sense! Here are the top 10 worst movies: train_set %&gt;% count(movieId) %&gt;% left_join(movie_reg_avgs) %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;% arrange(b_i) %&gt;% select(title, b_i, n) %&gt;% slice(1:10) %&gt;% knitr::kable() #&gt; Joining, by = &quot;movieId&quot; title b_i n Battlefield Earth -2.06 14 Joe’s Apartment -1.78 7 Speed 2: Cruise Control -1.69 20 Super Mario Bros. -1.60 13 Police Academy 6: City Under Siege -1.57 10 After Earth -1.52 4 Disaster Movie -1.52 3 Little Nicky -1.51 17 Cats &amp; Dogs -1.47 6 Blade: Trinity -1.46 11 Do we improve our results? predicted_ratings &lt;- test_set %&gt;% left_join(movie_reg_avgs, by=&#39;movieId&#39;) %&gt;% mutate(pred = mu + b_i) %&gt;% .$pred model_3_rmse &lt;- RMSE(predicted_ratings, test_set$rating) rmse_results &lt;- bind_rows(rmse_results, data_frame(method=&quot;Regularized Movie Effect Model&quot;, RMSE = model_2_rmse )) rmse_results %&gt;% knitr::kable() method RMSE Just the average 1.048 Movie Effect Model 0.986 Movie + User Effects Model 0.885 Regularized Movie Effect Model 0.885 This provides a very large improvement. 72.3 Choosing the penalty terms Note that \\(\\lambda\\) is a tuning parameter. We can use cross-validation to choose it. lambdas &lt;- seq(0, 10, 0.25) mu &lt;- mean(train_set$rating) just_the_sum &lt;- train_set %&gt;% group_by(movieId) %&gt;% summarize(s = sum(rating - mu), n_i = n()) rmses &lt;- sapply(lambdas, function(l){ predicted_ratings &lt;- test_set %&gt;% left_join(just_the_sum, by=&#39;movieId&#39;) %&gt;% mutate(b_i = s/(n_i+l)) %&gt;% mutate(pred = mu + b_i) %&gt;% .$pred return(RMSE(predicted_ratings, test_set$rating)) }) qplot(lambdas, rmses) lambdas[which.min(rmses)] #&gt; [1] 3 However, while we show this as an illustration, in practice we should be using full cross validation just on the train set, without using the test set until the final assessment. We can use regularization for the estimate user effects as well. We are minimizing: \\[ \\frac{1}{N} \\sum_{u,i} \\left(y_{i,u} - \\mu - b_i - b_u \\right)^2 + \\lambda \\left(\\sum_{i} b_i^2 + \\sum_{u} b_u^2\\right) \\] The estimates that minimize this can be found similarly to what we did above. Here we use cross-validation to pick a \\(\\lambda\\): lambdas &lt;- seq(0, 10, 0.25) rmses &lt;- sapply(lambdas, function(l){ mu &lt;- mean(train_set$rating) b_i &lt;- train_set %&gt;% group_by(movieId) %&gt;% summarize(b_i = sum(rating - mu)/(n()+l)) b_u &lt;- train_set %&gt;% left_join(b_i, by=&quot;movieId&quot;) %&gt;% group_by(userId) %&gt;% summarize(b_u = sum(rating - b_i - mu)/(n()+l)) predicted_ratings &lt;- test_set %&gt;% left_join(b_i, by = &quot;movieId&quot;) %&gt;% left_join(b_u, by = &quot;userId&quot;) %&gt;% mutate(pred = mu + b_i + b_u) %&gt;% .$pred return(RMSE(predicted_ratings, test_set$rating)) }) qplot(lambdas, rmses) For the full model, the optimal \\(\\lambda\\) is: lambda &lt;- lambdas[which.min(rmses)] lambda #&gt; [1] 3.75 rmse_results &lt;- bind_rows(rmse_results, data_frame(method=&quot;Regularized Movie + User Effect Model&quot;, RMSE = min(rmses))) rmse_results %&gt;% knitr::kable() method RMSE Just the average 1.048 Movie Effect Model 0.986 Movie + User Effects Model 0.885 Regularized Movie Effect Model 0.885 Regularized Movie + User Effect Model 0.881 Exercises An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school. set.seed(1986) n &lt;- round(2^rnorm(1000, 8, 1)) Now let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate. mu &lt;- round(80 + 2*rt(1000, 5)) range(mu) schools &lt;- data.frame(id = paste(&quot;PS&quot;,1:100), size = n, quality = mu, rank = rank(-mu)) We can see that the top 10 schools are: schools %&gt;% top_n(10, quality) %&gt;% arrange(desc(quality)) Now let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points: scores &lt;- sapply(1:nrow(schools), function(i){ scores &lt;- rnorm(schools$size[i], schools$quality[i], 30) scores }) schools &lt;- schools %&gt;% mutate(score = sapply(avg_score, mean)) What are the top schools based on the average score? Show just the ID, size and the average score. Compare the median school size to the median school size of the top 10 schools based on the score. So according to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference chapters! In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score. Let’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools: overall &lt;- mean(sapply(scores, mean)) and then define, for each school, how it deviates from that average. Write code that estimates the score above the average for each school but dividing by \\(n + \\alpha\\) instead of \\(n\\), with \\(n\\) the schools size and \\(\\alpha\\) a regularization parameters. Try \\(\\alpha = 3\\) Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\) Rank the schools based on the average obtained with the best \\(\\alpha\\). Note that no small school is incorrectly included. A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean. "],
["matrix-factorization.html", "Chapter 73 Matrix factorization 73.1 Factors 73.2 Connection to SVD and PCA Exercises", " Chapter 73 Matrix factorization Matrix factorization is a widely used concept in machine learning. It is very much related to factor analysis, singular value decomposition (SVD) and principal component analysis (PCA). Here we describe the concept in the context of movie recommendation systems. We have described how the model: \\[ Y_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i} \\] accounts for movie to movie differences through the \\(b_i\\) and user to user differences through the \\(b_u\\). But this model leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals: \\[ r_{u,i} = y_{u,i} - \\hat{b}_i - \\hat{b}_u \\] To see this, we will convert the data into a matrix so that each user gets a row and each movie gets a column so that \\(y_{u,i}\\) is the entry in row \\(u\\) and column \\(i\\). For illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. We also keep Scent of a Woman (movieId == 3252) becuase we use it for a specific example: train_small &lt;- movielens %&gt;% group_by(movieId) %&gt;% filter(n() &gt;= 50 | movieId == 3252) %&gt;% ungroup() %&gt;% group_by(userId) %&gt;% filter(n() &gt;= 50) %&gt;% ungroup() y &lt;- train_small %&gt;% select(userId, movieId, rating) %&gt;% spread(movieId, rating) %&gt;% as.matrix() We add row names and column names: rownames(y)&lt;- y[,1] y &lt;- y[,-1] movie_titles &lt;- movielens %&gt;% select(movieId, title) %&gt;% distinct() colnames(y) &lt;- with(movie_titles, title[match(colnames(y), movieId)]) and convert them to residuals by removing the column and row effects: y &lt;- sweep(y, 2, colMeans(y, na.rm=TRUE)) y &lt;- sweep(y, 1, rowMeans(y, na.rm=TRUE)) If the model above explains all the signals, and the \\(\\varepsilon\\) are just noise, then the residuals for different movies should be independent from each other. But they are not. Here is an example: m_1 &lt;- &quot;Godfather, The&quot; m_2 &lt;- &quot;Godfather: Part II, The&quot; qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2) This plot says that users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. A similar relationship is seen when comparing The Godfather and Goodfellas: m_1 &lt;- &quot;Godfather, The&quot; m_3 &lt;- &quot;Goodfellas&quot; qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3) Although not as strong, there is still correlation. We see correlations between other movies as well: m_4 &lt;- &quot;You&#39;ve Got Mail&quot; m_5 &lt;- &quot;Sleepless in Seattle&quot; qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5) We can see a pattern: x &lt;- y[, c(m_1, m_2, m_3, m_4, m_5)] colnames(x)[1:2] &lt;- c(&quot;Godfather&quot;, &quot;Godfather 2&quot;) cor(x, use=&quot;pairwise.complete&quot;) %&gt;% knitr::kable() Godfather Godfather 2 Goodfellas You’ve Got Mail Sleepless in Seattle Godfather 1.000 0.829 0.444 -0.440 -0.378 Godfather 2 0.829 1.000 0.521 -0.331 -0.358 Goodfellas 0.444 0.521 1.000 -0.481 -0.402 You’ve Got Mail -0.440 -0.331 -0.481 1.000 0.533 Sleepless in Seattle -0.378 -0.358 -0.402 0.533 1.000 There seems to be people that like romantic comedies more than expected, while others that like gangster movies more than expected. These results tell us that there is structure in the data. But how can we model this? 73.1 Factors Here is an illustration, using a simulation, of how we can use some structure to predict the \\(r_{u,i}\\). Suppose our residuals r look like this: q &lt;- matrix(c(1 , 1, 1, -1, -1), ncol=1) rownames(q) &lt;- c(&quot;Godfather&quot;, &quot;Godfather 2&quot;, m_3, m_4, m_5) p &lt;- matrix(rep(c(2,0,-2), c(3,5,4)), ncol=1) rownames(p) &lt;- 1:nrow(p) set.seed(1) r &lt;- jitter(p %*% t(q)) round(r, 1) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail Sleepless in Seattle #&gt; 1 1.8 2.1 1.8 -1.8 -1.8 #&gt; 2 1.9 1.9 1.9 -2.3 -1.8 #&gt; 3 2.1 2.2 1.6 -1.8 -2.0 #&gt; 4 0.3 0.0 -0.1 -0.1 0.3 #&gt; 5 -0.2 0.2 0.3 0.3 0.0 #&gt; 6 0.3 0.4 -0.1 0.1 -0.2 #&gt; 7 0.4 -0.1 0.0 0.2 -0.3 #&gt; 8 0.1 0.2 0.1 0.0 -0.3 #&gt; 9 -1.9 -1.7 -2.0 2.0 1.9 #&gt; 10 -2.4 -2.2 -2.3 2.2 2.0 #&gt; 11 -2.2 -1.9 -1.7 1.6 2.1 #&gt; 12 -2.3 -2.3 -1.9 2.0 1.9 There seems to be pattern here. In fact, we can see very strong correlation patterns: cor(r) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail #&gt; Godfather 1.000 0.987 0.979 -0.979 #&gt; Godfather 2 0.987 1.000 0.986 -0.981 #&gt; Goodfellas 0.979 0.986 1.000 -0.989 #&gt; You&#39;ve Got Mail -0.979 -0.981 -0.989 1.000 #&gt; Sleepless in Seattle -0.989 -0.988 -0.986 0.976 #&gt; Sleepless in Seattle #&gt; Godfather -0.989 #&gt; Godfather 2 -0.988 #&gt; Goodfellas -0.986 #&gt; You&#39;ve Got Mail 0.976 #&gt; Sleepless in Seattle 1.000 We can create vectors Q and P, that can explain much of the structure we see. The Q would look like this: t(q) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail Sleepless in Seattle #&gt; [1,] 1 1 1 -1 -1 and it narrows down movies to two groups: gangster and romance. We can also reduce the users to three groups: p #&gt; [,1] #&gt; 1 2 #&gt; 2 2 #&gt; 3 2 #&gt; 4 0 #&gt; 5 0 #&gt; 6 0 #&gt; 7 0 #&gt; 8 0 #&gt; 9 -2 #&gt; 10 -2 #&gt; 11 -2 #&gt; 12 -2 those that like gangster movies, but hate romance ones, the reverse, and those that don’t care. The main point here is that we can reconstruct this vector of length 60 with a couple of vectors totaling 17 values. If these are the residuals for users \\(u=1,\\dots,12\\) for movies \\(i=1,\\dots,5\\) we can write the following mathematical formula for our residuals \\(r_{u,i}\\). \\[ r_{u,i} \\approx p_u q_i \\] This implies that we can explain more variability by modifying our previous model for movie recommendations to: \\[ Y_{u,i} = \\mu + b_i + b_u + p_u q_i + \\varepsilon_{i,j} \\] However, what we motivated the need for the \\(p_u q_i\\) term with a simple simulation. The structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have other factors. For example, we may have: set.seed(1) m_6 &lt;- &quot;Scent of a Woman&quot; q &lt;- cbind(c(1 , 1, 1, -1, -1, -1), c(1 , 1, -1, -1, -1, 1)) rownames(q) &lt;- c(&quot;Godfather&quot;, &quot;Godfather 2&quot;, m_3, m_4, m_5, m_6) p &lt;- cbind(rep(c(2,0,-2), c(3,5,4)), c(-1,1,1,0,0,1,1,1,0,-1,-1,-1))/2 rownames(p) &lt;- 1:nrow(p) r &lt;- jitter(p %*% t(q), factor=1) round(r, 1) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail Sleepless in Seattle #&gt; 1 0.5 0.5 1.5 -0.4 -0.5 #&gt; 2 1.5 1.5 0.5 -1.6 -1.5 #&gt; 3 1.5 1.6 0.4 -1.5 -1.5 #&gt; 4 0.1 0.0 0.0 0.0 0.1 #&gt; 5 -0.1 0.0 0.1 0.1 0.0 #&gt; 6 0.6 0.6 -0.5 -0.5 -0.6 #&gt; 7 0.6 0.5 -0.5 -0.4 -0.6 #&gt; 8 0.5 0.6 -0.5 -0.5 -0.6 #&gt; 9 -1.0 -0.9 -1.0 1.0 1.0 #&gt; 10 -1.6 -1.6 -0.6 1.6 1.5 #&gt; 11 -1.6 -1.5 -0.4 1.4 1.5 #&gt; 12 -1.6 -1.6 -0.5 1.5 1.5 #&gt; Scent of a Woman #&gt; 1 -1.4 #&gt; 2 -0.5 #&gt; 3 -0.5 #&gt; 4 0.0 #&gt; 5 0.0 #&gt; 6 0.5 #&gt; 7 0.5 #&gt; 8 0.6 #&gt; 9 0.9 #&gt; 10 0.6 #&gt; 11 0.5 #&gt; 12 0.6 Now we see another factor: love, hates, or doesn’t care about Al Pacino. The correlation is a bit more complicated now cor(r) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail #&gt; Godfather 1.000 0.998 0.527 -0.997 #&gt; Godfather 2 0.998 1.000 0.546 -0.998 #&gt; Goodfellas 0.527 0.546 1.000 -0.553 #&gt; You&#39;ve Got Mail -0.997 -0.998 -0.553 1.000 #&gt; Sleepless in Seattle -0.999 -0.998 -0.528 0.997 #&gt; Scent of a Woman -0.575 -0.593 -0.994 0.599 #&gt; Sleepless in Seattle Scent of a Woman #&gt; Godfather -0.999 -0.575 #&gt; Godfather 2 -0.998 -0.593 #&gt; Goodfellas -0.528 -0.994 #&gt; You&#39;ve Got Mail 0.997 0.599 #&gt; Sleepless in Seattle 1.000 0.572 #&gt; Scent of a Woman 0.572 1.000 Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation: six_movies &lt;- c(m_1, m_2, m_3, m_4, m_5, m_6) x &lt;- y[,six_movies] colnames(x)[1:2] &lt;- c(&quot;Godfather&quot;, &quot;Godfather 2&quot;) cor(x, use=&quot;pairwise.complete&quot;) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail #&gt; Godfather 1.0000 0.829 0.444 -0.440 #&gt; Godfather 2 0.8285 1.000 0.521 -0.331 #&gt; Goodfellas 0.4441 0.521 1.000 -0.481 #&gt; You&#39;ve Got Mail -0.4397 -0.331 -0.481 1.000 #&gt; Sleepless in Seattle -0.3781 -0.358 -0.402 0.533 #&gt; Scent of a Woman 0.0589 0.119 -0.123 -0.170 #&gt; Sleepless in Seattle Scent of a Woman #&gt; Godfather -0.378 0.0589 #&gt; Godfather 2 -0.358 0.1186 #&gt; Goodfellas -0.402 -0.1230 #&gt; You&#39;ve Got Mail 0.533 -0.1699 #&gt; Sleepless in Seattle 1.000 -0.1822 #&gt; Scent of a Woman -0.182 1.0000 To explain this more complicated structure, we need two factors. For example something like this: t(q) #&gt; Godfather Godfather 2 Goodfellas You&#39;ve Got Mail Sleepless in Seattle #&gt; [1,] 1 1 1 -1 -1 #&gt; [2,] 1 1 -1 -1 -1 #&gt; Scent of a Woman #&gt; [1,] -1 #&gt; [2,] 1 one for ganster versus romance and another for Al Pacino versus no Al Pacino. We also need two sets of coefficients: p #&gt; [,1] [,2] #&gt; 1 1 -0.5 #&gt; 2 1 0.5 #&gt; 3 1 0.5 #&gt; 4 0 0.0 #&gt; 5 0 0.0 #&gt; 6 0 0.5 #&gt; 7 0 0.5 #&gt; 8 0 0.5 #&gt; 9 -1 0.0 #&gt; 10 -1 -0.5 #&gt; 11 -1 -0.5 #&gt; 12 -1 -0.5 to explain the \\(3\\times 3\\) types of users. The model with two factors has more parameters, but still less than the original data: \\[ Y_{u,i} = \\mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\varepsilon_{i,j} \\] Note that this is not a linear model. To fit this model we need to use an algorithm other those used by lm to find the parameters that minimize the least squares. Also, the winning algorithms for the Netflix challenge used regularization to penalize for large values of \\(p\\) and \\(q\\), rather than using least squares. Implementing this approach is beyond the scope of this book. 73.2 Connection to SVD and PCA The decomposition: \\[ r_{u,i} \\approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i} \\] is very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors \\(p\\) and \\(q\\) that permit us to rewrite the matrix \\(\\mbox{r}\\) with \\(m\\) rows and \\(n\\) columns as: \\[ r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\dots + p_{u,m} q_{m,i} \\] with the variability of each term decreasing and with the \\(p\\)s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability. Let’s see an example with the movie data. To compute the decomposition, we will make the residuals with NAs equal to 0: y[is.na(y)] &lt;- 0 pca &lt;- prcomp(y) The \\(q\\) vectors are called the principal components and they are stored in this matrix: dim(pca$rotation) #&gt; [1] 454 292 While the \\(p\\), or the user effects, are here: dim(pca$x) #&gt; [1] 292 292 We can see the variability of each of the vectors: plot(pca$sdev) and see that just the first few already explain a large percent: var_explained &lt;- cumsum(pca$sdev^2/sum(pca$sdev^2)) plot(var_explained) We also notice that the first two principal components are related to the structure in opinions about movies: library(ggrepel) pcs &lt;- data.frame(pca$rotation, name = colnames(y)) highlight &lt;- filter(pcs, PC1 &lt; -0.1 | PC1 &gt; 0.1 | PC2 &lt; -0.075 | PC2 &gt; 0.1) pcs %&gt;% ggplot(aes(PC1, PC2)) + geom_point() + geom_text_repel(aes(PC1, PC2, label=name), data = highlight, size = 2) Just by looking at the top 10 in each direction, we see a meaningful pattern. The first PC shows the difference between critically acclaimed movies on one side: pcs %&gt;% select(name, PC1) %&gt;% arrange(PC1) %&gt;% slice(1:10) #&gt; name PC1 #&gt; 1 Pulp Fiction -0.161 #&gt; 2 Seven (a.k.a. Se7en) -0.152 #&gt; 3 Fargo -0.139 #&gt; 4 2001: A Space Odyssey -0.139 #&gt; 5 Silence of the Lambs, The -0.134 #&gt; 6 Clockwork Orange, A -0.128 #&gt; 7 Taxi Driver -0.127 #&gt; 8 Being John Malkovich -0.119 #&gt; 9 Royal Tenenbaums, The -0.106 #&gt; 10 Shining, The -0.106 and Hollywood blockbusters on the other: pcs %&gt;% select(name, PC1) %&gt;% arrange(desc(PC1)) %&gt;% slice(1:10) #&gt; name #&gt; 1 Independence Day (a.k.a. ID4) #&gt; 2 Shrek #&gt; 3 Spider-Man #&gt; 4 Titanic #&gt; 5 Twister #&gt; 6 Armageddon #&gt; 7 Harry Potter and the Sorcerer&#39;s Stone (a.k.a. Harry Potter and the Philosopher&#39;s Stone) #&gt; 8 Forrest Gump #&gt; 9 Lord of the Rings: The Return of the King, The #&gt; 10 Enemy of the State #&gt; PC1 #&gt; 1 0.1523 #&gt; 2 0.1304 #&gt; 3 0.1131 #&gt; 4 0.1118 #&gt; 5 0.1110 #&gt; 6 0.1077 #&gt; 7 0.1026 #&gt; 8 0.0987 #&gt; 9 0.0942 #&gt; 10 0.0932 While the second PC seems to go from artsy, independent films: pcs %&gt;% select(name, PC2) %&gt;% arrange(PC2) %&gt;% slice(1:10) #&gt; name PC2 #&gt; 1 Shawshank Redemption, The -0.0919 #&gt; 2 Truman Show, The -0.0916 #&gt; 3 Little Miss Sunshine -0.0901 #&gt; 4 Slumdog Millionaire -0.0882 #&gt; 5 Amelie (Fabuleux destin d&#39;Amélie Poulain, Le) -0.0848 #&gt; 6 Kill Bill: Vol. 1 -0.0831 #&gt; 7 American Beauty -0.0806 #&gt; 8 City of God (Cidade de Deus) -0.0760 #&gt; 9 Mars Attacks! -0.0747 #&gt; 10 Beautiful Mind, A -0.0743 to nerd favorites: pcs %&gt;% select(name, PC2) %&gt;% arrange(desc(PC2)) %&gt;% slice(1:10) #&gt; name PC2 #&gt; 1 Lord of the Rings: The Two Towers, The 0.3291 #&gt; 2 Lord of the Rings: The Fellowship of the Ring, The 0.3216 #&gt; 3 Lord of the Rings: The Return of the King, The 0.2178 #&gt; 4 Matrix, The 0.2156 #&gt; 5 Star Wars: Episode IV - A New Hope 0.2019 #&gt; 6 Star Wars: Episode VI - Return of the Jedi 0.1804 #&gt; 7 Star Wars: Episode V - The Empire Strikes Back 0.1581 #&gt; 8 Spider-Man 2 0.1086 #&gt; 9 Dark Knight, The 0.1021 #&gt; 10 Speed 0.0983 Fitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the recommenderlab package. The details are beyond the scope of this book. Exercises In this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices. The SVD tells us that we can decompose an \\(N\\times p\\) matrix \\(Y\\) with \\(p &lt; N\\) as \\[ Y = U D V^{\\top} \\] With \\(U\\) and \\(V\\) orthogonal of dimensions \\(N\\times p\\) and \\(p\\times p\\) respectively and \\(D\\) a \\(p \\times p\\) diagonal matrix with the values of the diagonal decreasing: \\[d_{1,1} \\geq d_{2,2} \\geq \\dots d_{p,p}\\]. In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this: set.seed(1987) n &lt;- 100 k &lt;- 8 Sigma &lt;- 64 * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) m &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma) m &lt;- m[order(rowMeans(m), decreasing = TRUE),] y &lt;- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3) colnames(y) &lt;- c(paste(rep(&quot;Math&quot;,k), 1:k, sep=&quot;_&quot;), paste(rep(&quot;Science&quot;,k), 1:k, sep=&quot;_&quot;), paste(rep(&quot;Arts&quot;,k), 1:k, sep=&quot;_&quot;)) Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just a random independent numbers. Are all students just about as good? Does being good in one subject imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this \\(100 \\times 24\\) dataset. You can visualize the 24 test scores for the 100 students by plotting an image: my_image &lt;- function(x, zlim = range(x), ...){ colors = rev(RColorBrewer::brewer.pal(9, &quot;RdBu&quot;)) cols &lt;- 1:ncol(x) rows &lt;- 1:nrow(x) image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, col = colors, zlim = zlim, ...) abline(h=rows + 0.5, v = cols + 0.5) axis(side = 1, cols, colnames(x), las = 2) } my_image(y) How would you describe the data based on this figure? A. The test scores are all independent of each other. B. The students that test well are at the top of the image and there seems to be three groupings by subject. C. The students that are good at math are not good at science. D. The students that are good at math are not good at humanities. You can examine the correlation between the test scores directly like this: my_image(cor(y), zlim = c(-1,1)) range(cor(y)) axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2) Which of the following best describes what you see? A. The test scores are independent. B. Math and Science are highly correlated but the humanities are not. C. There is high correlation between tests in the same subject but no correlation across subjects. D. There is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject. Remember that orthogonality means that \\(U^{\\top}U\\) and \\(V^{\\top}V\\) are equal to the identity matrix. This implies that we can also rewrite the decomposition as \\[ Y V = U D \\mbox{ or } U^{\\top}Y = D V^{\\top}\\] We can think of \\(YV\\) and \\(U^{\\top}V\\) as two transformation of Y that preserve the total variability of \\(Y\\) since \\(U\\) and \\(V\\) are orthogonal. Use the function svd to compute the SVD of y. This function will return \\(U\\), \\(V\\) and the diagonal entries of \\(D\\). s &lt;- svd(y) names(s) You can check that the SVD works by typing: y_svd &lt;- s$u %*% diag(s$d) %*% t(s$v) max(abs(y - y_svd)) Compute the sum of squares of the columns of \\(Y\\) and store them in ss_y. Then the sum of squares of columns of the transformed \\(YV\\) and store them in \\(ss_yv\\). Confirm that sum(ss_y) is equal to sum(ss_yv) We see that the total sum of squares is preserved. This is because \\(V\\) is orthogonal. Now to start understanding how \\(YV\\) is useful plot ss_y against the column number and then do the same for \\(ss_yv\\). What do you observe? We see that the variability of the columns of \\(YV\\) is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute ss_yv because we already have the answer. How? Remember that \\(YV = UD\\) and because \\(U\\) is orthogonal, we know that the sum of squares of the columns of \\(UD\\) are the diagonal entries of \\(D\\) squared. Confirm this by plotting the square root of \\(ss_yv\\) versus the diagonal entries of \\(D\\) So from the above we know that the sum of squares of the columns of \\(Y\\) (the total sum of squares) add up to the sum of s$d^2 and that the transformation \\(YV\\) gives us columns with sums of squares equal to s$d^2. Now compute what percent of the total variability is explained by just the first three columns of \\(YV\\). We see that almost 99% of the variability is explained by the first three columns of \\(YV = UD\\). So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write \\(U\\) out in its columns \\([U_1, U_2, \\dots, U_p]\\) then \\(UD\\) is equal to \\[UD = [U_1 d_{1,1}, U_2 d_{2,2}, \\dots, U_p d_{p,p}]\\] Use the sweep function to compute \\(UD\\) without constructing diag(s$d) nor matrix multiplication. We know that \\(U_1 d_{1,1}\\), the first column of \\(UD\\), has the most variability of all the columns of \\(UD\\). Earlier we saw an image of \\(Y\\) my_image(y) in which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against \\(U_1 d_{1,1}\\) and describe what you find. We note that the signs in SVD are arbitrary because: \\[ U D V^{\\top} = (-U) D (-V)^{\\top} \\] With this in mind we see that the first column of \\(UD\\) is almost identical to the average score for each student except for the sign. This implies that multiplying \\(Y\\) by the first column of \\(V\\) must be performing a similar operation to taking the average. Make an image plot of \\(V\\) and describe the first column relative to others and how this relates to taking an average. We already saw that we can rewrite \\(UD\\) as \\[U_1 d_{1,1} + U_2 d_{2,2} + \\dots + U_p d_{p,p}\\] with \\(U_j\\) the j-th column of \\(U\\). This implies that we can rewrite the entire SVD as: \\[Y = U_1 d_{1,1} V_1 ^{\\top} + U_2 d_{2,2} V_2 ^{\\top} + \\dots + U_p d_{p,p} V_p ^{\\top}\\] with \\(V_j\\) the jth column of \\(V\\). Plot \\(U_1\\), then plot \\(V_1^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_1 d_{1,1} V_1 ^{\\top}\\) and compare it to the image of \\(Y\\). Hint: use the my_image function defined above. Use the drop=FALSE argument to assure the subsets of matrices are matrices. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the a \\(100 \\times 24\\) matrix. This is our first matrix factorization: \\[ Y \\approx d_{1,1} U_1 V_1^{\\top}\\] We know it explains s$d[1]^2/sum(s$d^2) * 100 percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code: resid &lt;- y - with(s,(u[, 1, drop=FALSE]*d[1]) %*% t(v[, 1, drop=FALSE])) my_image(cor(resid), zlim = c(-1,1)) axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2) Now that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot \\(U_2\\), then plot \\(V_2^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_2 d_{2,2} V_2 ^{\\top}\\) and compare it to the image of resid. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of s$v[,2]. Adding the matrix we obtain with these two columns will help with our approximation: \\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} \\] We know it will explain ```r sum(s$d[1:2]^2)/sum(s$d^2) * 100 ``` percent of the total variability. We can compute new residuals like this: resid &lt;- y - with(s,sweep(u[, 1:2], 2, d[1:2], FUN=&quot;*&quot;) %*% t(v[, 1:2])) my_image(cor(resid), zlim = c(-1,1)) axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2) and see that the structure that is left is driven by the differences between math and science. Confirm this by plotting \\(U_3\\), then plot \\(V_3^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_3 d_{3,3} V_3 ^{\\top}\\) and compare it to the image of resid. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of s$v[,3]. Adding the matrix we obtain with these two columns will help with our approximation: \\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\] We know it will explain: sum(s$d[1:3]^2)/sum(s$d^2) * 100 percent of the total variability. We can compute new residuals like this: resid &lt;- y - with(s,sweep(u[, 1:3], 2, d[1:3], FUN=&quot;*&quot;) %*% t(v[, 1:3])) my_image(cor(resid), zlim = c(-1,1)) axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2) We no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model: \\[ Y = d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top} + \\varepsilon\\] with \\(\\varepsilon\\) a matrix of independent identically distributed errors. This model is useful because we summarize of \\(100 \\times 24\\) observations with \\(3 \\times (100+24+1) = 375\\) numbers. Furthermore, the three components of the model have useful interpretations: 1 - the overall ability of a student, 2 - the difference in ability between the math/sciences and arts and 3 - the remaining differences between the three subjects. The sizes \\(d_{1,1}, d_{2,2}\\) and \\(d_{3,3}\\) tells us the variability explained by each component. Finally, note that the components \\(d_{j,j} U_j V_j^{\\top}\\) are equivalent to the jth principal component. Finish the exercise by plotting an image of \\(Y\\), an image of \\(d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\) and an image of the residuals, all with the same zlim. Advanced. The movielens dataset included` in the dslabs package is a small subset of a larger dataset with millions of ratings. You can find the entire latest dataset here https://grouplens.org/datasets/movielens/20m/. Create your own recommendation system using all the tools we have shown you. "],
["clustering.html", "Chapter 74 Clustering 74.1 Hierarchical clustering 74.2 k-means 74.3 Heatmaps 74.4 Filtering features Exercises", " Chapter 74 Clustering In this chapter we focus on a type of machine learning refereed to as supervised. The name comes from the fact that we use the outcome to supervise the creation of our prediction algorithm. There is another subset of machine learning referred to as unsupervised. In this subset we do not necessarily know the outcomes and instead are interested in discovering groups. These algorithms are also referred to as clustering algorithm since predictors are used to define clusters. In the two examples we have shown here clustering would not be very useful. In the first example, if we are simply given the heights we will not be able to discover two groups, males and females, because the intersection is large. In the second example, we can see from plotting the predictors that discovering the two digits, two and seven, will be challenging: data(&quot;mnist_27&quot;) mnist_27$train %&gt;% qplot(x_1, x_2, data = .) However, there are applications in which unsupervised learning can be a powerful technique, in particular as an exploratory tool. A first step in any clustering algorithm is defining a distance between observations or groups of observations. Then we need to decide how to join observations into clusters. There are many algorithms for doing this. Here we introduce two as examples: hierarchical and k-means. We will construct a simple example based on movie ratings. Here we quickly construct a matrix x that has ratings for the 50 movies with the most ratings. data(&quot;movielens&quot;) top &lt;- movielens %&gt;% group_by(movieId) %&gt;% summarize(n=n(), title = first(title)) %&gt;% top_n(50, n) %&gt;% .$movieId x &lt;- movielens %&gt;% filter(movieId %in% top) %&gt;% group_by(userId) %&gt;% filter(n() &gt;= 25) %&gt;% ungroup() %&gt;% select(title, userId, rating) %&gt;% spread(userId, rating) row_names &lt;- str_remove(x$title, &quot;: Episode&quot;) %&gt;% str_trunc(20) x &lt;- x[,-1] %&gt;% as.matrix() x &lt;- sweep(x, 2, colMeans(x, na.rm = TRUE)) x &lt;- sweep(x, 1, rowMeans(x, na.rm = TRUE)) rownames(x) &lt;- row_names We want to use these data to find out if there are clusters of movies based on the ratings from 139 movie raters. A first step is to find the distance between each pair of movies using the dist function: d &lt;- dist(x) 74.1 Hierarchical clustering With the distance between each pair of movies computed, we need an algorithms to define groups from these. Hierarchical clustering starts by defining each observation as a group, then the two closest groups are joined into a group iteratively until there is just one group including all the observations. The hclust function that implements this algorithm and it takes a distance as input. h &lt;- hclust(d) We can see the resulting groups using a dendrogram. plot(h, cex = 0.75) To interpret this graph we do the following. To find the distance between any two movies find the location they split. This location is the distance between the groups. So the distance between the Star Wars movies is 8 or less, while the distance between Raiders of the Lost of Arc and Silence of the Lambs is about 17. To generate actual groups we can do one of two things: 1) decide on a minimum distance needed for observations to be in the same group or 2) decide on the number of groups you want and then find the minimum distance that achieves this. The function cutree: groups &lt;- cutree(h, k = 10) split(names(groups), groups) #&gt; $`1` #&gt; [1] &quot;Ace Ventura: Pet ...&quot; &quot;Dumb &amp; Dumber (Du...&quot; &quot;Mask, The&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;Aladdin&quot; &quot;Beauty and the Beast&quot; &quot;Sixth Sense, The&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;American Beauty&quot; &quot;E.T. the Extra-Te...&quot; &quot;Fargo&quot; #&gt; [4] &quot;Fight Club&quot; &quot;Godfather, The&quot; &quot;Pulp Fiction&quot; #&gt; [7] &quot;Seven (a.k.a. Se7en)&quot; &quot;Silence of the La...&quot; &quot;Twelve Monkeys (a...&quot; #&gt; [10] &quot;Usual Suspects, The&quot; #&gt; #&gt; $`4` #&gt; [1] &quot;Apollo 13&quot; &quot;Braveheart&quot; &quot;Dances with Wolves&quot; #&gt; [4] &quot;Forrest Gump&quot; &quot;Good Will Hunting&quot; &quot;Saving Private Ryan&quot; #&gt; [7] &quot;Schindler&#39;s List&quot; &quot;Shawshank Redempt...&quot; #&gt; #&gt; $`5` #&gt; [1] &quot;Back to the Future&quot; &quot;Groundhog Day&quot; &quot;Princess Bride, The&quot; #&gt; [4] &quot;Raiders of the Lo...&quot; #&gt; #&gt; $`6` #&gt; [1] &quot;Batman&quot; &quot;Jurassic Park&quot; &quot;Lion King, The&quot; #&gt; [4] &quot;Men in Black (a.k...&quot; &quot;Shrek&quot; &quot;Toy Story&quot; #&gt; #&gt; $`7` #&gt; [1] &quot;Fugitive, The&quot; &quot;Independence Day ...&quot; &quot;Mission: Impossible&quot; #&gt; [4] &quot;Speed&quot; #&gt; #&gt; $`8` #&gt; [1] &quot;Gladiator&quot; &quot;Matrix, The&quot; &quot;Terminator 2: Jud...&quot; #&gt; [4] &quot;Terminator, The&quot; &quot;True Lies&quot; #&gt; #&gt; $`9` #&gt; [1] &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; #&gt; [4] &quot;Star Wars IV - A ...&quot; &quot;Star Wars V - The...&quot; &quot;Star Wars VI - Re...&quot; #&gt; #&gt; $`10` #&gt; [1] &quot;Titanic&quot; Note that the clustering provides some insights into types of movies. We can change the size of the group by either making k larger or h smaller. Note that we can also explore the data to see if there are clusters of movie raters. h_2 &lt;- dist(t(x)) %&gt;% hclust() plot(h_2, cex = 0.25) 74.2 k-means To use the k-means clustering algorithm we have to pre-define \\(k\\), the number of clusters we want to define. The k-means algorithm is iterative. The first step is to define \\(k\\) centers. Then each observation is assigned to the cluster with the closest center to that observation. In a second step the centers are redefined using the observation in each cluster: the column means are used to define a centroid. We repeat these two steps until the centers converge. The kemans function included in R-base does not handle NAs. For illustrative purposes we will fill out the NAs with 0s. In general, the choice of how to fill in missing data, or if one should do it at all, should be made with care. x_0 &lt;- x x_0[is.na(x_0)] &lt;- 0 k &lt;- kmeans(x_0, centers = 10) The cluster assignments are in the cluster component: groups &lt;- k$cluster split(names(groups), groups) #&gt; $`1` #&gt; [1] &quot;Apollo 13&quot; &quot;Dances with Wolves&quot; &quot;E.T. the Extra-Te...&quot; #&gt; [4] &quot;Godfather, The&quot; &quot;Good Will Hunting&quot; &quot;Saving Private Ryan&quot; #&gt; [7] &quot;Schindler&#39;s List&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;Aladdin&quot; &quot;Beauty and the Beast&quot; &quot;Gladiator&quot; #&gt; [4] &quot;Lion King, The&quot; &quot;Princess Bride, The&quot; &quot;Shrek&quot; #&gt; [7] &quot;Toy Story&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;Titanic&quot; #&gt; #&gt; $`4` #&gt; [1] &quot;Ace Ventura: Pet ...&quot; &quot;Dumb &amp; Dumber (Du...&quot; &quot;Groundhog Day&quot; #&gt; [4] &quot;Mask, The&quot; #&gt; #&gt; $`5` #&gt; [1] &quot;American Beauty&quot; &quot;Fargo&quot; &quot;Fight Club&quot; #&gt; [4] &quot;Pulp Fiction&quot; &quot;Seven (a.k.a. Se7en)&quot; &quot;Silence of the La...&quot; #&gt; [7] &quot;Twelve Monkeys (a...&quot; &quot;Usual Suspects, The&quot; #&gt; #&gt; $`6` #&gt; [1] &quot;Braveheart&quot; #&gt; #&gt; $`7` #&gt; [1] &quot;Back to the Future&quot; &quot;Batman&quot; &quot;Fugitive, The&quot; #&gt; [4] &quot;Matrix, The&quot; &quot;Mission: Impossible&quot; &quot;Raiders of the Lo...&quot; #&gt; [7] &quot;Speed&quot; &quot;Star Wars IV - A ...&quot; &quot;Star Wars V - The...&quot; #&gt; [10] &quot;Star Wars VI - Re...&quot; &quot;Terminator 2: Jud...&quot; &quot;Terminator, The&quot; #&gt; [13] &quot;True Lies&quot; #&gt; #&gt; $`8` #&gt; [1] &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; #&gt; #&gt; $`9` #&gt; [1] &quot;Forrest Gump&quot; &quot;Shawshank Redempt...&quot; &quot;Sixth Sense, The&quot; #&gt; #&gt; $`10` #&gt; [1] &quot;Independence Day ...&quot; &quot;Jurassic Park&quot; &quot;Men in Black (a.k...&quot; Note that because the first center is chosen at random, the final clusters are random. We impose some stability by repeating the entire several times and averaging the results. k &lt;- kmeans(x_0, centers = 10, nstart = 25) groups &lt;- k$cluster split(names(groups), groups) #&gt; $`1` #&gt; [1] &quot;American Beauty&quot; &quot;Fargo&quot; &quot;Fight Club&quot; #&gt; [4] &quot;Pulp Fiction&quot; &quot;Seven (a.k.a. Se7en)&quot; &quot;Silence of the La...&quot; #&gt; [7] &quot;Twelve Monkeys (a...&quot; &quot;Usual Suspects, The&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;Titanic&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;Forrest Gump&quot; &quot;Shawshank Redempt...&quot; &quot;Sixth Sense, The&quot; #&gt; #&gt; $`4` #&gt; [1] &quot;Apollo 13&quot; &quot;Braveheart&quot; &quot;Dances with Wolves&quot; #&gt; [4] &quot;E.T. the Extra-Te...&quot; &quot;Gladiator&quot; &quot;Godfather, The&quot; #&gt; [7] &quot;Good Will Hunting&quot; &quot;Saving Private Ryan&quot; &quot;Schindler&#39;s List&quot; #&gt; #&gt; $`5` #&gt; [1] &quot;Aladdin&quot; &quot;Beauty and the Beast&quot; &quot;Lion King, The&quot; #&gt; [4] &quot;Princess Bride, The&quot; &quot;Raiders of the Lo...&quot; &quot;Shrek&quot; #&gt; [7] &quot;Toy Story&quot; #&gt; #&gt; $`6` #&gt; [1] &quot;Star Wars IV - A ...&quot; &quot;Star Wars V - The...&quot; &quot;Star Wars VI - Re...&quot; #&gt; #&gt; $`7` #&gt; [1] &quot;Back to the Future&quot; &quot;Batman&quot; &quot;Fugitive, The&quot; #&gt; [4] &quot;Groundhog Day&quot; &quot;Matrix, The&quot; &quot;Mission: Impossible&quot; #&gt; [7] &quot;Speed&quot; &quot;Terminator 2: Jud...&quot; &quot;Terminator, The&quot; #&gt; [10] &quot;True Lies&quot; #&gt; #&gt; $`8` #&gt; [1] &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; &quot;Lord of the Rings...&quot; #&gt; #&gt; $`9` #&gt; [1] &quot;Ace Ventura: Pet ...&quot; &quot;Dumb &amp; Dumber (Du...&quot; &quot;Mask, The&quot; #&gt; #&gt; $`10` #&gt; [1] &quot;Independence Day ...&quot; &quot;Jurassic Park&quot; &quot;Men in Black (a.k...&quot; 74.3 Heatmaps A powerful visualization tool for discovering clusters or patterns in your data is the heatmap. The idea is simple: plot an image of your data matrix with both the columns and rows ordered according to the results of clustering algorithm. Here is an example using the two hierarchical clustering results we have computed: image(x[h$order, h_2$order], col = RColorBrewer::brewer.pal(11, &quot;Spectral&quot;)) There is also heatmap function that you can use on the original matrix heatmap(x, col = RColorBrewer::brewer.pal(11, &quot;Spectral&quot;)) 74.4 Filtering features If the information about clusters in included in just a few features, including all the features can add enough noise that detecting clusters becomes challenging. One simple approach to try to remove feature with no information is to only include those with high variance. In the movie example, a user with low variance in their ratings is not really informative: all the movies seem about the same to them. Here is an example of how we can include only the features with high variance. library(matrixStats) sds &lt;- rowSds(x, na.rm = TRUE) o &lt;- order(sds, decreasing = TRUE)[1:50] heatmap(x[,o], col = RColorBrewer::brewer.pal(11, &quot;Spectral&quot;)) Exercises Load the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d. Make a hierarchical clustering plot and add the tissue types as labels. Run a k-means clustering on the data with \\(K=7\\). Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes. Select the 50 most variable genes. Make sure the observations show up in the columns, that the predictor are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, &quot;RdBu&quot;) for a better use of colors. Three general guiding principles that motivate what we learn here are 1) be systematic when organizing your filesystem, 2) automatize when possible, and 3) minimize the use of the mouse. As you become more proficient at coding, you will find that 1) you want to minimize the time you spend remembering what you called a file or where you put it, 2) if you find yourself repeating the same task over and over, there is probably a way to automatize, and 3) anytime your fingers leave the keyboard, it results in loss of productivity. A typical data analysis challenge may involve several parts, each involving several data files, including files containing the scripts we use to analyze data. Keeping all this organized can be challenging. We will learn to use the Unix shell as a tool for managing files and directories on your computer system. Using Unix will permit you to use the keyboard, rather than the mouse, when creating folders, moving from directory to directory, and renaming, deleting or moving files. We also provide specific suggestions on how to keep the file system organized. The data analysis process is iterative and adaptive. As a result, we are constantly editing our scripts and reports. In this chapter, we introduce you to the version control system Git, which is a powerful tool for keeping track of these changes. We also introduce you to GitHub, a service that permits you to host and share your code. We will demonstrate how you can use this service to facilitate collaborations. Keep in mind that another positive benefit of using GitHub is that you can easily showcase your work to potential employers. Finally, we learn to write reports in R markdown, which permits you to incorporate text and code into a single document. We will demonstrate how, using the knitr package, we can write reproducible and aesthetically pleasing reports by running the analysis and generating the report simultaneously. We will put all this together using the powerful integrated desktop environment RStudio. A first step is to install all the tools we will need to do this. These tools are all free. "],
["installing-r-rstudio-and-git.html", "Chapter 75 Installing R, RStudio and Git 75.1 Installing R 75.2 Installing RStudio 75.3 Installing Git", " Chapter 75 Installing R, RStudio and Git First, although not necessary, we recommend you use Chrome as your browser. You can freely download and install Chrome here. In this section, we will describe how to install the software tools that we recommend as the main productivity tools for data science. To motivate the use of these tools, we will also provide brief illustrations of how we use them. We will install the following three software tools: R - The programming language we use to analyze data. RStudio - The integrated desktop environment we use to edit, organize and test our R scripts. Git - The version control system we use to keep track of changes made to our code and to sync local copies of our code with copies hosted on GitHub. Git Bash (for Windows users only) - A piece of software that emulates Unix on Windows machines. We will also show how to open a GitHub account and sync it with RStudio. 75.1 Installing R RStudio is an interactive desktop environment, but it is not R, nor does it include R when you download and install it. Therefore, to use RStudio, we first need to install R. You can download R from the Comprehensive R Archive Network (CRAN). Search for CRAN on your browser: Once on the CRAN page, select the version for your operating system: Linux, Mac OS X or Windows. Here we show screenshots for Windows, but the process is similar for the other platforms. When they differ, we will also show screenshots for Mac OS X. Once at the CRAN download page, you will have several choices. You want to install the base subdirectory. This installs the basic packages you need to get started. We will later learn how to install other needed packages from within R, rather than from this webpage. Click on the link for the latest version to start the download. If you are using Chrome, at the bottom of your browser you should see a tab that shows you the progress of the download. Once the installer file downloads, you can go ahead and click on that tab to start the installation process. Other browsers may be different so you will have to find where they store downloaded files and click on them to get the process started. If using Safari on a Mac, you can access the download here: Now you can now click through different choices to finish the installation. We recommend you select all the default choices. Even when you get an ominous warning. When selecting the language, consider that it will be easier to follow this book if you select English. Continue to select all the defaults: On the Mac it looks different, but you are also accepting the defaults: Congratulations! You have installed R. 75.2 Installing RStudio You can start by searching for RStudio on your browser: You should find the RStudio website as shown above. Once there, click on Download RStudio. This will give you several options. For what we do in this book, it is more than enough to use the free Desktop version: Once you select this option, it will take you to a page in which the operating system options are provided. Click the link showing your operating system. Once the installation file is downloaded, click on the downloaded file to start the installation process: We recommend clicking yes on all the defaults. On the Mac, there are fewer clicks. You basically drag and drop the RStudio Icon into the Applications folder icon here: Congratulations! You have installed RStudio. You can now get started as you do on any other program in your computer. On Windows, you can open RStudio from the Start menu. If RStudio does not appear, you can search for it: On the Mac, it will be in the Applications folder: Pro tip for the Mac: To avoid using the mouse to open RStudio, hit command+spacebar to open Spotlight Search and type RStudio into that search bar, then hit enter. 75.3 Installing Git Another great advantage of RStudio projects is that one can share them with collaborators or the public through GitHub. To do this, we will need a piece of software named Git as well as access to a Unix terminal. The installation process for Git is quite different for Mac and Windows. We include both below. Git is what we refer to as a version control system. These are useful for tracking changes to files as well as coordinating the editing of code by multiple collaborators. We will later learn how to use GitHub, which is a hosting system for code. You need Git to interact with GitHub. Having your code and, more generally, data science projects on GitHub is, among other things, a good way to showcase your work to current and potential employers. Git is most effectively used with Unix, although one can also use it through RStudio. In the next section, we describe Unix in more detail. Here we show you how to install software that permits you to use Git and Unix. The installation process is quite different for Windows and Mac, so we include two different sections. 75.3.1 Installing Git and Git Bash on Windows Warning: These instructions are not for Mac users. There are several pieces of software that will permit you to perform Unix commands on Windows. We will be using Git Bash as it interfaces with RStudio and it is automatically installed when we install Git for Windows. Start by searching for Git for windows on your browser and clicking on the link from git-scm.com. This will take you to the Download Git page from which you can download the more recent maintained build: You can then accept to run the installer and agree to the license: In one of the installation steps, you will be asked to pick the default editor for Git. Unless you are already a vi or vim user, we recommend against selecting vim which might be the default. If you do not recognize an editor you are familiar with among the options given, we recommend that you select nano as your default editor for Git since it is the easiest to learn: The next installation decision is actually an important one. This installation process installs Git Bash. We recommend that you select Git and optional Unix tools from the Windows Command prompt as this will permit you to learn Unix from within RStudio. However, if you do this, some commands that run on your Windows command line will stop working. If you do not use your windows command line, then this should not be a problem. Also, most, if not all, of these Windows command lines have a Unix equivalent that you will be able to use now. You can now continue selecting the default options. You have now installed Git on Windows. A final and important step is to change a preference in RStudio so that Git Batch becomes the default Unix shell in RStudio. In RStudio, go to preferences (under the File pull down menu), then select Terminal, then select Git Bash: To check that you in fact are using Git Bash in RStudio, you can open a New Terminal in RStudio: It should look something like this: 75.3.2 Installing Git on the Mac Before we show you the installation process, we introduce you to the Terminal. Macs already come with this terminal and it can be used to learn Unix. We can also use it to check if Git is already installed and, if not, start the installation process. To open a terminal, you can use command+spacebar and type terminal or you can find it in Utilities: You can also open the terminal by hitting command+spacebar to open Spotlight Search and type Terminal into that search bar, then hit enter. Once you start the terminal, you will see a console like this: You might have Git installed already. One way to check is by asking for the version by typing: git --version If you get a version number back, it is already installed. If not, you will get the following message: and you will be asked if you want to install it. You should click Install: This will take you through the installation process: Once installed, you can check for the version again and it should show you something like this: Congratulations. You have installed Git on your Mac. Reminder: On Windows, we install Git Bash. We do not need to do this on the Mac since they come with the terminal pre-installed and we can use this to run Unix commands. "],
["github.html", "Chapter 76 GitHub 76.1 GitHub Accounts 76.2 GitHub repositories", " Chapter 76 GitHub GitHub is an online service that permits you to organize and share your code in what are called repositories. In a later section, we will show you how to use Git and GitHub to organize your data science projects through RStudio. Before we do that, we create an account and a repository. 76.1 GitHub Accounts The next step is to get a GitHub account. Basic GitHub accounts are free. The first step is to go to GitHub where you will see a box in which you can sign up. You want to pick a name carefully. It should be short, easy to remember and to spell, somehow related to your name, and professional. This last one is important since you might be sending potential employers a link to your GitHub account. In the example below, I am sacrificing on the ease of spelling to incorporate my name. Your initials and last name are usually a good choice. If you have a very common name, then this may have to be taken into account. A simple solution would be to add numbers or spell out part of your name. The account I use for my research, rafalab, is the same one I use for my webpage and Twitter, which makes it easy to remember for those that follow my work. Once you have a GitHub account, you are ready to connect RStudio to this account. You start by going to the Global Options and selecting Git/SVN: You then need to enter a path for the Git executable we just installed. On the Windows default installation, this will be C:/Program File/Git/bin/git.exe, but you should find it by browsing your system as this can change from system to system. Now to avoid entering our GitHub password every time we try to access our repository, we will create what is called SSH RSA Key. RStudio can do this for us automatically if we click on the Create RSA Key button: You can follow the default instructions as shown below: RStudio and GitHub should now be able to connect and we are ready to create a first GitHub code repository. 76.2 GitHub repositories You are now ready to create a GitHub repository (repo). The general idea is that you will have at least two copies of your code: one on your computer and one on GitHub. If you add collaborators to this project, then each will have a copy on their computer. The GitHub copy is usually considered the master copy that each collaborator syncs to. Git will help you keep all the different copies synced. As mentioned, one of the advantages of keeping code on a GitHub repository is that you can easily share it with potential employers interested in seeing examples of your work. Because many data science companies use version control systems, like Git, to collaborate on projects, they might also be impressed that you already know at least the basics. The first step in creating a repo for your code is to initialize on GitHub. Because you already created an account, you will have a repo on GitHub with the URL _http:\\\\github.com/username_. To create a repo, first login to your account by clicking the Sign In button on http://github.com. You might already be signed in, in which case the Sign In button will not show up: If signing in, you will have to enter your username and password. We recommend you set up your browser to remember this to avoid typing it in each time. Once on your account, you can click on Repositories: Then you click on New to create a new repo: You will then want to choose a good descriptive name for the project. In the future, you might have dozens of repos so keep that in mind when choosing a name. Here we will use homerwork-0. We recommend you make the repo public. If you want to keep it private, you will have to pay a monthly charge. You now have your first repo on GitHub. The next step will be to clone it on your computer and start editing and syncing using Git. To do this, it is convenient to copy the link provided by GitHub specifically to connect to this repo, using Git as shown below. We will later need to copy and paste this so make sure to remember this step. "],
["rstudio.html", "Chapter 77 RStudio 77.1 The panes 77.2 Key bindings 77.3 Installing R packages 77.4 Running commands while editing scripts 77.5 Global options 77.6 Keeping organized with RStudio projects 77.7 Using Git and GitHub in RStudio 77.8 Running R without RStudio", " Chapter 77 RStudio RStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but many other useful tools. In this section, we go over some of the basics. 77.1 The panes When you start RStudio for the first time, you will see three panes. The left pane shows the R console. On the right, the top pane includes three tabs: Extensions, History and Connections, while the bottom pane shows five tabs: File, Plots, Packages, Help and Viewer. You can click on each tab to move across the different features. To start a new script, you can click on File, the New File, then R Script. This starts a new pane on the left and it is here where you can start writing your script. 77.2 Key bindings Many tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as key bindings. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac. Although in this tutorial we often show how to use the mouse, we highly recommend that you memorize key bindings for the operations you use most. RStudio provides a useful cheat sheet with the most widely used commands. You can get it from RStudio directly: and it looks like this: You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking. 77.3 Installing R packages Most of what we have learned in this book depends on the tidyverse. The data we have been working on depend on the dslabs package. These packages do not come pre-installed in R. In fact, the default installation of R is quite minimal and, for many of your projects, you will need to download and install one or more packages. You can install packages directly from R with the command install.packages. To install the tidyverse package, we would type, in the R console: install.packages(&quot;tidyverse&quot;) We can install more than one package at once by feeding a character vector to this function: install.packages(c(&quot;tidyverse&quot;, &quot;dslabs&quot;)) You can also install packages using RStudio in the following way: One advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package: Once you select your package, we recommend selecting all the defaults: Remember that installing tidyverse actually installs several packages. Once packages are installed, you can load them into R and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in R not RStudio. It is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script. You can see all the packages you have installed using the following function: installed.packages() 77.4 Running commands while editing scripts There are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example. Let’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or using the key binding Ctrl-S on Windows and command-S on the Mac. When you ask for the document to be saved for the first time, RStudio will prompt you for a name. You want to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix .R. We will call this script my-first-script.R. Now we are ready to start editing our first script. The first lines of code in an R script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type library() it starts auto-completing with libraries that we have installed. Note what happens when we type library(ti): Another feature you may have noticed is that when you type library( the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis. Now we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by sourcing in the code. To do this, click on the Run button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac. Once you run the code, you will see it appear in the R console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files. To run one line at a time instead of the entire script, you can use Control-Enter or Windows and Command-Return on the Mac. 77.5 Global options You can change the look and functionality of RStudio quite a bit. To change the global options you click on Tools then Global Options…. As an example, we show how to change the appearance of the editor. To do this click on Appearance and then notice the Editor theme options. You can click on these and see examples of how your editor will look. I personally like the Cobalt option. This makes your editor look like this: As a second example, we show how to make a change that we highly recommend. This is to change the Save workspace to .RData on exit to Never and uncheck the Restore .RData into workspace at start. By default, R saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. We find that this causes confusion especially when we share code with colleagues and assume they have this .RData file. To change these options, make your General settings look like this: 77.6 Keeping organized with RStudio projects A data analysis project is not always a dataset and a script. It often involves several scripts, the data may be saved across several files, and it is often convenient to save intermediate files. RStudio projects provide a way to keep all this organized in one folder. We will later learn how RStudio facilitates sharing work in these projects. To organize yourself on a computer, it is essential that you understand your filesystem. A systematically organized filesystem can greatly increase your productivity, especially if you work on more than one project at at time. In a later section, we explain how Unix provides a powerful tool to help you with this. In this section, we will create a folder in a default location for illustrative purposes. Once you become a regular R user, you will want to think carefully about the best location for the folder in which you will keep a new project. To start a project, click on File and then New Project Unless you have a pre-selected folder to save the work, you will select the New Directory option. Then, for a data analysis project, you usually select the New Project option: Now you will have to decide on the location of the folder that will be associated with your project, as well as the name of the folder. When choosing a folder name, just like with file names, make sure it is a meaningful name that will help you remember what the project is about. As with files, we recommend using lower case letters, no spaces, and hyphens to separate words. We will call the folder for this project my-first-project. This will then generate a file called my-first-project.Rproj in the folder associated with the project. We will see how this is useful a few lines below. You will be given options on where this folder should be on your file system. In this example, we will place it in our home folder, but this is generally not good practice. As we describe in more detail later, you want to organize your file system following a hierarchical approach and you might have a folder called projects where you keep a folder for each project. When you start using RStudio with a project, you will see the project name in the upper left corner. This will remind you what project this particular RStudio session belongs to. When you open an RStudio session with no project, it will say Project: (None). When working on a project, all files will be saved and searched for in the folder associated with the project. Below, we show an example of a script that we wrote and saved with the name code.R. Because we used a meaningful name for the project, we can be a bit less informative when we save the files. Although we do not do it here, you can have several scripts open at once. You simply need to click File, then New File and pick the type of file you want to edit. One of the main advantages of using Projects is that after closing RStudio, if we wish to continue where we left off on the project, we simply double click or open the file saved when we first created the RStudio project. In this case, the file is called my-first-project.Rproj. If we open this file, RStudio will start up and open the scripts we were editing. 77.7 Using Git and GitHub in RStudio We are now ready to clone a repo, start editing files on our computer and syncing to GitHub. We will use RStudio to facilitate this. We will also use Unix for the first time! A first step is to let Git know who we are. This will make it easier to connect with GitHub. We start by opening a terminal window in RStudio (remember you can get one through Tools in the menu bar). Now we use the git config command to tell Git who we are. We will type the following two commands in our terminal window: git config --global user.name &quot;Your Name&quot; git config --global user.mail &quot;your@email.com&quot; You need to use the email account that you used to open your GitHub account. The RStudio sessions should look something like this: Now we are ready to start a RStudio project that uses version control and stores the code on a GitHub repo. To do this, we start a project but, instead of New Directory, we will select Version Control: Then we will select Git as our version control system: The repository URL is the link you used to clone. Above, we used the https://github.com/username/homework-0.git as an example. In the project directory name, you need to put the name of the folder that was generated, which in our example will be the name of the repo homework-0. This will create a folder called homework-0 on your local system. Once you do this, the project is created and it is aware of the connection to a GitHub repo. You will see on the top right corner the name and type of project as well as a new tab on the upper right pane titled Git. If you select this tab, it will show you the files on your project with some icons that give you information about these files and their relationship to the repo. In the example below, we already added a file to the folder, called code.R which you can see in the editing pane. We now need to pay attention to the Git pane. It is important to know that your local files and the GitHub repo will not be synched automatically. You have to sync when you are ready and we show you how below. To truly understand why RStudio does not automatically sync, we need to learn more details about Git, and we will do so a bit later. Right now, we will quickly show you how to sync with this simple example. The main actions in Git are to: pull changes from the remote repo, in this case the GitHub repo, add files, or as we say in the Git lingo stage files, commit changes to the local repo and push changes to the remote repo, in our case the GitHub repo. Before we start working on a collaborative project, usually the first thing we do is pull in the changes from the remote repo, in our case the one on GitHub. However, for the example shown here, since we are starting with an empty repo and we are the only ones making changes, we don’t need to start by pulling. In RStudio, the status of the file as it relates to the remote and local repos are represented in the status symbols with colors. A yellow square means that Git knows nothing about this file. To sync with the GitHub repo, we need to add the file, then commit the change to our local Git repo, then push the change to the GitHub repo. Right now, the file is just on our computer. To add the file using RStudio, we click the Stage box. You will see that the status icon now changes to a green A. Note: we are only adding the code.R file. We don’t necessarily need to add all the files in our local repo to the GitHub repo; only the ones we want to keep track of or the ones we want to share. If our work is producing files of a certain type that we do not want to keep track of, we can add the suffix that defines these files to the .gitignore file. More details on using .gitignore are included here. These files will stop appearing in your RStudio Git pane. For the example shown here, we will only be adding the code.R. But, in general, for an RStudio project, we recommend adding both the .gitignore and .Rproj files. Now we are ready to commit the file to our local repo. In RStudio, we can use the Commit button. This will open a new dialog window. With Git, whenever we commit a change, we are required to enter a comment describing the changes being committed. In this case, we will simply describe that we are adding a new script. In this dialog box, RStudio also gives you a summary of what you are changing to the GitHub repo. In this case, because it is a new file, the entire file is highlighted as green, which highlights the changes. Once we hit the commit button, we should see a message from Git with a summary of the changes that were committed. Now we are ready to push these changes to the GitHub repo. We can do this by clicking on the Push button on the top right corner: We now see a message from Git letting us know that the push has succeeded. In the pop-up window we no longer see the code.R file. This is because no new changes have been performed since we last pushed. We can exit this pop-up window now and continue working on our code. If we now visit our repo on the web, we will see that it matches our local copy. Congratulations you have successfully created a GitHub code repository! Soon we will learn how to share our code. But before we continue learning about Git, we will provide a brief introduction to Unix and how it is used to keep organized. 77.8 Running R without RStudio Although we highly recommend that beginners use R through RStudio, you can use R without RStudio. You can start it like any other program. If you followed the default installation on Windows, a shortcut will appear on your desktop which you can click to start R. On the Mac, R will be in the Application folder. If you start R without RStudio, you will see an R console in which you can start typing commands: But we will be much more productive using an editor developed for coding, such as the one provided by RStudio. In the next section, we demonstrate how to install RStudio. "],
["organizing-with-unix.html", "Chapter 78 Organizing with Unix 78.1 The terminal 78.2 The filesystem 78.3 Unix commands 78.4 Some examples 78.5 More Unix commands 78.6 Preparing for a data science project", " Chapter 78 Organizing with Unix Unix is the operating system of choice in data science. We will introduce you to the Unix way of thinking using an example: how to keep a data analysis project organized. We will learn some of the most commonly used commands along the way. However, we won’t go into the details here. We highly encourage you to learn more, especially when you find yourself using the mouse too much or performing a repetitive task often. In those cases, there is probably a more efficient way to do it in Unix. Here are some basic courses to get you started: https://www.codecademy.com/learn/learn-the-command-line https://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1 https://www.coursera.org/learn/unix There are many reference books as well. Bite Size Linux and Bite Size Command Line are two particularly clear, succinct, and complete examples. When searching for Unix resources, keep in mind that other terms used to describe what we will learn here are Linux, the shell and the command line. Basically, what we are learning is a series of commands and a way of thinking that facilitates the organization of files without using the mouse. To serve as motivation, we are going to start constructing a directory using Unix tools and RStudio. 78.1 The terminal The terminal is our window into the Unix world. Instead of clicking, dragging and dropping to organize our files and folders, we will be typing commands into the terminal. The way we do this is similar to how we type commands into the R console, but instead of generating plots and statistical summaries, we will be organizing files on our system. We have already described how we can access a terminal using RStudio, namely by going to Tools, then Terminal, then New Terminal. But often we want access to the terminal, but do not need RStudio. We already described how to access the terminal on the Mac by opening the application in the Utilities folder: You can also use the Spotlight feature on the Mac by typing command-spacebar, then type Terminal. On Windows, assuming you’ve installed Git Bash, we can also access a terminal without RStudio by running the Git Bash program: Once you have a terminal open, you can start typing commands. You should see a blinking cursor at the spot where what you type will show up. This position is called the command line. Once you type something and hit enter on Windows or return on the Mac, Unix will try to execute this command. If you want to try out an example, type this command into your command line: echo &quot;hello world&quot; The command echo is similar to cat in R. Executing this line should print out hello world, then return back to the command line. Notice that you can’t use the mouse to move around in the terminal. You have to use the keyboard. To go back to a command you previously typed, you can use the up arrow. Above we included a chunk of code showing Unix commands in the same way we have previously shown R commands. We will make sure to distinguish when the command is meant for R and when it is meant for Unix. 78.2 The filesystem We refer to all the files, folder, and programs on your computer as the filesystem. Keep in mind that folders and programs are also files, but this is a technicality we rarely think about and ignore in this book. We will focus on files and folders for now and discuss programs, or executables, in a later section. 78.2.1 Directories and subdirectories The first concept you need to grasp to become a Unix user is how your file system is organized. You should think of it as a series of nested folders each containing files, folders, and executables. Here is a visual representation of the structure we are describing: In Unix, we refer to folders as directories. Directories that are inside other directories are often referred to as subdirectories. So, for example, in the figure above, the directory docs has two subdirectories: reports and resumes, and docs is a subdirectory of home. 78.2.2 The home directory The home directory is where all your stuff is kept. In the figure above, the directory called home represents your home directory, but that is rarely the name used. On your system, the name of your home directory is likely the same as your username on that system. Below is an example on Windows showing a home directory, in this case, named rafa: Here is an example from a Mac: Now, look back at the figure showing a filesystem. Suppose you are using a point-and-click system and you want to remove the file cv.tex. Imagine that on your screen you can see the home directory. To erase this file, you would double click on the home directory, then docs, the resumes, and then drag cv.tex to the trash. Here you are experiencing the hierarchical nature of the system: cv.tex is a file inside the resumes directory, which is a subdirectory inside the docs directory, which is a subdirectory of the home directory. Now suppose you can’t see your home directory on your screen. You would somehow need to make it appear on your screen. One way to do this is to navigate from what is called the root directory all the way to your home directory. Any file system will have what is called a root directory which is the directory that contains all directories. The home directory shown in the figure above will usually be two or more levels from the root. On Windows, you will have a structure like this: while on the Mac, it will be like this: Note for Windows User: The typical R installation will make your Documents directory your home directory in R. This will likely be different from your home directory in Git Bash. Generally, when we discuss home directories, we refer to the Unix home directory which for Windows, in this book, is the Git Bash unix directory. 78.2.3 Working directory The concept of a current location is part of the point-and-click experience: at any given moment we are in a folder and see the content of that folder. As you search for a file, as we did above, you are experiencing the concept of a current location: once you double click on a directory, you change locations and are now in that folder, as opposed to the folder you were in before. In Unix, we don’t have the same visual cues, but the concept of a current location is indispensable. We refer to this as the working directory. Each terminal window you have open has a working directory associated with it. How do we know what is our working directory? To answer this, we learn our first Unix command: pwd, which stands for print working directory. This command returns the working directory. Open a terminal and type: pwd We do not show the result of running this command because it will be quite different on your system compared to others. If you open a terminal and type pwd as your first command, you should see something like /Users/yourusername on a Mac or something like /c/Users/yourusername on Windows. The character string returned by calling pwd represents your working directory. When we first open a terminal, it will start in our home directory so in this case the working directory is the home directory. Notice that the forward slashes / in the strings above separate directories. So, for example, the location /c/Users/rafa implies that our working directory is called rafa and it is a subdirectory of Users, which is a subdirectory of c, which is a subdirectory of the root directory. The root directory is therefore represented by just a forward slash: /. 78.2.4 Paths We refer to the string returned by pwd as the full path of the working directory. The name comes from the fact that this string spells out the path you need to follow to get to the directory in question from the root directory. Every directory has a full path. Later, we will learn about relative paths, which tell us how to get to a directory from the working directory. In Unix, we use the shorthand ~ as a nickname for your home directory. So, for example, if docs is a directory in your home directory, the full path for docs can be written like this ~/docs. Most terminals will show the path to your working directory right on the command line. If you are using default settings and open a terminal on the Mac, you will see that right at the command line you have something like computername:~ username with ~ representing your working directory, which in this example is the home directory ~. The same is true for the Git Bash terminal where you will see something like username@computername MINGW64 ~, with the working directory at the end. When we change directories, we will see this change on both Macs and Windows. 78.3 Unix commands We will now learn a series of Unix commands that will permit us to prepare a directory for a data science project. We also provide examples of commands that, if you type into your terminal, will return an error. This is because we are assuming the file system in the earlier diagram. Your file system is different. In the next section, we will provide examples that you can type in. 78.3.1 ls: Listing directory content In a point-and-click system, we know what is in a directory because we see it. In the terminal, we do not see the icons. Instead, we use the command ls to list the directory content. To see the content of you home directory, open a terminal and type: ls We will see more examples soon. 78.3.2 mkdir and rmdir: make and remove a directory When we are preparing for a data science project, we will need to create directories. In Unix, we can do this with the command mkdir, which stands for make directory. Because you will soon be working on several projects, we highly recommend creating a directory called projects in your home directory. You can try this particular example on your system. Open a terminal and type: mkdir projects If you do this correctly, nothing will happen: no news is good news. If the directory already exists, you will get an error message and the existing directory will remain untouched. To confirm that you created these directories, you can list the directories: ls You should see the directories we just created listed. Perhaps you can also see many other directories that come pre-installed on your computer. For illustrative purposes, let’s make a few more directories. You can list more than one directory name like this: mkdir docs teaching You can check to see if the three directories were created: ls If you made a mistake and need to remove the directory, you can use the command rmdir to remove it. mkdir junk rmdir junk This will remove the directory as long as it is empty. If it is not empty, you will get an error message and the directory will remain untouched. To remove directories that are not empty, we we will learn about the command rm later. 78.3.3 cd: Navigating the filesystem by changing directories Next we want to create directories inside directories that we have already created. We also want to avoid pointing and clicking our way through the filesystem. We will explain how do this in Unix, using the command line. Suppose we open a terminal. Our working directory is our home directory. We want to change our working directory to projects. We do this using the cd command, which stands for change directory: cd projects To check that the working directory changed, we can use a command we previously learned: pwd Our working directory should now be ~/projects. Note that on your computer the home directory ~ will be spelled out to something like /c/Users/yourusername). Important Pro Tip: In Unix you can auto-complete by hitting tab. This means that we can type cd d then hit tab. Unix will either auto-complete if docs is the only directory/file starting with d or show you the options. Try it out! Using Unix without auto-complete will make it unbearable. When using cd, we can either type a full path, which will start with / or ~, or a relative path. In the example above, in which we typed cd projects, we used a relative path. If the path you type does not start with / or ~, Unix will assume you are typing a relative path, meaning that it will look for the directory in your current working directory. So something like this will give you an error: cd Users because there is no Users directory in your working directory. Now suppose we want to move back to the directory in which projects is a subdirectory, referred to as the parent directory. We could use the full path of the parent directory, but Unix provides a shortcut for this: the parent directory of the working directory is represented with two dots: ... So to move back we simply type: cd .. You should now be back in your home directory which you can confirm using pwd. Because we can use full paths with cd, the following command: cd ~ will always take us back to the home directory, no matter where we are in the filesystem. The working directory also has a nickname and it is a single .. So if you type: cd . You will not move. Although this particular use of . is not useful, this nickname does come in handy sometimes. The reasons are not relevant for this section, but you should still be aware of this fact. In summary, we have learned that when using cd we either stay put, move to a new directory using the desired directories name, or move back to the parent directory using ... Now when typing directory names, we can concatenate directories with the forward-slashes. So if we want a command that takes us to the projects directory no matter where we are in the filesystem, we can type: cd ~/projects which is equivalent to writing the entire path out. For example, in Windows we would write something like cd /c/Users/yourusername/projects The last two commands are equivalent and in both cases we are typing the full path. When typing out the path of the directory we want, either full or relative, we can concatenate directories with the forward-slashes. We already saw that we can move to projects directory regardless of where we are by typing the full path like this: cd ~/projects We can also concatenate directory names for relative paths. So, for instance, if we want to move back to the parent directory of the parent directory of the working directory, we can type: cd ../.. A couple of final tips related to the cd command. First, you can go back to whatever directory you just left by typing: cd - This can be useful if you type a very long path and then realize you want to go back to where you were, and that too has a very long path. Second, if you just type: cd you will be returned to your home directory. 78.4 Some examples Let’s explore some examples of using cd. To help visualize, we will show the graphical representation of our file system vertically: Suppose our working directory is ~/projects and we want to move to figs in project-1. Here it is convenient to use relative paths: cd project-1/figs Now suppose our working directory is ~/projects and we want to move to reports in docs, how can we do this? One way is to use relative paths: cd ../docs/reports Another is to use the full path: cd ~/docs/reports If you are trying this out on your system, remember to use auto-complete. Let’s examine one more example. Suppose we are in ~/projects/project-1/figs and want to change to ~/projects/project-2. Again, there are two ways. With relative paths: cd ../../proejct-2 and with full paths: cd ~/projects/project-2 78.5 More Unix commands 78.5.1 mv: moving files In a point-and-click system, we move files from one directory to another by dragging and dropping. In Unix, we use the mv command. Warning: mv will not ask “are you sure?” if your move results in overwriting a file. Now that you know how to use full and relative paths, using mv is relatively straightforward. The general form is: mv path_to_file path_to_destination_directory So, for example, if we want to move the file cv.tex from resumes to reports, you could use the full paths like this: mv ~/docs/resumes/cv.tex ~/docs/reports/ You can also use relative paths. So you could do this: cd ~/docs/resumes mv cv.tex ../reports/ or this: cd ~/docs/reports/ mv ../cv.tex ./ Notice that, in the last one, we used the working directory shortcut . to give a relative path as the destination directory. We can also use mv to change the name of a file. To do this, instead of the second argument being the destination directory, it also includes a filename. So, for example, to change the name from cv.tex to resume.tex, we simply type: cd ~/docs/resumes mv cv.tex resume.tex We can also combine the move and a rename. For example: cd ~/docs/resumes mv cv.tex ../reports/resume.tex And we can move entire directories. So to move the resumes directory into reports, we do as follows: mv ~/docs/resumes ~/docs/reports/ It is important to add the last / to make it clear you do not want to rename the resumes directory to reports, but rather move into reports. 78.5.2 cp: copying files The command cp behaves similar to mv except instead of moving, we copy the file, meaning that the original file stays untouched. So in all the mv examples above, you can switch mv to cp and they will copy instead of move with one exception: we can’t copy entire directories without learning about arguments. 78.5.3 rm: removing files In point-and-click systems, we remove files by dragging and dropping them into the trash or using a special click on the mouse. In Unix, we use the rm command. Warning: Unlike throwing files into the trash, rm is permanent so be careful! The general way it works is as follows: rm filename You can actually list files as well like this: rm filename-1 filename-2 filename-3 You can use full or relative paths. To remove directories, you will have to learn about arguments which we do later. 78.5.4 less: looking at a file Often you want to quickly look at the content of a file. If this file is a text file, the quickest way to do is by using the command less. To look a the file cv.tex, you do this: cd ~/docs/resumes less cv.tex To exit the viewer, you type q. If the files are long, you can use the arrow keys to move up and down. There are many other keyboard commands you can use within less to, for example, search or jump pages. You will learn more about this in a later section. If you are wondering why the command is called less, it is because the original was called more, as in “show me more of this file”. The second version was called less because of the saying “less is more”. 78.6 Preparing for a data science project We are now ready to prepare a directory for a project. You should start by creating a directory where you will keep all your projects. We recommend a directory called projects in your home directory. To do this you would type: cd ~ mkdir projects Our project relates to gun violence murders so we will call the directory for our project murders. It will be a subdirectory in our projects directories. In the murders directory, we will create two subdirectories to hold the raw data and intermediate data. We will call these data and rda respectively. Open a terminal and make sure you are in the home directory: cd ~ Now run the following commands to create the directory structure we want. At the end, we use ls and pwd to confirm we have generated the correct directories in the correct working directory: cd projects mkdir murders cd murders mkdir data rdas ls pwd Note that the full path of our murders dataset is ~/projects/murders. So if we open a new terminal and want to navigate into that directory we type: cd projects/murders In RStudio, when you start a new project, you can pick Existing Directory instead of New Directory: and write the full path of the murders directory: Once you do this, you will see the rdas and data directories you created in the Files tab. Keep in mind that when we are in this project, our default working directory will be ~/projects/murders. You can confirm this by typing getwd() into your R session. This is important because it will help us organize the code when we need to write file paths. Pro tip: always use relative paths in code for data science projects. These should be relative to the default working directory. The problem with using full paths is that your code is unlikely to work on filesystems other that yours since the directory structures will be different. This includes using the home directory ~ as part of your path. Let’s now write a script that downloads a file into the data directory. We will call this file download-data.R. The content of this file will be: url &lt;- &quot;https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv&quot; dest_file &lt;- &quot;data/murders.csv&quot; download.file(url, destfile = dest_file) Notice that we are using the relative path data/muders.csv. Run this code in R and you will see that a file is added to the data directory. Now we are ready to write a script to read this data and prepare a table that we can use for analysis. Call the file wrangle-data.R. The content of this file will be: library(tidyverse) murders &lt;- read_csv(&quot;data/murders.csv&quot;) murders &lt;-murders %&gt;% mutate(region = factor(region), rate = total / population * 10^5) save(murders, file = &quot;rdas/murders.rda&quot;) Again note that we use relative paths exclusively. In this file, we introduce a command we have not seen: save. The save command in R saves objects into what is called an rda file: rda is short for R data. We recommend using the .rda suffix on files saving R objects. You will see that .RData is also used. If you run this code above, the processed data object will be saved in a file in the rda directory. Although not the case here, this approach is often practical because generating the data object we use for final analyses and plots can be a complex and time consuming process. So we run this process once and save the file. But we still want to be able to generate the entire analysis from the raw data. Now we are ready to write the analysis file. Let’s call it analysis.R. The content should be the following: library(tidyverse) load(&quot;rdas/murders.rda&quot;) murders %&gt;% mutate(abb = reorder(abb, rate)) %&gt;% ggplot(aes(abb, rate)) + geom_bar(width = 0.5, stat = &quot;identity&quot;, color = &quot;black&quot;) + coord_flip() If you run this analysis, you will see that it generates a plot. Now suppose we want to save the generated plot for use in a report or presentation. We can do this with the ggplot command ggsave. But where do we put the graph? We should be systematically organized so we will save plots to a directory called figs. Start by creating a directory by typing the following in the terminal: mkdir figs and then you can add the line: ggsave(&quot;figs/barplot.png&quot;) to your R script. If you run the script now, a png file will be saved into the figs directory. If we wanted to copy that file to some other directory where we are developing a presentation, we can avoid using the mouse by using the cp command in our terminal. You now have a self-contained analysis in one directory. One final recommendation is to create a README.txt file describing what each of these files does for the benefit of others reading your code, including your future self. This would not be a script but just some notes. One of the options provided when opening a new file in RStudio is a text file. You can save something like this into the text file: We analyze US gun murder data collected by the FBI. download-data.R - Downloads csv file to data directory wrangle-data.R - Creates a derived dataset and saves as R object in rdas directory analysis.R - A plot is generated and saved in the figs directory. If you now type pwd, you will see you are in ~/projects/murders with ~ representing the home directory. You have now successfully used Unix and RStudio to create a directory for a project. When a new project comes your way, you can simply cd back to project directory, create a new directory and start a new project. You can see a version of this project, organized with Unix directories, on GitHub. You can download a copy to your computer by using the git clone command on your terminal. This command will create a directory called murders on your working directory so be careful where you call it from. git clone https://github.com/rairizarry/murders.git "],
["git.html", "Chapter 79 Git 79.1 Why use Git and GitHub? 79.2 Overview of Git 79.3 Initilazing a Git directory", " Chapter 79 Git Here we provide some more details on Git and GitHub. However, we are only scratching the surface. To learn more about this topic, we highly recommend the following resources: Codeacademy GitHub Guides Try Git tutorial Happy Git and GitHub for the useR 79.1 Why use Git and GitHub? There are three main reasons to use Git and GitHub. Share: Even if we do not take advantage of the advanced and powerful version control functionality, we can still use Git and GitHub to share our code. We have already shown how we can do this with RStudio. Collaborating: Once you set up a central repo, you can have multiple people make changes to code and keep versions synched. GitHub provides a free service for centralized repos. GitHub also has a special utility, called a pull request, that can be used by anybody to suggest changes to your code. You can easily either accept or deny the request. Version control: The version control capabilities of Git permit us to keep track of changes we make to our code. We can also revert back to previous versions of files. Git also permits us to create branches in which we can test out ideas, then decide if we merge the new branch with the original. Here we focus on the sharing aspects of Git and GitHub and refer the reader to the links above to learn more about this powerful tool. 79.2 Overview of Git To effectively permit version control and collaboration in Git, files move across four different areas: But how does it all get started? There are two ways. We can clone an existing repo or initialize one. We will explore cloning first. 79.2.1 Clone We are going to clone an existing Upstream Repository. You can see it on GitHub here. By visiting this page, you can see multiple files and directories. This is the Upstream Repository. By clicking the green clone button, we can copy the repo’s URL https://github.com/rairizarry/murders.git. But what does clone mean? Rather than download all these files to your computer, we are going to actually copy the entire Git structure, which means we will add the files and directories to each of the three local stages: Working Directory, Staging Area, and Local Repository. When you clone, all three are exactly the same to start. You can quickly see an example of this by doing the following. Open a terminal and type: pwd mkdir git-example cd git-example git clone https://github.com/rairizarry/murders.git cd murders #&gt; /Users/ririzarr/myDocuments/teaching/data-science/dsbook #&gt; Cloning into &#39;murders&#39;... You now have cloned a GitHub repo and have a working Git directory, with all the files, on your system. ls #&gt; README.txt #&gt; analysis.R #&gt; data #&gt; download-data.R #&gt; murders.Rproj #&gt; rdas #&gt; report.Rmd #&gt; report.md #&gt; report_files #&gt; wrangle-data.R The Working Directory is the same as your Unix working directory. When you edit files using an editor such as RStudio, you change the files in this area and only in this area. Git can tell you how these files relate to the versions of the files in other areas with the command git status: If you check the status now, you will see that nothing has changed and you get the following message: git status #&gt; On branch master #&gt; Your branch is up to date with &#39;origin/master&#39;. #&gt; #&gt; nothing to commit, working tree clean Now we are going to make changes to these files. Eventually, we want these new version of the files to be tracked and synched with the upstream repo. But we don’t want to keep track of every little change: we don’t want to sync until we are sure these versions are final enough to share. For this reason, edits in the staging area are not kept by the version control system. To demonstrate, we add a file to the staging area with the git add command. Below we create a file using the Unix echo command just as an example (in reality you would use RStudio): echo &quot;test&quot; &gt;&gt; new-file.txt We are also adding a temporary file that we do not want to track at all: echo &quot;temporary&quot; &gt;&gt; tmp.txt Now we can stage the file we eventually want to add to our repository: git add new-file.txt Notice what the status says now: git status #&gt; On branch master #&gt; Your branch is up to date with &#39;origin/master&#39;. #&gt; #&gt; Changes to be committed: #&gt; (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) #&gt; #&gt; new file: new-file.txt #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; tmp.txt Because new-file.txt is staged, the current version of the file will get added to the local repository next time we commit, which we do as follows: git commit -m &quot;adding a new file&quot; #&gt; [master d1231ab] adding a new file #&gt; 1 file changed, 1 insertion(+) #&gt; create mode 100644 new-file.txt We have now changed to local repo: git status #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 1 commit. #&gt; (use &quot;git push&quot; to publish your local commits) #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; tmp.txt #&gt; #&gt; nothing added to commit but untracked files present (use &quot;git add&quot; to track) However, if we edit that file again, it changes only in the working directory. To add to the local repo, we need to stage it and commit the changes that are added to the local repo: echo &quot;adding a line&quot; &gt;&gt; new-file.txt git add new-file.txt git commit -m &quot;adding a new line to new-file&quot; git status #&gt; [master 8797450] adding a new line to new-file #&gt; 1 file changed, 1 insertion(+) #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 2 commits. #&gt; (use &quot;git push&quot; to publish your local commits) #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; tmp.txt #&gt; #&gt; nothing added to commit but untracked files present (use &quot;git add&quot; to track) Note that this step is often unnecessary in our uses of Git. We can skip the staging part if we add the file name to the commit command like this: echo &quot;adding a second line&quot; &gt;&gt; new-file.txt git commit -m &quot;minor change to new-file&quot; new-file.txt git status #&gt; [master 52519be] minor change to new-file #&gt; 1 file changed, 1 insertion(+) #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 3 commits. #&gt; (use &quot;git push&quot; to publish your local commits) #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; tmp.txt #&gt; #&gt; nothing added to commit but untracked files present (use &quot;git add&quot; to track) We can keep track of all the changes we have made with: git log new-file.txt #&gt; commit 52519be1640a76f7e847bc97f4dbc2b0ac41a8a1 #&gt; Author: Rafael A. Irizarry &lt;rairizarry@gmail.com&gt; #&gt; Date: Tue Nov 13 11:01:30 2018 -0500 #&gt; #&gt; minor change to new-file #&gt; #&gt; commit 8797450a21c317d37c1fc1958ed28019783bfeb5 #&gt; Author: Rafael A. Irizarry &lt;rairizarry@gmail.com&gt; #&gt; Date: Tue Nov 13 11:01:30 2018 -0500 #&gt; #&gt; adding a new line to new-file #&gt; #&gt; commit d1231ab47b81976e6ceb08d459dcefe45bc08bd1 #&gt; Author: Rafael A. Irizarry &lt;rairizarry@gmail.com&gt; #&gt; Date: Tue Nov 13 11:01:29 2018 -0500 #&gt; #&gt; adding a new file To keep everything synched, the final step is to push the changes to the upstream repo. This is done with the git push command like this: git push However, in this particular example, you will not be able to do this because you do not have permission to edit the upstream repo. If this was your repo, you could. If this is a collaborative project, the upstream repo may change and become different than our version. To update our local repository to be like the upstream repo, we use the command fetch: git fetch And then to make these copies to the staging and working directory areas, we use the command: git merge However, we often just want to change both with one command. For this, we use: git pull We earlier learned how RStudio has buttons to do all this. The details provided here should help you understand what happens in the background. 79.3 Initilazing a Git directory Now let’s learn the second way we can get started: by initializing a directory on our own computer rather than cloning. We will show how we created the GitHub for our gun murders project. We first created a project on our computer so we already had all the files and directory ready. But we did not yet have a Git local repo or GitHub upstream repo. We start by creating a new repo on our GitHub page: We click on the New button: We called it murders to match the name of the directory on our local system. But if you are doing this for another project, please choose an appropriate name. We then get a series of instructions on how to get started. But we can instead use what we have learned. The main thing we need from this page is to copy the repo’s URL, in this case: https://github.com/rairizarry/murders.git. At this moment, we can start a terminal and cd into our local projects directory. In our example, it would be: cd ~/projects/murders We then intialize the directory. This turns the directory into Git directory and Git starts tracking: git init All the files are now only in our working directory. The next step is to connect the local repo with the GitHub repo. In a previous example, we had RStudio do this for us. Now we need to do it ourselves. We can by adding any of the files and committing it: git add README.txt git commit -m &quot;First commit. Adding README.txt file just to get started&quot; We now have a file in our local repo and can connect it to the upstream repo, which has url: https://github.com/rairizarry/murders.git. To do this, we use the the command git remote add. git remote add origin `https://github.com/rairizarry/murders.git` We can now use git push since there is a connection to an upstream repo: git push We can continue adding and committing each file, but it might be easier to use RStudio. To do this, start the project by opening the Rproj file. The git icons should appear: We can now go to GitHub and confirm that our files are there. "],
["reproducible-reports-with-r-markdown.html", "Chapter 80 Reproducible reports with R Markdown 80.1 R Markdown 80.2 knitR 80.3 More on R markdown", " Chapter 80 Reproducible reports with R Markdown The final product of a data analysis project is often a report. Many scientific publications can be thought of as a final report of a data analysis. The same is true for news articles based on data, an analysis report for your company, or lecture notes for a class on how to analyze data. The reports are often on paper or in a PDF that includes a textual description of the findings along with some figures and tables resulting from the analysis. Imagine that after you finish the analysis and the report, you are told that you were given the wrong dataset, You are sent a new one and you are asked run the same analysis with this new dataset. Or what if you realize that a mistake was made and need to re-examine the code, fix the error and re-run the analysis? Or imagine that someone you are training wants to see the code and be able to reproduce the results to learn about your approach? Situations like the ones just described are actually quite common for a data scientist. Here, we describe how to generate reproducible reports with R markdown and the knitR package in a way that will greatly help with situations such as the ones described here. The main feature is that code and textual descriptions can be combined into the same document, and the figures and tables produced by the code are automatically added to the document. getwd() #&gt; [1] &quot;/Users/ririzarr/myDocuments/teaching/data-science/dsbook&quot; 80.1 R Markdown R markdown is a format for literate programming document. It is based on markdown, a markup language that is widely used to generate html pages. You can learn more about markdown here. Literate programming weaves instructions, documentation and detailed comments in between machine executable code, producing a document that describes the program that is best for human understanding (Knuth 1984). Unlike a word processor, such as Microsoft Word, where what you see is what you get, with R markdown, you need to compile the document into the final report. The R markdown document looks different than the final product. This seems like a disadvantage at first, but it is not because, for example, instead of producing plots and inserting them one by one into the word processing document, the plots are automatically added. In RStudio, you can start an R Markdown document by clicking on File, New File, the R Markdown, like this: You will then be asked to enter a title and author for your document. We are going to prepare a report on gun murders so we will give it an appropriate name. You can also decide what format you would like the final report to be in: HTML, PDF, or Microsoft Word. Later, we can easily change this, but here we select html as it is the preferred format for debugging purposes: This will generate a template file: As a convention, we use the Rmd suffix for these files. Once you gain experience with R Markdown, you will be able to do this without the template and can simply start from a blank template. In the template, you will see several things to note. 80.1.1 The header At the top you see: --- title: &quot;Report on Gun Murders&quot; author: &quot;Rafael Irizarry&quot; date: &quot;April 16, 2018&quot; output: html_document --- The things between the --- is the header. We actually don’t need a header, but it is often useful. You can define many other things in the header than what is included in the template. We don’t discuss those here, but much information is available online. The one parameter that we will highlight is output. By changing this to, say, pdf_document, we can control the type of output that is produced when we compile. 80.1.2 R code chunks In various places in the document, we see something like this: ```{r} summary(pressure) ``` To add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows. These are the code chunks. When you compile the document, the R code inside the chunk, in this case summary(pressure), will be evaluated and the result included in that position in the final document. This applies to plots as well; the plot will be placed in that position. We can write something like this: ```{r} plot(pressure) ``` By default, the code will show up as well. To avoid having the code show up, you can use an argument. To avoid this, you can use the argument echo=FALSE. For example: ```{r, echo=FALSE} plot(pressure) ``` We recommend getting into the habit of adding a label to the R code chunks. This will be very useful when debugging, among other situations. You do this by adding a descriptive word like this: ```{r pressure-summary} summary(pressure) ``` 80.1.3 Global options One of the R chunks contains a complex looking call: ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` We will not cover this here, but as you become more experienced with R Markdown, you will learn the advantages of setting global options for the compilation process. 80.2 knitR We use the knitR package to compile R markdown documents. The specific function used to compile is the knit function which takes a filename as input. RStudio provides a button that actually makes it easier to compile the document. For the screenshot below, we have edited the document so that a report on gun murders is produced. You can see the file here. You can now click on the Knit button: The first time you click on the Knit button, a dialog box may appear asking you to install packages you need: Once you have installed the packages, clicking the Knit will compile your R markdown file and the resulting document will pop-up: This produces an html document which you can see in your working directory. To view it, open a terminal and list the files. You can open the file in a browser and use this to present your analysis. You can also produce a PDF or Microsoft document by changing: output: html_document to output: pdf_document or output: word_document. We can also produce documents that render on GitHub using output: github_document like this: This will produce a markdown file, with suffix md, that renders in GitHub. Because we have uploaded these files to GitHub, you can click on the md file: and you will see the report as a webpage: This is a convenient way to share your reports. 80.3 More on R markdown There is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including: RStudio’s tutorial The cheat sheet The knitR book "],
["advanced-unix.html", "Chapter 81 Advanced Unix 81.1 Arguments 81.2 Getting help 81.3 Pipes 81.4 Wild cards 81.5 Environment variables 81.6 Shells 81.7 Executables 81.8 Permissions and file types 81.9 Commands you should learn 81.10 File manipulation in R", " Chapter 81 Advanced Unix Most Unix implementations include a large number of powerful tools and utilities. We have just learned the very basics here. We recommend that you use Unix as your main file management tool. It will take time to become comfortable with it, but as you struggle, you will find yourself learning just by looking up solutions on the internet. In this section, we superficially cover slightly more advanced topics. The main purpose of the section is to make you aware of what is available rather than explain everything in detail. 81.1 Arguments Most Unix commands can be run with arguments. Arguments are typically defined by using a dash - or two dashes --, followed by a letter or a word. An example of an argument is the -r in front of rm. The r stands for recursive and the result is that files and directories are removed recursively, which means that if you type: rm -r directory-name all files, subdirectories, files in subdirectories, subdirectories in subdirectories, etc. will be removed. This is equivalent to throwing a folder in the trash, except you can’t recover it. Once you remove it, it is deleted for good. Often, when you are removing directories, you will encounter files that are protected. In such cases, you can use the argument -f which stands for force. You can also combine arguments. So, for instance, to remove a directory regardless of protected files, you type: rm -rf directory-name Remember that once you remove there is no going back, so use this command very carefully. A command that is often called with argument is ls. Here are some examples: ls -a The a stands for all. This argument makes ls show you all files in the directory, including hidden files. In Unix, all files starting with a . are hidden. Many applications create hidden directories to store important information without getting in the way of your work. An example is git. Once you initialize a directory as a git directory with git init, a hidden directory called .git is created. Another hidden file is the .gitignore file. Another example of using an argument is: ls -l The l stands for long and the result is that more information about the files is shown. It is often useful to see files in chronological order. For that we use: ls -t and to reverse the order of how files are shown you can use: ls -r We can combine all these arguments to show more information for all files in reverse chronological order: ls -lart Each command has a different set of arguments. In the next section, we learn how to find out what they each do. 81.2 Getting help As you may have noticed, Unix uses an extreme version of abbreviations. This makes it very efficient, but hard to guess how to call commands. To make up for this weakness, Unix includes complete help files or man pages (man is short for manual). In most systems, you can type man followed by the command name to get help. So for ls, we would type: man ls This command is not available in some of the compact implementations of Unix, such as Git Bash. An alternative way to get help that works on Git Bash is to type the command followed by --help. So for ls, it would be as follows: ls --help 81.3 Pipes The help pages are typically long and if you type the commands above to see the help, it scrolls all the way to the end. It would be useful if we could save the help to a file and then use less to see it. The pipe, written like this |, does something similar. It pipes the results of a command to the command after the pipe. This is similar to the pipe %&gt;% that we use in R. To get more help we thus can type: man ls | less or in Git Bash: ls --help | less This is also useful when listing files with many files. We can type: ls -lart | less 81.4 Wild cards One of the most powerful aspects of Unix are the wild cards. Suppose we want to remove all the temporary html files produced while trouble shooting for a project. Imagine there are dozens of files. It would be quite painful to remove them one by one. In Unix, we can actually write an expression that means all the files that end in .html. To do this we type wildcard *. Specifically, to list all html files, we would type: ls *.html To remove all html files in a directory, we would type: rm *.html The other useful wild card is the ? symbol. This means any character. So if all the files we want to erase have the form file-001.html with the numbers going from 1 to 999, we can type: rm file-???.html This will only remove files with that format. We can combine wild cards. For example, to remove all files with the name file-001 regardless of suffix, we can type: rm file-???.* 81.5 Environment variables Unix has settings that affect your command line environment. These are called environment variables. The home directory is one of them. We can actually change some of these. In Unix, variables are distinguished from other entities by adding a $ in front. The home directory is stored in $HOME. Earlier we saw that echo is the Unix command for print. So we can see our home directory by typing: echo $HOME You can see them all by typing: env You can change some of these environment variables. But their names vary across different shells. We describe shells in the next section. 81.6 Shells Much of what we use in this chapter is part of what is called the Unix shell. There are actually different shells, but they appear quite similar. The differences are almost unnoticeable. They are also important, although we do not cover those here. You can see what shell you are using by typing: echo $SHELL The most common one is bash. Once you know the shell, you can change environmental variables. In Bash Shell, we do it using export variable value. To change the path, described in more detail soon, type: (Don’t actually run this command though!) export PATH = /usr/bin/ There is a program that is run before each terminal starts, where you can edit variables so they change whenever you call the terminal. This changes in different implementations, but if using bash, you can create a file called .bashrc, .bash_profile,.bash_login, or .profile. You might already have one. 81.7 Executables In Unix, all programs are files. They are called executables. So ls, mv and git are all files. But where are these program files? You can find out using the command which: which git #&gt; /usr/bin/git That directory is probably full of program files. The directory /usr/bin usually holds many program files. If you type: ls /usr/bin in your terminal, you will see several executable files. There are other directories that usually hold program files. The Application directory in the MAC or Program Files directory in Windows are examples. When you type ls, Unix knows to run a program which is an executable that is stored in some other directory. So how does Unix know where to find it? This information is included in the environmental variable $PATH. If you type: echo $PATH you will see a list of directories separated by :. The directory /usr/bin is probably one of the first ones on the list. Unix looks for program files in those directories in that order. Although we don’t teach it here, you can actually create executables yourself. However, if you put it in your working directory and this directory is not on the path, you can’t run it just by typing the command. You get around this by typing the full path. So if your command is called my-ls, you can type: ./my-ls Once you have mastered the basics of Unix, you should consider learning to write your own executables as they can help alleviate repetitive work. 81.8 Permissions and file types If you type: ls -l At the beginning, you will see a series of symbols like this -rw-r--r--. This string indicates the type of file: regular file -, directory d, or executable x. This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute if the file is executable? This is more advanced than what we cover here, but you can learn much more in a Unix reference book. 81.9 Commands you should learn There are many commands that we do not teach in this book, but we want to make you aware of them and what they do. They are: open/start - On the mac open filename tries to figure out the right application of the filename and open it with that application. This is a very useful command. On Git Bash, you can try start filename. Try opening an R or Rmd file with open or start: it should open them with RStudio. nano - A bare-bones editor. ln - create a symbolic link. We do not recommend its use, but you should be familiar with it. tar - archive files and subdirectories of a directory into one file. ssh - connect to another computer. grep - search for patterns in a file. awk/sed - These are two very powerful commands that permit you to find specific strings in files and change them. 81.10 File manipulation in R We can also perform file management from within R. The key functions to learn about can be seen by looking at the help file for ?files. Another useful function is unlink. Although not recommended, note that you can run Unix commands in R using system. "]
]
