<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Statistical models | Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.20.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Statistical models | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Statistical models | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2020-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference.html"/>
<link rel="next" href="regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i>Case studies</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who will find this book useful?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i>What does this book cover?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i>What is not covered by this book?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started with R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>1.2</b> The R console</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> Scripts</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>1.4.1</b> The panes</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>1.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>1.5</b> Installing R packages</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> R basics</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>2.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>2.2</b> The very basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>2.2.2</b> The workspace</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>2.2.3</b> Functions</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>2.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>2.2.5</b> Variable names</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>2.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>2.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>2.2.8</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>2.4</b> Data types</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> Data frames</a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>2.4.2</b> Examining an object</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>2.4.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>2.4.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>2.4.6</b> Lists</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectors</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>2.6.1</b> Creating vectors</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>2.6.2</b> Names</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>2.6.3</b> Sequences</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>2.6.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>2.7</b> Coercion</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>2.7.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> Sorting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>2.9.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>2.11</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>2.11.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>2.11.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>2.12</b> Exercises</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>2.13</b> Indexing</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>2.13.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>2.13.2</b> Logical operators</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>2.14</b> Exercises</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>2.15</b> Basic plots</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#exercises-6"><i class="fa fa-check"></i><b>2.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>3</b> Programming basics</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="3.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>3.2</b> Defining functions</a></li>
<li class="chapter" data-level="3.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> Namespaces</a></li>
<li class="chapter" data-level="3.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>3.4</b> For-loops</a></li>
<li class="chapter" data-level="3.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="3.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-7"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> The tidyverse</a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Tidy data</a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#exercises-8"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#manipulating-data-frames"><i class="fa fa-check"></i><b>4.3</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#subsetting-with-filter"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>4.3.3</b> Selecting columns with <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#exercises-9"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>4.5</b> The pipe: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#exercises-10"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#summarizing-data"><i class="fa fa-check"></i><b>4.7</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Group then summarize with <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#sorting-data-frames"><i class="fa fa-check"></i><b>4.8</b> Sorting data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#nested-sorting"><i class="fa fa-check"></i><b>4.8.1</b> Nested sorting</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#the-top-n"><i class="fa fa-check"></i><b>4.8.2</b> The top <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#exercises-11"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> Tibbles</a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>4.10.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>4.10.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>4.10.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#create-a-tibble-using-tibble-instead-of-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Create a tibble using <code>tibble</code> instead of <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>4.11</b> The dot operator</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>4.13</b> The <strong>purrr</strong> package</a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-conditionals"><i class="fa fa-check"></i><b>4.14</b> Tidyverse conditionals</a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#exercises-12"><i class="fa fa-check"></i><b>4.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importing data</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>5.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>5.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>5.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>5.1.3</b> The working directory</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>5.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>5.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>5.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#exercises-13"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>5.4</b> Downloading files</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>5.5</b> R-base importing functions</a></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>5.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>5.8</b> Organizing data with spreadsheets</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#exercises-14"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>6</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>7.1</b> The components of a graph</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects"><i class="fa fa-check"></i><b>7.2</b> <code>ggplot</code> objects</a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>7.3</b> Geometries</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>7.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>7.5</b> Layers</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>7.5.1</b> Tinkering with arguments</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>7.6</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>7.7</b> Scales</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>7.8</b> Labels and titles</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>7.9</b> Categories as colors</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>7.10</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Add-on packages</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>7.12</b> Putting it all together</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>7.14</b> Grids of plots</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#exercises-15"><i class="fa fa-check"></i><b>7.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Visualizing data distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>8.1</b> Variable types</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#case-study-describing-student-heights"><i class="fa fa-check"></i><b>8.2</b> Case study: describing student heights</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>8.3</b> Distribution function</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>8.5</b> Histograms</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>8.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>8.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>8.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#exercises-16"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> The normal distribution</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#standard-units"><i class="fa fa-check"></i><b>8.9</b> Standard units</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>8.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>8.12</b> Boxplots</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Stratification</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Case study: describing student heights (continued)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#exercises-17"><i class="fa fa-check"></i><b>8.15</b> Exercises</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#barplots"><i class="fa fa-check"></i><b>8.16.1</b> Barplots</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histograms-1"><i class="fa fa-check"></i><b>8.16.2</b> Histograms</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#density-plots"><i class="fa fa-check"></i><b>8.16.3</b> Density plots</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#boxplots-1"><i class="fa fa-check"></i><b>8.16.4</b> Boxplots</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#qq-plots"><i class="fa fa-check"></i><b>8.16.5</b> QQ-plots</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#images"><i class="fa fa-check"></i><b>8.16.6</b> Images</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#quick-plots"><i class="fa fa-check"></i><b>8.16.7</b> Quick plots</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#exercises-18"><i class="fa fa-check"></i><b>8.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#case-study-new-insights-on-poverty"><i class="fa fa-check"></i><b>9.1</b> Case study: new insights on poverty</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>9.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>9.2</b> Scatterplots</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>9.3</b> Faceting</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>9.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>9.4</b> Time series plots</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>9.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#data-transformations"><i class="fa fa-check"></i><b>9.5</b> Data transformations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>9.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>9.5.2</b> Which base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>9.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#visualizing-multimodal-distributions"><i class="fa fa-check"></i><b>9.6</b> Visualizing multimodal distributions</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#comparing-multiple-distributions-with-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>9.7</b> Comparing multiple distributions with boxplots and ridge plots</a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-2"><i class="fa fa-check"></i><b>9.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>9.7.2</b> Ridge plots</a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>9.7.3</b> Example: 1970 versus 2010 income distributions</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>9.7.4</b> Accessing computed variables</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>9.7.5</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#the-ecological-fallacy-and-importance-of-showing-the-data"><i class="fa fa-check"></i><b>9.8</b> The ecological fallacy and importance of showing the data</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>9.8.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>10</b> Data visualization principles</a><ul>
<li class="chapter" data-level="10.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>10.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="10.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>10.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="10.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>10.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="10.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-categories-by-a-meaningful-value"><i class="fa fa-check"></i><b>10.4</b> Order categories by a meaningful value</a></li>
<li class="chapter" data-level="10.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>10.5</b> Show the data</a></li>
<li class="chapter" data-level="10.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons"><i class="fa fa-check"></i><b>10.6</b> Ease comparisons</a><ul>
<li class="chapter" data-level="10.6.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-common-axes"><i class="fa fa-check"></i><b>10.6.1</b> Use common axes</a></li>
<li class="chapter" data-level="10.6.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>10.6.2</b> Align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="10.6.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>10.6.3</b> Consider transformations</a></li>
<li class="chapter" data-level="10.6.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>10.6.4</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="10.6.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>10.7</b> Think of the color blind</a></li>
<li class="chapter" data-level="10.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#plots-for-two-variables"><i class="fa fa-check"></i><b>10.8</b> Plots for two variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>10.8.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>10.9</b> Encoding a third variable</a></li>
<li class="chapter" data-level="10.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>10.10</b> Avoid pseudo-three-dimensional plots</a></li>
<li class="chapter" data-level="10.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>10.11</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="10.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>10.12</b> Know your audience</a></li>
<li class="chapter" data-level="10.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-19"><i class="fa fa-check"></i><b>10.13</b> Exercises</a></li>
<li class="chapter" data-level="10.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Case study: vaccines and infectious diseases</a></li>
<li class="chapter" data-level="10.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-20"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Robust summaries</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>11.1</b> Outliers</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>11.2</b> Median</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>11.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>11.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>11.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-21"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>11.7</b> Case study: self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Statistics with R</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>12</b> Introduction to statistics with R</a></li>
<li class="chapter" data-level="13" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>13</b> Probability</a><ul>
<li class="chapter" data-level="13.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>13.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>13.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="13.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>13.1.2</b> Notation</a></li>
<li class="chapter" data-level="13.1.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>13.1.3</b> Probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-categorical-data"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo simulations for categorical data</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>13.2.1</b> Setting the random seed</a></li>
<li class="chapter" data-level="13.2.2" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i><b>13.2.2</b> With and without replacement</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>13.3</b> Independence</a></li>
<li class="chapter" data-level="13.4" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>13.4</b> Conditional probabilities</a></li>
<li class="chapter" data-level="13.5" data-path="probability.html"><a href="probability.html#addition-and-multiplication-rules"><i class="fa fa-check"></i><b>13.5</b> Addition and multiplication rules</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>13.5.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="13.5.2" data-path="probability.html"><a href="probability.html#multiplication-rule-under-independence"><i class="fa fa-check"></i><b>13.5.2</b> Multiplication rule under independence</a></li>
<li class="chapter" data-level="13.5.3" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>13.5.3</b> Addition rule</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>13.6</b> Combinations and permutations</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>13.6.1</b> Monte Carlo example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probability.html"><a href="probability.html#examples"><i class="fa fa-check"></i><b>13.7</b> Examples</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probability.html"><a href="probability.html#monty-hall-problem"><i class="fa fa-check"></i><b>13.7.1</b> Monty Hall problem</a></li>
<li class="chapter" data-level="13.7.2" data-path="probability.html"><a href="probability.html#birthday-problem"><i class="fa fa-check"></i><b>13.7.2</b> Birthday problem</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probability.html"><a href="probability.html#infinity-in-practice"><i class="fa fa-check"></i><b>13.8</b> Infinity in practice</a></li>
<li class="chapter" data-level="13.9" data-path="probability.html"><a href="probability.html#exercises-22"><i class="fa fa-check"></i><b>13.9</b> Exercises</a></li>
<li class="chapter" data-level="13.10" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>13.10</b> Continuous probability</a></li>
<li class="chapter" data-level="13.11" data-path="probability.html"><a href="probability.html#theoretical-continuous-distributions"><i class="fa fa-check"></i><b>13.11</b> Theoretical continuous distributions</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>13.11.1</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="13.11.2" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>13.11.2</b> The probability density</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>13.12</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="13.13" data-path="probability.html"><a href="probability.html#continuous-distributions"><i class="fa fa-check"></i><b>13.13</b> Continuous distributions</a></li>
<li class="chapter" data-level="13.14" data-path="probability.html"><a href="probability.html#exercises-23"><i class="fa fa-check"></i><b>13.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>14</b> Random variables</a><ul>
<li class="chapter" data-level="14.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>14.1</b> Random variables</a></li>
<li class="chapter" data-level="14.2" data-path="random-variables.html"><a href="random-variables.html#sampling-models"><i class="fa fa-check"></i><b>14.2</b> Sampling models</a></li>
<li class="chapter" data-level="14.3" data-path="random-variables.html"><a href="random-variables.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>14.3</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="14.4" data-path="random-variables.html"><a href="random-variables.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>14.4</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="14.5" data-path="random-variables.html"><a href="random-variables.html#notation-for-random-variables"><i class="fa fa-check"></i><b>14.5</b> Notation for random variables</a></li>
<li class="chapter" data-level="14.6" data-path="random-variables.html"><a href="random-variables.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>14.6</b> The expected value and standard error</a><ul>
<li class="chapter" data-level="14.6.1" data-path="random-variables.html"><a href="random-variables.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>14.6.1</b> Population SD versus the sample SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="random-variables.html"><a href="random-variables.html#central-limit-theorem"><i class="fa fa-check"></i><b>14.7</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="14.7.1" data-path="random-variables.html"><a href="random-variables.html#how-large-is-large-in-the-central-limit-theorem"><i class="fa fa-check"></i><b>14.7.1</b> How large is large in the Central Limit Theorem?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="random-variables.html"><a href="random-variables.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>14.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="14.9" data-path="random-variables.html"><a href="random-variables.html#law-of-large-numbers"><i class="fa fa-check"></i><b>14.9</b> Law of large numbers</a><ul>
<li class="chapter" data-level="14.9.1" data-path="random-variables.html"><a href="random-variables.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>14.9.1</b> Misinterpreting law of averages</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="random-variables.html"><a href="random-variables.html#exercises-24"><i class="fa fa-check"></i><b>14.10</b> Exercises</a></li>
<li class="chapter" data-level="14.11" data-path="random-variables.html"><a href="random-variables.html#case-study-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="random-variables.html"><a href="random-variables.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>14.11.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="14.11.2" data-path="random-variables.html"><a href="random-variables.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="random-variables.html"><a href="random-variables.html#exercises-25"><i class="fa fa-check"></i><b>14.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Statistical inference</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#polls"><i class="fa fa-check"></i><b>15.1</b> Polls</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>15.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>15.2</b> Populations, samples, parameters, and estimates</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#the-sample-average"><i class="fa fa-check"></i><b>15.2.1</b> The sample average</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parameters"><i class="fa fa-check"></i><b>15.2.2</b> Parameters</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>15.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>15.2.4</b> Properties of our estimate: expected value and standard error</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#exercises-26"><i class="fa fa-check"></i><b>15.3</b> Exercises</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>15.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#the-spread"><i class="fa fa-check"></i><b>15.4.2</b> The spread</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>15.4.3</b> Bias: why not run a very large poll?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#exercises-27"><i class="fa fa-check"></i><b>15.5</b> Exercises</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>15.6</b> Confidence intervals</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>15.6.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#the-correct-language"><i class="fa fa-check"></i><b>15.6.2</b> The correct language</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#exercises-28"><i class="fa fa-check"></i><b>15.7</b> Exercises</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#power"><i class="fa fa-check"></i><b>15.8</b> Power</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>15.9</b> p-values</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Association tests</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>15.10.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#chi-square-test"><i class="fa fa-check"></i><b>15.10.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> The odds ratio</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>15.10.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#small-count-correction"><i class="fa fa-check"></i><b>15.10.6</b> Small count correction</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>15.10.7</b> Large samples, small p-values</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#exercises-29"><i class="fa fa-check"></i><b>15.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Statistical models</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#poll-aggregators"><i class="fa fa-check"></i><b>16.1</b> Poll aggregators</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#poll-data"><i class="fa fa-check"></i><b>16.1.1</b> Poll data</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#pollster-bias"><i class="fa fa-check"></i><b>16.1.2</b> Pollster bias</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Data-driven models</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#exercises-30"><i class="fa fa-check"></i><b>16.3</b> Exercises</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#bayes-theorem"><i class="fa fa-check"></i><b>16.4.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>16.5</b> Bayes theorem simulation</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-in-practice"><i class="fa fa-check"></i><b>16.5.1</b> Bayes in practice</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#hierarchical-models"><i class="fa fa-check"></i><b>16.6</b> Hierarchical models</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#exercises-31"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Case study: election forecasting</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#the-general-bias"><i class="fa fa-check"></i><b>16.8.2</b> The general bias</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>16.8.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>16.8.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#forecasting"><i class="fa fa-check"></i><b>16.8.5</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#exercises-32"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>17.1</b> Case study: is height hereditary?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>17.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>17.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Conditional expectations</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>17.4</b> The regression line</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>17.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>17.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>17.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>17.4.4</b> Warning: there are two regression lines</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#exercises-33"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>18</b> Linear models</a><ul>
<li class="chapter" data-level="18.1" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball"><i class="fa fa-check"></i><b>18.1</b> Case study: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="linear-models.html"><a href="linear-models.html#sabermetics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetics</a></li>
<li class="chapter" data-level="18.1.2" data-path="linear-models.html"><a href="linear-models.html#baseball-basics"><i class="fa fa-check"></i><b>18.1.2</b> Baseball basics</a></li>
<li class="chapter" data-level="18.1.3" data-path="linear-models.html"><a href="linear-models.html#no-awards-for-bb"><i class="fa fa-check"></i><b>18.1.3</b> No awards for BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="linear-models.html"><a href="linear-models.html#base-on-balls-or-stolen-bases"><i class="fa fa-check"></i><b>18.1.4</b> Base on balls or stolen bases?</a></li>
<li class="chapter" data-level="18.1.5" data-path="linear-models.html"><a href="linear-models.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>18.1.5</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="linear-models.html"><a href="linear-models.html#confounding"><i class="fa fa-check"></i><b>18.2</b> Confounding</a><ul>
<li class="chapter" data-level="18.2.1" data-path="linear-models.html"><a href="linear-models.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>18.2.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="18.2.2" data-path="linear-models.html"><a href="linear-models.html#multivariate-regression"><i class="fa fa-check"></i><b>18.2.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="linear-models.html"><a href="linear-models.html#lse"><i class="fa fa-check"></i><b>18.3</b> Least squares estimates</a><ul>
<li class="chapter" data-level="18.3.1" data-path="linear-models.html"><a href="linear-models.html#interpreting-linear-models"><i class="fa fa-check"></i><b>18.3.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="18.3.2" data-path="linear-models.html"><a href="linear-models.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>18.3.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="linear-models.html"><a href="linear-models.html#the-lm-function"><i class="fa fa-check"></i><b>18.3.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="18.3.4" data-path="linear-models.html"><a href="linear-models.html#lse-are-random-variables"><i class="fa fa-check"></i><b>18.3.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="18.3.5" data-path="linear-models.html"><a href="linear-models.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>18.3.5</b> Predicted values are random variables</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="linear-models.html"><a href="linear-models.html#exercises-34"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
<li class="chapter" data-level="18.5" data-path="linear-models.html"><a href="linear-models.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="linear-models.html"><a href="linear-models.html#the-broom-package"><i class="fa fa-check"></i><b>18.5.1</b> The broom package</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="linear-models.html"><a href="linear-models.html#exercises-35"><i class="fa fa-check"></i><b>18.6</b> Exercises</a></li>
<li class="chapter" data-level="18.7" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>18.7</b> Case study: Moneyball (continued)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="linear-models.html"><a href="linear-models.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>18.7.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="18.7.2" data-path="linear-models.html"><a href="linear-models.html#picking-nine-players"><i class="fa fa-check"></i><b>18.7.2</b> Picking nine players</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="linear-models.html"><a href="linear-models.html#the-regression-fallacy"><i class="fa fa-check"></i><b>18.8</b> The regression fallacy</a></li>
<li class="chapter" data-level="18.9" data-path="linear-models.html"><a href="linear-models.html#measurement-error-models"><i class="fa fa-check"></i><b>18.9</b> Measurement error models</a></li>
<li class="chapter" data-level="18.10" data-path="linear-models.html"><a href="linear-models.html#exercises-36"><i class="fa fa-check"></i><b>18.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>19</b> Association is not causation</a><ul>
<li class="chapter" data-level="19.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>19.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="19.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>19.2</b> Outliers</a></li>
<li class="chapter" data-level="19.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>19.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="19.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>19.4</b> Confounders</a><ul>
<li class="chapter" data-level="19.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>19.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="19.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>19.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="19.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>19.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>19.5</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="19.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-37"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="20" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>20</b> Introduction to data wrangling</a></li>
<li class="chapter" data-level="21" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>21</b> Reshaping data</a><ul>
<li class="chapter" data-level="21.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-38"><i class="fa fa-check"></i><b>21.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>22</b> Joining tables</a><ul>
<li class="chapter" data-level="22.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>22.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="22.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>22.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>22.3</b> Set operators</a><ul>
<li class="chapter" data-level="22.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>22.3.1</b> Intersect</a></li>
<li class="chapter" data-level="22.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>22.3.2</b> Union</a></li>
<li class="chapter" data-level="22.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-39"><i class="fa fa-check"></i><b>22.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>23</b> Web scraping</a><ul>
<li class="chapter" data-level="23.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>23.2</b> The rvest package</a></li>
<li class="chapter" data-level="23.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> CSS selectors</a></li>
<li class="chapter" data-level="23.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-40"><i class="fa fa-check"></i><b>23.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>24</b> String processing</a><ul>
<li class="chapter" data-level="24.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>24.1</b> The stringr package</a></li>
<li class="chapter" data-level="24.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>24.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="24.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>24.3</b> Case study 2: self-reported heights</a></li>
<li class="chapter" data-level="24.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>24.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="24.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>24.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="24.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>24.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="24.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>24.5.2</b> Special characters</a></li>
<li class="chapter" data-level="24.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>24.5.3</b> Character classes</a></li>
<li class="chapter" data-level="24.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>24.5.4</b> Anchors</a></li>
<li class="chapter" data-level="24.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>24.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="24.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>24.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>24.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>24.5.8</b> Not</a></li>
<li class="chapter" data-level="24.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>24.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="24.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>24.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>24.7</b> Testing and improving</a></li>
<li class="chapter" data-level="24.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>24.8</b> Trimming</a></li>
<li class="chapter" data-level="24.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>24.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="24.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>24.10</b> Case study 2: self-reported heights (continued)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>24.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="24.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>24.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>24.11</b> String splitting</a></li>
<li class="chapter" data-level="24.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>24.12</b> Case study 3: extracting tables from a PDF</a></li>
<li class="chapter" data-level="24.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recoding</a></li>
<li class="chapter" data-level="24.14" data-path="string-processing.html"><a href="string-processing.html#exercises-41"><i class="fa fa-check"></i><b>24.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>25</b> Parsing dates and times</a><ul>
<li class="chapter" data-level="25.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>25.1</b> The date data type</a></li>
<li class="chapter" data-level="25.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> The lubridate package</a></li>
<li class="chapter" data-level="25.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-42"><i class="fa fa-check"></i><b>25.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>26</b> Text mining</a><ul>
<li class="chapter" data-level="26.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>26.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="26.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>26.2</b> Text as data</a></li>
<li class="chapter" data-level="26.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>26.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="26.4" data-path="text-mining.html"><a href="text-mining.html#exercises-43"><i class="fa fa-check"></i><b>26.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introduction to machine learning</a><ul>
<li class="chapter" data-level="27.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>27.1</b> Notation</a></li>
<li class="chapter" data-level="27.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>27.2</b> An example</a></li>
<li class="chapter" data-level="27.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-44"><i class="fa fa-check"></i><b>27.3</b> Exercises</a></li>
<li class="chapter" data-level="27.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>27.4</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>27.4.1</b> Training and test sets</a></li>
<li class="chapter" data-level="27.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>27.4.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="27.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>27.4.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="27.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>27.4.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="27.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>27.4.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="27.4.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>27.4.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="27.4.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>27.4.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="27.4.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-45"><i class="fa fa-check"></i><b>27.5</b> Exercises</a></li>
<li class="chapter" data-level="27.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>27.6</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>27.6.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="27.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>27.6.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="27.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>27.6.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-46"><i class="fa fa-check"></i><b>27.7</b> Exercises</a></li>
<li class="chapter" data-level="27.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Case study: is it a 2 or a 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>28</b> Smoothing</a><ul>
<li class="chapter" data-level="28.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>28.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="28.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>28.3</b> Local weighted regression (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>28.3.1</b> Fitting parabolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="smoothing.html"><a href="smoothing.html#beware-of-default-smoothing-parameters"><i class="fa fa-check"></i><b>28.3.2</b> Beware of default smoothing parameters</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="smoothing.html"><a href="smoothing.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Connecting smoothing to machine learning</a></li>
<li class="chapter" data-level="28.5" data-path="smoothing.html"><a href="smoothing.html#exercises-47"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Cross validation</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#over-training"><i class="fa fa-check"></i><b>29.1.1</b> Over-training</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#over-smoothing"><i class="fa fa-check"></i><b>29.1.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>29.1.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>29.2</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>29.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-48"><i class="fa fa-check"></i><b>29.4</b> Exercises</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#exercises-49"><i class="fa fa-check"></i><b>29.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> The caret package</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>30.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Cross validation</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>30.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>31</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="31.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>31.1</b> Linear regression</a><ul>
<li class="chapter" data-level="31.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>31.1.1</b> The <code>predict</code> function</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-50"><i class="fa fa-check"></i><b>31.2</b> Exercises</a></li>
<li class="chapter" data-level="31.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>31.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="31.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>31.3.1</b> Generalized linear models</a></li>
<li class="chapter" data-level="31.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression-with-more-than-one-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Logistic regression with more than one predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-51"><i class="fa fa-check"></i><b>31.4</b> Exercises</a></li>
<li class="chapter" data-level="31.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="31.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-52"><i class="fa fa-check"></i><b>31.6</b> Exercises</a></li>
<li class="chapter" data-level="31.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>31.7</b> Generative models</a><ul>
<li class="chapter" data-level="31.7.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>31.7.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="31.7.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.3</b> Quadratic discriminant analysis</a></li>
<li class="chapter" data-level="31.7.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="31.7.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>31.7.5</b> Connection to distance</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>31.8</b> Case study: more than three classes</a></li>
<li class="chapter" data-level="31.9" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-53"><i class="fa fa-check"></i><b>31.9</b> Exercises</a></li>
<li class="chapter" data-level="31.10" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>31.10</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>31.10.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="31.10.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>31.10.2</b> CART motivation</a></li>
<li class="chapter" data-level="31.10.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>31.10.3</b> Regression trees</a></li>
<li class="chapter" data-level="31.10.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>31.10.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>31.11</b> Random forests</a></li>
<li class="chapter" data-level="31.12" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-54"><i class="fa fa-check"></i><b>31.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>32</b> Machine learning in practice</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>32.1</b> Preprocessing</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest"><i class="fa fa-check"></i><b>32.2</b> k-nearest neighbor and random forest</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>32.3</b> Variable importance</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>32.4</b> Visual assessments</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>32.5</b> Ensembles</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-55"><i class="fa fa-check"></i><b>32.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>33</b> Large datasets</a><ul>
<li class="chapter" data-level="33.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="33.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>33.1.1</b> Notation</a></li>
<li class="chapter" data-level="33.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>33.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="33.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>33.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="33.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>33.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="33.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>33.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="33.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>33.1.9</b> Matrix algebra operations</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>33.2</b> Exercises</a></li>
<li class="chapter" data-level="33.3" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>33.3</b> Distance</a><ul>
<li class="chapter" data-level="33.3.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>33.3.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="33.3.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>33.3.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="33.3.3" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance-example"><i class="fa fa-check"></i><b>33.3.3</b> Euclidean distance example</a></li>
<li class="chapter" data-level="33.3.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Predictor space</a></li>
<li class="chapter" data-level="33.3.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>33.3.5</b> Distance between predictors</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>33.4</b> Exercises</a></li>
<li class="chapter" data-level="33.5" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>33.5</b> Dimension reduction</a><ul>
<li class="chapter" data-level="33.5.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>33.5.1</b> Preserving distance</a></li>
<li class="chapter" data-level="33.5.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advanced"><i class="fa fa-check"></i><b>33.5.2</b> Linear transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogonal-transformations-advanced"><i class="fa fa-check"></i><b>33.5.3</b> Orthogonal transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>33.5.4</b> Principal component analysis</a></li>
<li class="chapter" data-level="33.5.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>33.5.5</b> Iris example</a></li>
<li class="chapter" data-level="33.5.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>33.5.6</b> MNIST example</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>33.6</b> Exercises</a></li>
<li class="chapter" data-level="33.7" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>33.7</b> Recommendation systems</a><ul>
<li class="chapter" data-level="33.7.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>33.7.1</b> Movielens data</a></li>
<li class="chapter" data-level="33.7.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>33.7.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="33.7.3" data-path="large-datasets.html"><a href="large-datasets.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Loss function</a></li>
<li class="chapter" data-level="33.7.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>33.7.4</b> A first model</a></li>
<li class="chapter" data-level="33.7.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>33.7.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="33.7.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>33.7.6</b> User effects</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="large-datasets.html"><a href="large-datasets.html#exercises-59"><i class="fa fa-check"></i><b>33.8</b> Exercises</a></li>
<li class="chapter" data-level="33.9" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>33.9</b> Regularization</a><ul>
<li class="chapter" data-level="33.9.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>33.9.1</b> Motivation</a></li>
<li class="chapter" data-level="33.9.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>33.9.2</b> Penalized least squares</a></li>
<li class="chapter" data-level="33.9.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>33.9.3</b> Choosing the penalty terms</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-60"><i class="fa fa-check"></i><b>33.10</b> Exercises</a></li>
<li class="chapter" data-level="33.11" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>33.11</b> Matrix factorization</a><ul>
<li class="chapter" data-level="33.11.1" data-path="large-datasets.html"><a href="large-datasets.html#factors-analysis"><i class="fa fa-check"></i><b>33.11.1</b> Factors analysis</a></li>
<li class="chapter" data-level="33.11.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>33.11.2</b> Connection to SVD and PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="large-datasets.html"><a href="large-datasets.html#exercises-61"><i class="fa fa-check"></i><b>33.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Clustering</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>34.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>34.3</b> Heatmaps</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>34.4</b> Filtering features</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#exercises-62"><i class="fa fa-check"></i><b>34.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity Tools</b></span></li>
<li class="chapter" data-level="35" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>35</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>36.1</b> Installing R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>36.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html"><i class="fa fa-check"></i><b>37</b> Accessing the terminal and installing Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>37.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>37.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#naming-convention"><i class="fa fa-check"></i><b>38.1</b> Naming convention</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> The terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> The filesystem</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>38.3.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>38.3.2</b> The home directory</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Working directory</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>38.4</b> Unix commands</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>38.5</b> Some examples</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>38.6</b> More Unix commands</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>38.8</b> Advanced Unix</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>38.8.1</b> Arguments</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>38.8.2</b> Getting help</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>38.8.3</b> Pipes</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>38.8.4</b> Wild cards</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>38.8.5</b> Environment variables</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> Shells</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>38.8.7</b> Executables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>38.8.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>38.8.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>38.8.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git and GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>39.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#github-accounts"><i class="fa fa-check"></i><b>39.2</b> GitHub accounts</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> GitHub repositories</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Overview of Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>39.4.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Reproducible projects with RStudio and R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#rstudio-projects"><i class="fa fa-check"></i><b>40.1</b> RStudio projects</a></li>
<li class="chapter" data-level="40.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#the-header"><i class="fa fa-check"></i><b>40.2.1</b> The header</a></li>
<li class="chapter" data-level="40.2.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>40.2.2</b> R code chunks</a></li>
<li class="chapter" data-level="40.2.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#global-options"><i class="fa fa-check"></i><b>40.2.3</b> Global options</a></li>
<li class="chapter" data-level="40.2.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> More on R markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizing a data science project</a><ul>
<li class="chapter" data-level="40.3.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-directories-in-unix"><i class="fa fa-check"></i><b>40.3.1</b> Create directories in Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-an-rstudio-project"><i class="fa fa-check"></i><b>40.3.2</b> Create an RStudio project</a></li>
<li class="chapter" data-level="40.3.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#edit-some-r-scripts"><i class="fa fa-check"></i><b>40.3.3</b> Edit some R scripts</a></li>
<li class="chapter" data-level="40.3.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-some-more-directories-using-unix"><i class="fa fa-check"></i><b>40.3.4</b> Create some more directories using Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-a-readme-file"><i class="fa fa-check"></i><b>40.3.5</b> Add a README file</a></li>
<li class="chapter" data-level="40.3.6" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#initializing-a-git-directory"><i class="fa fa-check"></i><b>40.3.6</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="40.3.7" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-commit-and-push-files-using-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Add, commit, and push files using RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="models" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Statistical models</h1>
<blockquote>
<blockquote>
<p>“All models are wrong, but some are useful.” –George E. P. Box</p>
</blockquote>
</blockquote>
<p>The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>:</p>
<blockquote>
<blockquote>
<p>Anybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.</p>
</blockquote>
</blockquote>
<p>To which Nate Silver responded via Twitter:</p>
<blockquote>
<blockquote>
<p>If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?</p>
</blockquote>
</blockquote>
<p>In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?</p>
<p>In this chapter we will demonstrate how <em>poll aggregators</em>, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the <em>statistical models</em>, also known as <em>probability models</em>, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. In this chapter, we motivate the models, building on the statistical inference concepts we learned in Chapter <a href="inference.html#inference">15</a>. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones, which we introduce towards the end of the chapter in Section <a href="models.html#election-forecasting">16.8</a>.</p>
<div id="poll-aggregators" class="section level2">
<h2><span class="header-section-number">16.1</span> Poll aggregators</h2>
<p>As we described earlier, a few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="models.html#cb583-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb583-2"><a href="models.html#cb583-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb583-3"><a href="models.html#cb583-3"></a>d &lt;-<span class="st"> </span><span class="fl">0.039</span></span>
<span id="cb583-4"><a href="models.html#cb583-4"></a>Ns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1298</span>, <span class="dv">533</span>, <span class="dv">1342</span>, <span class="dv">897</span>, <span class="dv">774</span>, <span class="dv">254</span>, <span class="dv">812</span>, <span class="dv">324</span>, <span class="dv">1291</span>, <span class="dv">1056</span>, <span class="dv">2172</span>, <span class="dv">516</span>)</span>
<span id="cb583-5"><a href="models.html#cb583-5"></a>p &lt;-<span class="st"> </span>(d <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb583-6"><a href="models.html#cb583-6"></a></span>
<span id="cb583-7"><a href="models.html#cb583-7"></a>polls &lt;-<span class="st"> </span><span class="kw">map_df</span>(Ns, <span class="cf">function</span>(N) {</span>
<span id="cb583-8"><a href="models.html#cb583-8"></a>  x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>p, p))</span>
<span id="cb583-9"><a href="models.html#cb583-9"></a>  x_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(x)</span>
<span id="cb583-10"><a href="models.html#cb583-10"></a>  se_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(x_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_hat) <span class="op">/</span><span class="st"> </span>N)</span>
<span id="cb583-11"><a href="models.html#cb583-11"></a>  <span class="kw">list</span>(<span class="dt">estimate =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x_hat <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb583-12"><a href="models.html#cb583-12"></a>    <span class="dt">low =</span> <span class="dv">2</span><span class="op">*</span>(x_hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>se_hat) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb583-13"><a href="models.html#cb583-13"></a>    <span class="dt">high =</span> <span class="dv">2</span><span class="op">*</span>(x_hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>se_hat) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb583-14"><a href="models.html#cb583-14"></a>    <span class="dt">sample_size =</span> N)</span>
<span id="cb583-15"><a href="models.html#cb583-15"></a>}) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">poll =</span> <span class="kw">seq_along</span>(Ns))</span></code></pre></div>
<p>Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:</p>
<p><img src="book_files/figure-html/simulated-polls-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.</p>
<p>Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.</p>
<p>Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:</p>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="models.html#cb584-1"></a><span class="kw">sum</span>(polls<span class="op">$</span>sample_size)</span>
<span id="cb584-2"><a href="models.html#cb584-2"></a><span class="co">#&gt; [1] 11269</span></span></code></pre></div>
<p>participants. Basically, we construct an estimate of the spread, let’s call it <span class="math inline">\(d\)</span>, with a weighted average in the following way:</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="models.html#cb585-1"></a>d_hat &lt;-<span class="st"> </span>polls <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb585-2"><a href="models.html#cb585-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">sum</span>(estimate<span class="op">*</span>sample_size) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(sample_size)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb585-3"><a href="models.html#cb585-3"></a><span class="st">  </span><span class="kw">pull</span>(avg)</span></code></pre></div>
<p>Once we have an estimate of <span class="math inline">\(d\)</span>, we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.018.</p>
<p>Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.</p>
<p><img src="book_files/figure-html/confidence-coverage-2008-election-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.</p>
<p>Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the <em>New York Times</em> reported<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a> the following probabilities for Hillary Clinton winning the presidency:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
NYT
</th>
<th style="text-align:left;">
538
</th>
<th style="text-align:left;">
HuffPost
</th>
<th style="text-align:left;">
PW
</th>
<th style="text-align:left;">
PEC
</th>
<th style="text-align:left;">
DK
</th>
<th style="text-align:left;">
Cook
</th>
<th style="text-align:left;">
Roth
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Win Prob
</td>
<td style="text-align:left;">
85%
</td>
<td style="text-align:left;">
71%
</td>
<td style="text-align:left;">
98%
</td>
<td style="text-align:left;">
89%
</td>
<td style="text-align:left;">
&gt;99%
</td>
<td style="text-align:left;">
92%
</td>
<td style="text-align:left;">
Lean Dem
</td>
<td style="text-align:left;">
Lean Dem
</td>
</tr>
</tbody>
</table>
<!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))-->
<p>For example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled <em>Trump Is Just A Normal Polling Error Behind Clinton</em><a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>.
By understanding statistical models and how these forecasters use them, we will start to understand how this happened.</p>
<p>Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:</p>
<p><img src="book_files/figure-html/fivethirtyeight-densities-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model.
<!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))--></p>
<p>We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in Sections <a href="models.html#bayesian-statistics">16.4</a> and <a href="models.html#bayesian-approach">16.8.1</a>.</p>
<div id="poll-data" class="section level3">
<h3><span class="header-section-number">16.1.1</span> Poll data</h3>
<p>We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the <strong>dslabs</strong> package:</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="models.html#cb586-1"></a><span class="kw">data</span>(polls_us_election_<span class="dv">2016</span>)</span></code></pre></div>
<p>The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="models.html#cb587-1"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb587-2"><a href="models.html#cb587-2"></a><span class="st">  </span><span class="kw">filter</span>(state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span> <span class="op">&amp;</span><span class="st"> </span>enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-31&quot;</span> <span class="op">&amp;</span></span>
<span id="cb587-3"><a href="models.html#cb587-3"></a><span class="st">           </span>(grade <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A+&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;A-&quot;</span>,<span class="st">&quot;B+&quot;</span>) <span class="op">|</span><span class="st"> </span><span class="kw">is.na</span>(grade)))</span></code></pre></div>
<p>We add a spread estimate:</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="models.html#cb588-1"></a>polls &lt;-<span class="st"> </span>polls <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb588-2"><a href="models.html#cb588-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>)</span></code></pre></div>
<p>For this example, we will assume that there are only two parties and call <span class="math inline">\(p\)</span> the proportion voting for Clinton and <span class="math inline">\(1-p\)</span> the proportion voting for Trump. We are interested in the spread <span class="math inline">\(2p-1\)</span>. Let’s call the spread <span class="math inline">\(d\)</span> (for difference).</p>
<p>We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread <span class="math inline">\(d\)</span> and the standard error is <span class="math inline">\(2\sqrt{p (1 - p) / N}\)</span>. Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:</p>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="models.html#cb589-1"></a>d_hat &lt;-<span class="st"> </span>polls <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb589-2"><a href="models.html#cb589-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">d_hat =</span> <span class="kw">sum</span>(spread <span class="op">*</span><span class="st"> </span>samplesize) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(samplesize)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb589-3"><a href="models.html#cb589-3"></a><span class="st">  </span><span class="kw">pull</span>(d_hat)</span></code></pre></div>
<p>and the standard error is:</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb590-1"><a href="models.html#cb590-1"></a>p_hat &lt;-<span class="st"> </span>(d_hat<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span> </span>
<span id="cb590-2"><a href="models.html#cb590-2"></a>moe &lt;-<span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(p_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_hat) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(polls<span class="op">$</span>samplesize))</span>
<span id="cb590-3"><a href="models.html#cb590-3"></a>moe</span>
<span id="cb590-4"><a href="models.html#cb590-4"></a><span class="co">#&gt; [1] 0.00662</span></span></code></pre></div>
<p>So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?</p>
<p>A histogram of the reported spreads shows a problem:</p>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="models.html#cb591-1"></a>polls <span class="op">%&gt;%</span></span>
<span id="cb591-2"><a href="models.html#cb591-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(spread)) <span class="op">+</span></span>
<span id="cb591-3"><a href="models.html#cb591-3"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;black&quot;</span>, <span class="dt">binwidth =</span> <span class="fl">.01</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/polls-2016-spread-histogram-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The data does not appear to be normally distributed and the standard error appears to be larger than 0.007. The theory is not quite working here.</p>
</div>
<div id="pollster-bias" class="section level3">
<h3><span class="header-section-number">16.1.2</span> Pollster bias</h3>
<p>Notice that various pollsters are involved and some are taking several polls a week:</p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="models.html#cb592-1"></a>polls <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(pollster) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="kw">n</span>())</span>
<span id="cb592-2"><a href="models.html#cb592-2"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb592-3"><a href="models.html#cb592-3"></a><span class="co">#&gt; # A tibble: 15 x 2</span></span>
<span id="cb592-4"><a href="models.html#cb592-4"></a><span class="co">#&gt;   pollster                                                   `n()`</span></span>
<span id="cb592-5"><a href="models.html#cb592-5"></a><span class="co">#&gt;   &lt;fct&gt;                                                      &lt;int&gt;</span></span>
<span id="cb592-6"><a href="models.html#cb592-6"></a><span class="co">#&gt; 1 ABC News/Washington Post                                       7</span></span>
<span id="cb592-7"><a href="models.html#cb592-7"></a><span class="co">#&gt; 2 Angus Reid Global                                              1</span></span>
<span id="cb592-8"><a href="models.html#cb592-8"></a><span class="co">#&gt; 3 CBS News/New York Times                                        2</span></span>
<span id="cb592-9"><a href="models.html#cb592-9"></a><span class="co">#&gt; 4 Fox News/Anderson Robbins Research/Shaw &amp; Company Research     2</span></span>
<span id="cb592-10"><a href="models.html#cb592-10"></a><span class="co">#&gt; 5 IBD/TIPP                                                       8</span></span>
<span id="cb592-11"><a href="models.html#cb592-11"></a><span class="co">#&gt; # … with 10 more rows</span></span></code></pre></div>
<p>Let’s visualize the data for the pollsters that are regularly polling:</p>
<p><img src="book_files/figure-html/pollster-bias-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="models.html#cb593-1"></a>polls <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(pollster) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb593-2"><a href="models.html#cb593-2"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">6</span>) <span class="op">%&gt;%</span></span>
<span id="cb593-3"><a href="models.html#cb593-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">se =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(p_hat <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>p_hat) <span class="op">/</span><span class="st"> </span><span class="kw">median</span>(samplesize)))</span>
<span id="cb593-4"><a href="models.html#cb593-4"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb593-5"><a href="models.html#cb593-5"></a><span class="co">#&gt; # A tibble: 5 x 2</span></span>
<span id="cb593-6"><a href="models.html#cb593-6"></a><span class="co">#&gt;   pollster                     se</span></span>
<span id="cb593-7"><a href="models.html#cb593-7"></a><span class="co">#&gt;   &lt;fct&gt;                     &lt;dbl&gt;</span></span>
<span id="cb593-8"><a href="models.html#cb593-8"></a><span class="co">#&gt; 1 ABC News/Washington Post 0.0265</span></span>
<span id="cb593-9"><a href="models.html#cb593-9"></a><span class="co">#&gt; 2 IBD/TIPP                 0.0333</span></span>
<span id="cb593-10"><a href="models.html#cb593-10"></a><span class="co">#&gt; 3 Ipsos                    0.0225</span></span>
<span id="cb593-11"><a href="models.html#cb593-11"></a><span class="co">#&gt; 4 The Times-Picayune/Lucid 0.0196</span></span>
<span id="cb593-12"><a href="models.html#cb593-12"></a><span class="co">#&gt; 5 USC Dornsife/LA Times    0.0183</span></span></code></pre></div>
<p>is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences <em>across the polls</em>. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them <em>pollster bias</em>.</p>
<p>In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.</p>
</div>
</div>
<div id="data-driven-model" class="section level2">
<h2><span class="header-section-number">16.2</span> Data-driven models</h2>
<p>For each pollster, let’s collect their last reported result before the election:</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="models.html#cb594-1"></a>one_poll_per_pollster &lt;-<span class="st"> </span>polls <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(pollster) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb594-2"><a href="models.html#cb594-2"></a><span class="st">  </span><span class="kw">filter</span>(enddate <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(enddate)) <span class="op">%&gt;%</span></span>
<span id="cb594-3"><a href="models.html#cb594-3"></a><span class="st">  </span><span class="kw">ungroup</span>()</span></code></pre></div>
<p>Here is a histogram of the data for these 15 pollsters:</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="models.html#cb595-1"></a><span class="kw">qplot</span>(spread, <span class="dt">data =</span> one_poll_per_pollster, <span class="dt">binwidth =</span> <span class="fl">0.01</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/pollster-bias-histogram-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.</p>
<p>The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We <em>assume</em> that the expected value of our urn is the actual spread <span class="math inline">\(d=2p-1\)</span>.</p>
<p>Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer <span class="math inline">\(\sqrt{p(1-p)}\)</span>. Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol <span class="math inline">\(\sigma\)</span> is used to represent this parameter.</p>
<p>In summary, we have two unknown parameters: the expected value <span class="math inline">\(d\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Our task is to estimate <span class="math inline">\(d\)</span>. Because we model the observed values <span class="math inline">\(X_1,\dots X_N\)</span> as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size <span class="math inline">\(N\)</span>, the probability distribution of the sample average <span class="math inline">\(\bar{X}\)</span> is approximately normal with expected value <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma/\sqrt{N}\)</span>. If we are willing to consider <span class="math inline">\(N=15\)</span> large enough, we can use this to construct confidence intervals.</p>
<p>A problem is that we don’t know <span class="math inline">\(\sigma\)</span>. But theory tells us that we can estimate the urn model <span class="math inline">\(\sigma\)</span> with the <em>sample standard deviation</em> defined as
<span class="math inline">\(s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2 / (N-1)}\)</span>.</p>
<p>Unlike for the population standard deviation definition, we now divide by <span class="math inline">\(N-1\)</span>. This makes <span class="math inline">\(s\)</span> a better estimate of <span class="math inline">\(\sigma\)</span>. There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.</p>
<p>The <code>sd</code> function in R computes the sample standard deviation:</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="models.html#cb596-1"></a><span class="kw">sd</span>(one_poll_per_pollster<span class="op">$</span>spread)</span>
<span id="cb596-2"><a href="models.html#cb596-2"></a><span class="co">#&gt; [1] 0.0242</span></span></code></pre></div>
<p>We are now ready to form a new confidence interval based on our new data-driven model:</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="models.html#cb597-1"></a>results &lt;-<span class="st"> </span>one_poll_per_pollster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb597-2"><a href="models.html#cb597-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(spread), </span>
<span id="cb597-3"><a href="models.html#cb597-3"></a>            <span class="dt">se =</span> <span class="kw">sd</span>(spread) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">length</span>(spread))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb597-4"><a href="models.html#cb597-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">start =</span> avg <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se, </span>
<span id="cb597-5"><a href="models.html#cb597-5"></a>         <span class="dt">end =</span> avg <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se) </span>
<span id="cb597-6"><a href="models.html#cb597-6"></a><span class="kw">round</span>(results <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb597-7"><a href="models.html#cb597-7"></a><span class="co">#&gt;   avg  se start end</span></span>
<span id="cb597-8"><a href="models.html#cb597-8"></a><span class="co">#&gt; 1 2.9 0.6   1.7 4.1</span></span></code></pre></div>
<p>Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.</p>
<p>Are we now ready to declare a probability of Clinton winning the popular vote? Not yet. In our model <span class="math inline">\(d\)</span> is a fixed parameter so we can’t talk about probabilities. To provide probabilities, we will need to learn about Bayesian statistics.</p>
</div>
<div id="exercises-30" class="section level2">
<h2><span class="header-section-number">16.3</span> Exercises</h2>
<p>We have been using urn models to motivate the use of probability models. Most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.</p>
<p>Let’s revisit the heights dataset. Suppose we consider the males in our course the population.</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="models.html#cb598-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb598-2"><a href="models.html#cb598-2"></a><span class="kw">data</span>(heights)</span>
<span id="cb598-3"><a href="models.html#cb598-3"></a>x &lt;-<span class="st"> </span>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(sex <span class="op">==</span><span class="st"> &quot;Male&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb598-4"><a href="models.html#cb598-4"></a><span class="st">  </span><span class="kw">pull</span>(height)</span></code></pre></div>
<p>1. Mathematically speaking, <code>x</code> is our population. Using the urn analogy, we have an urn with the values of <code>x</code> in it. What are the average and standard deviation of our population?</p>
<p>2. Call the population average computed above <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. Now take a sample of size 50, with replacement, and construct an estimate for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p>3. What does the theory tell us about the sample average <span class="math inline">\(\bar{X}\)</span> and how it is related to <span class="math inline">\(\mu\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li>It is practically identical to <span class="math inline">\(\mu\)</span>.</li>
<li>It is a random variable with expected value <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma/\sqrt{N}\)</span>.</li>
<li>It is a random variable with expected value <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma\)</span>.</li>
<li>Contains no information.</li>
</ol>
<p>4. So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use <span class="math inline">\(\bar{X}\)</span> as our estimate. We know from the answer to exercise 3 that the standard estimate of our error <span class="math inline">\(\bar{X}-\mu\)</span> is <span class="math inline">\(\sigma/\sqrt{N}\)</span>. We want to compute this, but we don’t know <span class="math inline">\(\sigma\)</span>. Based on what is described in this section, show your estimate of <span class="math inline">\(\sigma\)</span>.</p>
<p>5. Now that we have an estimate of <span class="math inline">\(\sigma\)</span>, let’s call our estimate <span class="math inline">\(s\)</span>. Construct a 95% confidence interval for <span class="math inline">\(\mu\)</span>.</p>
<p>6. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include <span class="math inline">\(\mu\)</span>?</p>
<p>7. In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="models.html#cb599-1"></a><span class="kw">data</span>(polls_us_election_<span class="dv">2016</span>)</span>
<span id="cb599-2"><a href="models.html#cb599-2"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb599-3"><a href="models.html#cb599-3"></a><span class="st">  </span><span class="kw">filter</span>(pollster <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Rasmussen Reports/Pulse Opinion Research&quot;</span>,</span>
<span id="cb599-4"><a href="models.html#cb599-4"></a>                         <span class="st">&quot;The Times-Picayune/Lucid&quot;</span>) <span class="op">&amp;</span></span>
<span id="cb599-5"><a href="models.html#cb599-5"></a><span class="st">           </span>enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-15&quot;</span> <span class="op">&amp;</span></span>
<span id="cb599-6"><a href="models.html#cb599-6"></a><span class="st">           </span>state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb599-7"><a href="models.html#cb599-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>) </span></code></pre></div>
<p>We want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll.</p>
<p>8. The data does seem to suggest there is a difference. However, these
data are subject to variability. Perhaps the differences we observe are due to chance.</p>
<p>The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call <span class="math inline">\(d\)</span>.</p>
<p>To answer the question “is there an urn model?”, we will model the observed data <span class="math inline">\(Y_{i,j}\)</span> in the following way:</p>
<p><span class="math display">\[
Y_{i,j} = d + b_i + \varepsilon_{i,j}
\]</span></p>
<p>with <span class="math inline">\(i=1,2\)</span> indexing the two pollsters, <span class="math inline">\(b_i\)</span> the bias for pollster <span class="math inline">\(i\)</span> and <span class="math inline">\(\varepsilon_ij\)</span> poll to poll chance variability. We assume the <span class="math inline">\(\varepsilon\)</span> are independent from each other, have expected value <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(\sigma_i\)</span> regardless of <span class="math inline">\(j\)</span>.</p>
<p>Which of the following best represents our question?</p>
<ol style="list-style-type: lower-alpha">
<li>Is <span class="math inline">\(\varepsilon_{i,j}\)</span> = 0?</li>
<li>How close are the <span class="math inline">\(Y_{i,j}\)</span> to <span class="math inline">\(d\)</span>?</li>
<li>Is <span class="math inline">\(b_1 \neq b_2\)</span>?</li>
<li>Are <span class="math inline">\(b_1 = 0\)</span> and <span class="math inline">\(b_2 = 0\)</span> ?</li>
</ol>
<p>9. In the right side of this model only <span class="math inline">\(\varepsilon_{i,j}\)</span> is a random variable. The other two are constants. What is the expected value of <span class="math inline">\(Y_{1,j}\)</span>?</p>
<p>10. Suppose we define <span class="math inline">\(\bar{Y}_1\)</span> as the average of poll results from the first poll, <span class="math inline">\(Y_{1,1},\dots,Y_{1,N_1}\)</span> with <span class="math inline">\(N_1\)</span> the number of polls conducted by the first pollster:</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="models.html#cb600-1"></a>polls <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb600-2"><a href="models.html#cb600-2"></a><span class="st">  </span><span class="kw">filter</span>(pollster<span class="op">==</span><span class="st">&quot;Rasmussen Reports/Pulse Opinion Research&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb600-3"><a href="models.html#cb600-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">N_1 =</span> <span class="kw">n</span>())</span></code></pre></div>
<p>What is the expected values <span class="math inline">\(\bar{Y}_1\)</span>?</p>
<p>11. What is the standard error of <span class="math inline">\(\bar{Y}_1\)</span> ?</p>
<p>12. Suppose we define <span class="math inline">\(\bar{Y}_2\)</span> as the average of poll results from the first poll, <span class="math inline">\(Y_{2,1},\dots,Y_{2,N_2}\)</span> with <span class="math inline">\(N_2\)</span> the number of polls conducted by the first pollster. What is the expected value <span class="math inline">\(\bar{Y}_2\)</span>?</p>
<p>13. What is the standard error of <span class="math inline">\(\bar{Y}_2\)</span> ?</p>
<p>14. Using what we learned by answering the questions above, what is the expected value of <span class="math inline">\(\bar{Y}_{2} - \bar{Y}_1\)</span>?</p>
<p>15. Using what we learned by answering the questions above, what is the standard error of <span class="math inline">\(\bar{Y}_{2} - \bar{Y}_1\)</span>?</p>
<p>16. The answer to the question above depends on <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>, which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.</p>
<p>17. What does the CLT tell us about the distribution of <span class="math inline">\(\bar{Y}_2 - \bar{Y}_1\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li>Nothing because this is not the average of a sample.</li>
<li>Because the <span class="math inline">\(Y_{ij}\)</span> are approximately normal, so are the averages.</li>
<li>Note that <span class="math inline">\(\bar{Y}_2\)</span> and <span class="math inline">\(\bar{Y}_1\)</span> are sample averages, so if we assume <span class="math inline">\(N_2\)</span> and <span class="math inline">\(N_1\)</span> are large enough, each is approximately normal. The difference of normals is also normal.</li>
<li>The data are not 0 or 1, so CLT does not apply.</li>
</ol>
<p>18. We have constructed a random variable that has expected value <span class="math inline">\(b_2 - b_1\)</span>, the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>, but we can plug the sample standard deviations we computed above. We started off by asking: is <span class="math inline">\(b_2 - b_1\)</span> different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_1\)</span>.</p>
<p>19. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value?</p>
<p>20. The statistic formed by dividing our estimate of <span class="math inline">\(b_2-b_1\)</span> by its estimated standard error:</p>
<p><span class="math display">\[
\frac{\bar{Y}_2 - \bar{Y}_1}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}
\]</span></p>
<p>is called the t-statistic. Now notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?</p>
<p>For this exercise, create a new table:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="models.html#cb601-1"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb601-2"><a href="models.html#cb601-2"></a><span class="st">  </span><span class="kw">filter</span>(enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-15&quot;</span> <span class="op">&amp;</span></span>
<span id="cb601-3"><a href="models.html#cb601-3"></a><span class="st">           </span>state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb601-4"><a href="models.html#cb601-4"></a><span class="st">  </span><span class="kw">group_by</span>(pollster) <span class="op">%&gt;%</span></span>
<span id="cb601-5"><a href="models.html#cb601-5"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb601-6"><a href="models.html#cb601-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb601-7"><a href="models.html#cb601-7"></a><span class="st">  </span><span class="kw">ungroup</span>()</span></code></pre></div>
<p>Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.</p>

</div>
<div id="bayesian-statistics" class="section level2">
<h2><span class="header-section-number">16.4</span> Bayesian statistics</h2>
<p>What does it mean when an election forecaster tells us that a given candidate has a 90% chance of winning? In the context of the urn model, this would be equivalent to stating that the probability <span class="math inline">\(p&gt;0.5\)</span> is 90%. However, as we discussed earlier, in the urn model <span class="math inline">\(p\)</span> is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we model <span class="math inline">\(p\)</span> as random variable and thus a statement such as “90% chance of winning” is consistent with the approach.</p>
<p>Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics.</p>
<p>In this chapter we briefly describe Bayesian statistics. For an in-depth treatment of this topic we recommend one of the following textbooks:</p>
<ul>
<li><p>Berger JO (1985). Statistical Decision Theory and Bayesian Analysis, 2nd edition. Springer-Verlag.</p></li>
<li><p>Lee PM (1989). Bayesian Statistics: An Introduction. Oxford.</p></li>
</ul>
<div id="bayes-theorem" class="section level3">
<h3><span class="header-section-number">16.4.1</span> Bayes theorem</h3>
<p>We start by describing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example.
Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation:</p>
<p><span class="math display">\[
\mbox{Prob}(+ \mid D=1)=0.99, \mbox{Prob}(- \mid D=0)=0.99 
\]</span></p>
<p>with <span class="math inline">\(+\)</span> meaning a positive test and <span class="math inline">\(D\)</span> representing if you actually have the disease (1) or not (0).</p>
<p>Suppose we select a random person and they test positive. What is the probability that they have the disease? We write this as <span class="math inline">\(\mbox{Prob}(D=1 \mid +)?\)</span> The cystic fibrosis rate is 1 in 3,900 which implies that <span class="math inline">\(\mbox{Prob}(D=1)=0.00025\)</span>. To answer this question, we will use Bayes theorem, which in general tells us that:</p>
<p><span class="math display">\[
\mbox{Pr}(A \mid B)  =  \frac{\mbox{Pr}(B \mid A)\mbox{Pr}(A)}{\mbox{Pr}(B)} 
\]</span></p>
<p>This equation applied to our problem becomes:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{Pr}(D=1 \mid +) &amp; =  \frac{ P(+ \mid D=1) \cdot P(D=1)} {\mbox{Pr}(+)} \\
&amp; =  \frac{\mbox{Pr}(+ \mid D=1)\cdot P(D=1)} {\mbox{Pr}(+ \mid D=1) \cdot P(D=1) + \mbox{Pr}(+ \mid D=0) \mbox{Pr}( D=0)} 
\end{aligned}
\]</span></p>
<p>Plugging in the numbers we get:</p>
<p><span class="math display">\[
\frac{0.99 \cdot 0.00025}{0.99 \cdot 0.00025 + 0.01 \cdot (.99975)}  =  0.02 
\]</span></p>
<p>This says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counter-intuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this, we run a Monte Carlo simulation.</p>
</div>
</div>
<div id="bayes-theorem-simulation" class="section level2">
<h2><span class="header-section-number">16.5</span> Bayes theorem simulation</h2>
<p>The following simulation is meant to help you visualize Bayes theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.</p>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb602-1"><a href="models.html#cb602-1"></a>prev &lt;-<span class="st"> </span><span class="fl">0.00025</span></span>
<span id="cb602-2"><a href="models.html#cb602-2"></a>N &lt;-<span class="st"> </span><span class="dv">100000</span></span>
<span id="cb602-3"><a href="models.html#cb602-3"></a>outcome &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Disease&quot;</span>,<span class="st">&quot;Healthy&quot;</span>), N, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </span>
<span id="cb602-4"><a href="models.html#cb602-4"></a>                  <span class="dt">prob =</span> <span class="kw">c</span>(prev, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>prev))</span></code></pre></div>
<p>Note that there are very few people with the disease:</p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="models.html#cb603-1"></a>N_D &lt;-<span class="st"> </span><span class="kw">sum</span>(outcome <span class="op">==</span><span class="st"> &quot;Disease&quot;</span>)</span>
<span id="cb603-2"><a href="models.html#cb603-2"></a>N_D</span>
<span id="cb603-3"><a href="models.html#cb603-3"></a><span class="co">#&gt; [1] 23</span></span>
<span id="cb603-4"><a href="models.html#cb603-4"></a>N_H &lt;-<span class="st"> </span><span class="kw">sum</span>(outcome <span class="op">==</span><span class="st"> &quot;Healthy&quot;</span>)</span>
<span id="cb603-5"><a href="models.html#cb603-5"></a>N_H</span>
<span id="cb603-6"><a href="models.html#cb603-6"></a><span class="co">#&gt; [1] 99977</span></span></code></pre></div>
<p>Also, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test, which is correct 99% of the time:</p>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb604-1"><a href="models.html#cb604-1"></a>accuracy &lt;-<span class="st"> </span><span class="fl">0.99</span></span>
<span id="cb604-2"><a href="models.html#cb604-2"></a>test &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;character&quot;</span>, N)</span>
<span id="cb604-3"><a href="models.html#cb604-3"></a>test[outcome <span class="op">==</span><span class="st"> &quot;Disease&quot;</span>]  &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;+&quot;</span>, <span class="st">&quot;-&quot;</span>), N_D, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </span>
<span id="cb604-4"><a href="models.html#cb604-4"></a>                                    <span class="dt">prob =</span> <span class="kw">c</span>(accuracy, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy))</span>
<span id="cb604-5"><a href="models.html#cb604-5"></a>test[outcome <span class="op">==</span><span class="st"> &quot;Healthy&quot;</span>]  &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;-&quot;</span>, <span class="st">&quot;+&quot;</span>), N_H, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </span>
<span id="cb604-6"><a href="models.html#cb604-6"></a>                                    <span class="dt">prob =</span> <span class="kw">c</span>(accuracy, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy))</span></code></pre></div>
<p>Because there are so many more controls than cases, even with a low false positive rate we get more controls than cases in the group that tested positive:</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="models.html#cb605-1"></a><span class="kw">table</span>(outcome, test)</span>
<span id="cb605-2"><a href="models.html#cb605-2"></a><span class="co">#&gt;          test</span></span>
<span id="cb605-3"><a href="models.html#cb605-3"></a><span class="co">#&gt; outcome       -     +</span></span>
<span id="cb605-4"><a href="models.html#cb605-4"></a><span class="co">#&gt;   Disease     0    23</span></span>
<span id="cb605-5"><a href="models.html#cb605-5"></a><span class="co">#&gt;   Healthy 99012   965</span></span></code></pre></div>
<p>From this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.022.</p>
<div id="bayes-in-practice" class="section level3">
<h3><span class="header-section-number">16.5.1</span> Bayes in practice</h3>
<p>José Iglesias is a professional baseball player. In April 2013, when he was starting his career, he was performing rather well:</p>
<table>
<thead>
<tr class="header">
<th>Month</th>
<th>At Bats</th>
<th>H</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April</td>
<td>20</td>
<td>9</td>
<td>.450</td>
</tr>
</tbody>
</table>
<p>The batting average (<code>AVG</code>) statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. An <code>AVG</code> of .450 means José has been successful 45% of the times he has batted (<code>At Bats</code>) which is rather high, historically speaking. Keep in mind that no one has finished a season with an <code>AVG</code> of .400 or more since Ted Williams did it in 1941! To illustrate the way hierarchical models are powerful, we will try to predict José’s batting average at the end of the season. Note that in a typical season, players have about 500 at bats.</p>
<p>With the techniques we have learned up to now, referred to as <em>frequentist techniques</em>, the best we can do is provide a confidence interval. We can think of outcomes from hitting as a binomial with a success rate of <span class="math inline">\(p\)</span>. So if the success rate is indeed .450, the standard error of just 20 at bats is:</p>
<p><span class="math display">\[
\sqrt{\frac{.450 (1-.450)}{20}}=.111
\]</span></p>
<p>This means that our confidence interval is <span class="math inline">\(.450 - .222\)</span> to <span class="math inline">\(.450 + .222\)</span> or <span class="math inline">\(.228\)</span> to <span class="math inline">\(.672\)</span>.</p>
<p>This prediction has two problems. First, it is very large, so not very useful. Second, it is centered at .450, which implies that our best guess is that this new player will break Ted Williams’ record.</p>
<p>If you follow baseball, this last statement will seem wrong and this is because you are implicitly using a hierarchical model that factors in information from years of following baseball. Here we show how we can quantify this intuition.</p>
<p>First, let’s explore the distribution of batting averages for all players with more than 500 at bats during the previous three seasons:</p>
<p><img src="book_files/figure-html/batting-averages-histogram-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The average player had an <code>AVG</code> of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean.</p>
<p>So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential.</p>
</div>
</div>
<div id="hierarchical-models" class="section level2">
<h2><span class="header-section-number">16.6</span> Hierarchical models</h2>
<p>The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, <span class="math inline">\(p\)</span>. Then we see 20 random outcomes with success probability <span class="math inline">\(p\)</span>.</p>
<p>We use a model to represent two levels of variability in our data. First, each player is assigned a natural ability to hit. We will use the symbol <span class="math inline">\(p\)</span> to represent this ability. You can think of <span class="math inline">\(p\)</span> as the batting average you would converge to if this particular player batted over and over again.</p>
<p>Based on the plots we showed earlier, we assume that <span class="math inline">\(p\)</span> has a normal distribution. With expected value .270 and standard error 0.027.</p>
<p>Now the second level of variability has to do with luck when batting. Regardless of how good the player is, sometimes you have bad luck and sometimes you have good luck. At each at bat, this player has a probability of success <span class="math inline">\(p\)</span>. If we add up these successes and failures, then the CLT tells us that the observed average, call it <span class="math inline">\(Y\)</span>, has a normal distribution with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span> with <span class="math inline">\(N\)</span> the number of at bats.</p>
<p>Statistical textbooks will write the model like this:
<span class="math display">\[
\begin{aligned}
p &amp;\sim N(\mu, \tau^2) \\
Y \mid p &amp;\sim N(p, \sigma^2) 
\end{aligned}
\]</span>
Here the <span class="math inline">\(\sim\)</span> symbol tells us the random variable on the left of the symbol follows the distribution on the right and <span class="math inline">\(N(a,b^2)\)</span> represents the normal distribution with mean <span class="math inline">\(a\)</span> and standard deviation <span class="math inline">\(b\)</span>. The <span class="math inline">\(\mid\)</span> is read as <em>conditioned on</em>, and it means that we are treating the random variable to the right of the symbol as known. We refer to the model as hierarchical because we need to know <span class="math inline">\(p\)</span>, the first level, in order to model <span class="math inline">\(Y\)</span>, the second level. In our example the first level describes randomness in assigning talent to a player and the second describes randomness in this particular player’s performance once we have fixed the talent parameter. In a Bayesian framework, the first level is called a <em>prior distribution</em> and the second the <em>sampling distribution</em>. The data analysis we have conducted here suggests that we set <span class="math inline">\(\mu = .270\)</span>, <span class="math inline">\(\tau = 0.027\)</span>, and <span class="math inline">\(\sigma^2 = p(1-p)/N\)</span>.</p>
<p>Now, let’s use this model for José’s data. Suppose we want to predict his innate ability in the form of his <em>true</em> batting average <span class="math inline">\(p\)</span>. This would be the hierarchical model for our data:</p>
<p><span class="math display">\[
\begin{aligned}
p &amp;\sim N(.275, .027^2) \\
Y \mid p &amp;\sim N(p, .111^2) 
\end{aligned}
\]</span></p>
<p>We now are ready to compute a posterior distribution to summarize our prediction of <span class="math inline">\(p\)</span>. The continuous version of Bayes’ rule can be used here to derive the <em>posterior probability function</em>, which is the distribution of <span class="math inline">\(p\)</span> assuming we observe <span class="math inline">\(Y=y\)</span>. In our case, we can show that when we fix <span class="math inline">\(Y=y\)</span>, <span class="math inline">\(p\)</span> follows a normal distribution with expected value:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{E}(p \mid Y=y) &amp;= B \mu + (1-B) y\\
&amp;= \mu + (1-B)(y-\mu)\\
\mbox{with } B &amp;= \frac{\sigma^2}{\sigma^2+\tau^2}
\end{aligned}
\]</span></p>
<p>This is a weighted average of the population average <span class="math inline">\(\mu\)</span> and the observed data <span class="math inline">\(y\)</span>. The weight depends on the SD of the population <span class="math inline">\(\tau\)</span> and the SD of our observed data <span class="math inline">\(\sigma\)</span>. This weighted average is sometimes referred to as <em>shrinking</em> because it <em>shrinks</em> estimates towards a prior mean. In the case of José Iglesias, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{E}(p \mid Y=.450) &amp;= B \times .275 + (1 - B) \times .450 \\
&amp;= .275 + (1 - B)(.450 - .275) \\
B &amp;=\frac{.111^2}{.111^2 + .027^2} = 0.944\\
\mbox{E}(p \mid Y=450) &amp;\approx .285
\end{aligned}
\]</span></p>
<p>We do not show the derivation here, but the standard error can be shown to be:</p>
<p><span class="math display">\[
\mbox{SE}(p\mid y)^2 = \frac{1}{1/\sigma^2+1/\tau^2}
= \frac{1}{1/.111^2 + 1/.027^2} = 0.00069
\]</span>
and the standard deviation is therefore <span class="math inline">\(0.026\)</span>. So we started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 <span class="math inline">\(\pm\)</span> 0.220. Then we used a Bayesian approach that incorporated data from other players and other years to obtain a posterior probability. This is actually referred to as an empirical Bayes approach because we used data to construct the prior. From the posterior, we can report what is called a 95% <em>credible interval</em> by reporting a region, centered at the mean, with a 95% chance of occurring. In our case, this turns out to be: .285 <span class="math inline">\(\pm\)</span> 0.052.</p>
<p>The Bayesian credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months:</p>
<table>
<thead>
<tr class="header">
<th>Month</th>
<th>At Bat</th>
<th>Hits</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April</td>
<td>20</td>
<td>9</td>
<td>.450</td>
</tr>
<tr class="even">
<td>May</td>
<td>26</td>
<td>11</td>
<td>.423</td>
</tr>
<tr class="odd">
<td>June</td>
<td>86</td>
<td>34</td>
<td>.395</td>
</tr>
<tr class="even">
<td>July</td>
<td>83</td>
<td>17</td>
<td>.205</td>
</tr>
<tr class="odd">
<td>August</td>
<td>85</td>
<td>25</td>
<td>.294</td>
</tr>
<tr class="even">
<td>September</td>
<td>50</td>
<td>10</td>
<td>.200</td>
</tr>
<tr class="odd">
<td>Total w/o April</td>
<td>330</td>
<td>97</td>
<td>.293</td>
</tr>
</tbody>
</table>
<p>Although both intervals included the final batting average, the Bayesian credible interval provided a much more precise prediction. In particular, it predicted that he would not be as good during the remainder of the season.</p>
</div>
<div id="exercises-31" class="section level2">
<h2><span class="header-section-number">16.7</span> Exercises</h2>
<p>1. In 1999, in England, Sally Clark<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 <span class="math inline">\(\times\)</span> 8,500 <span class="math inline">\(\approx\)</span> 73 million. Which of the following do you agree with?</p>
<ol style="list-style-type: lower-alpha">
<li>Sir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: <span class="math inline">\(\mbox{Pr}(\mbox{second case of SIDS} \mid \mbox{first case of SIDS}) &lt; \mbox{P}r(\mbox{first case of SIDS})\)</span>.</li>
<li>Nothing. The multiplication rule always applies in this way: <span class="math inline">\(\mbox{Pr}(A \mbox{ and } B) =\mbox{Pr}(A)\mbox{Pr}(B)\)</span></li>
<li>Sir Meadow is an expert and we should trust his calculations.</li>
<li>Numbers don’t lie.</li>
</ol>
<p>2. Let’s assume that there is in fact a genetic component to SIDS and the probability of <span class="math inline">\(\mbox{Pr}(\mbox{second case of SIDS} \mid \mbox{first case of SIDS}) = 1/100\)</span>, is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?</p>
<p>3. Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of <em>a mother is a son-murdering psychopath</em> given that
<em>two of her children are found dead with no evidence of physical harm</em>.
According to Bayes’ rule, what is this?</p>
<p>4. Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is:</p>
<p><span class="math display">\[
\mbox{Pr}(A \mid B) = 0.50
\]</span></p>
<p>with A = two of her children are found dead with no evidence of physical harm and B = a mother is a son-murdering psychopath = 0.50. Assume that the rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ theorem, what is the probability of <span class="math inline">\(\mbox{Pr}(B \mid A)\)</span> ?</p>
<p>5/. After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?</p>
<ol style="list-style-type: lower-alpha">
<li>He made an arithmetic error.</li>
<li>He made two mistakes. First, he misused the multiplication rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million.</li>
<li>He mixed up the numerator and denominator of Bayes’ rule.</li>
<li>He did not use R.</li>
</ol>
<p>6. Florida is one of the most closely watched states in the U.S. election because it has many electoral votes, and the election is generally close, and Florida tends to be a swing state that can vote either way. Create the following table with the polls taken during the last two weeks:</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb606-1"><a href="models.html#cb606-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb606-2"><a href="models.html#cb606-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb606-3"><a href="models.html#cb606-3"></a><span class="kw">data</span>(polls_us_election_<span class="dv">2016</span>)</span>
<span id="cb606-4"><a href="models.html#cb606-4"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb606-5"><a href="models.html#cb606-5"></a><span class="st">  </span><span class="kw">filter</span>(state <span class="op">==</span><span class="st"> &quot;Florida&quot;</span> <span class="op">&amp;</span><span class="st"> </span>enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-11-04&quot;</span> ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb606-6"><a href="models.html#cb606-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>)</span></code></pre></div>
<p>Take the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called <code>results</code>.</p>
<p>7. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread <span class="math inline">\(d\)</span> to be Normal with expected value <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span>. What are the interpretations of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are arbitrary numbers that let us make probability statements about <span class="math inline">\(d\)</span>.</li>
<li><span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set <span class="math inline">\(\mu\)</span> close to 0 because both Republicans and Democrats have won, and <span class="math inline">\(\tau\)</span> at about <span class="math inline">\(0.02\)</span>, because these elections tend to be close.</li>
<li><span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> summarize what we want to be true. We therefore set <span class="math inline">\(\mu\)</span> at <span class="math inline">\(0.10\)</span> and <span class="math inline">\(\tau\)</span> at <span class="math inline">\(0.01\)</span>.</li>
<li>The choice of prior has no effect on Bayesian analysis.</li>
</ol>
<p>8. The CLT tells us that our estimate of the spread <span class="math inline">\(\hat{d}\)</span> has normal distribution with expected value <span class="math inline">\(d\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\tau = 0.01\)</span>.</p>
<p>9. Now compute the standard deviation of the posterior distribution.</p>
<p>10. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.</p>
<p>11. According to this analysis, what was the probability that Trump wins Florida?</p>
<p>12. Now use <code>sapply</code> function to change the prior variance from <code>seq(0.05, 0.05, len = 100)</code> and observe how the probability changes by making a plot.</p>

</div>
<div id="election-forecasting" class="section level2">
<h2><span class="header-section-number">16.8</span> Case study: election forecasting</h2>
<p>In a previous section, we generated these data tables:</p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="models.html#cb607-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb607-2"><a href="models.html#cb607-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb607-3"><a href="models.html#cb607-3"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb607-4"><a href="models.html#cb607-4"></a><span class="st">  </span><span class="kw">filter</span>(state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span> <span class="op">&amp;</span><span class="st"> </span>enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-31&quot;</span> <span class="op">&amp;</span></span>
<span id="cb607-5"><a href="models.html#cb607-5"></a><span class="st">           </span>(grade <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A+&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;A-&quot;</span>,<span class="st">&quot;B+&quot;</span>) <span class="op">|</span><span class="st"> </span><span class="kw">is.na</span>(grade))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb607-6"><a href="models.html#cb607-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>)</span>
<span id="cb607-7"><a href="models.html#cb607-7"></a></span>
<span id="cb607-8"><a href="models.html#cb607-8"></a>one_poll_per_pollster &lt;-<span class="st"> </span>polls <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(pollster) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb607-9"><a href="models.html#cb607-9"></a><span class="st">  </span><span class="kw">filter</span>(enddate <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(enddate)) <span class="op">%&gt;%</span></span>
<span id="cb607-10"><a href="models.html#cb607-10"></a><span class="st">  </span><span class="kw">ungroup</span>()</span>
<span id="cb607-11"><a href="models.html#cb607-11"></a></span>
<span id="cb607-12"><a href="models.html#cb607-12"></a>results &lt;-<span class="st"> </span>one_poll_per_pollster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb607-13"><a href="models.html#cb607-13"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(spread), <span class="dt">se =</span> <span class="kw">sd</span>(spread)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">length</span>(spread))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb607-14"><a href="models.html#cb607-14"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">start =</span> avg <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>se, <span class="dt">end =</span> avg <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>se) </span></code></pre></div>
<p>Below, we will use these for our forecasting.</p>
<div id="bayesian-approach" class="section level3">
<h3><span class="header-section-number">16.8.1</span> Bayesian approach</h3>
<p>Pollsters tend to make probabilistic statements about the results of the election. For example, “The chance that Obama wins the electoral college is 91%” is a probabilistic statement about a parameter which in previous sections we have denoted with <span class="math inline">\(d\)</span>. We showed that for the 2016 election, FiveThirtyEight gave Clinton an 81.4% chance of winning the popular vote. To do this, they used the Bayesian approach we described.</p>
<p>We assume a hierarchical model similar to what we did to predict the performance of a baseball player. Statistical textbooks will write the model like this:</p>
<p><span class="math display">\[
\begin{aligned}
d &amp;\sim N(\mu, \tau^2) \mbox{ describes our best guess had we not seen any polling data}\\
\bar{X} \mid d &amp;\sim N(d, \sigma^2) \mbox{ describes randomness due to sampling and the  pollster effect}
\end{aligned}
\]</span></p>
<p>For our best guess, we note that before any poll data is available, we can use data sources other than polling data. A popular approach is to use what pollsters call <em>fundamentals</em>, which are based on properties about the current economy that historically appear to have an effect in favor or against the incumbent party. We won’t use these here. Instead, we will use <span class="math inline">\(\mu = 0\)</span>, which is interpreted as a model that simply does not provide any information on who will win. For the standard deviation, we will use recent historical data that shows the winner of the popular vote has an average spread of about 3.5%. Therefore, we set <span class="math inline">\(\tau = 0.035\)</span>.</p>
<p>Now we can use the formulas for the posterior distribution for the parameter <span class="math inline">\(d\)</span>: the probability of <span class="math inline">\(d&gt;0\)</span> given the observed poll data:</p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="models.html#cb608-1"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb608-2"><a href="models.html#cb608-2"></a>tau &lt;-<span class="st"> </span><span class="fl">0.035</span></span>
<span id="cb608-3"><a href="models.html#cb608-3"></a>sigma &lt;-<span class="st"> </span>results<span class="op">$</span>se</span>
<span id="cb608-4"><a href="models.html#cb608-4"></a>Y &lt;-<span class="st"> </span>results<span class="op">$</span>avg</span>
<span id="cb608-5"><a href="models.html#cb608-5"></a>B &lt;-<span class="st"> </span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>tau<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb608-6"><a href="models.html#cb608-6"></a></span>
<span id="cb608-7"><a href="models.html#cb608-7"></a>posterior_mean &lt;-<span class="st"> </span>B<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>B)<span class="op">*</span>Y</span>
<span id="cb608-8"><a href="models.html#cb608-8"></a>posterior_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>tau<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb608-9"><a href="models.html#cb608-9"></a></span>
<span id="cb608-10"><a href="models.html#cb608-10"></a>posterior_mean</span>
<span id="cb608-11"><a href="models.html#cb608-11"></a><span class="co">#&gt; [1] 0.0281</span></span>
<span id="cb608-12"><a href="models.html#cb608-12"></a>posterior_se</span>
<span id="cb608-13"><a href="models.html#cb608-13"></a><span class="co">#&gt; [1] 0.00615</span></span></code></pre></div>
<p>To make a probability statement, we use the fact that the posterior distribution is also normal. And we have a credible interval of:</p>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="models.html#cb609-1"></a>posterior_mean <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>, <span class="fl">1.96</span>)<span class="op">*</span>posterior_se</span>
<span id="cb609-2"><a href="models.html#cb609-2"></a><span class="co">#&gt; [1] 0.0160 0.0401</span></span></code></pre></div>
<p>The posterior probability <span class="math inline">\(\mbox{Pr}(d&gt;0 \mid \bar{X})\)</span> can be computed like this:</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="models.html#cb610-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">0</span>, posterior_mean, posterior_se)</span>
<span id="cb610-2"><a href="models.html#cb610-2"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>This says we are 100% sure Clinton will win the popular vote, which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference?</p>
</div>
<div id="the-general-bias" class="section level3">
<h3><span class="header-section-number">16.8.2</span> The general bias</h3>
<p>After elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our model does not take into account is that it is common to see a general bias that affects many pollsters in the same way making the observed data correlated. There is no good explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%.</p>
<p>Although we know this bias term affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for this variability.</p>
</div>
<div id="mathematical-representations-of-models" class="section level3">
<h3><span class="header-section-number">16.8.3</span> Mathematical representations of models</h3>
<p>Suppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of <span class="math inline">\(N\)</span>, so we observe several measurements of the spread <span class="math inline">\(X_1, \dots, X_J\)</span>. The theory tells us that these random variables have expected value <span class="math inline">\(d\)</span> and standard error <span class="math inline">\(2 \sqrt{p(1-p)/N}\)</span>. Let’s start by using the following model to describe the observed variability:</p>
<p><span class="math display">\[
X_j = d + \varepsilon_j.
\]</span>
We use the index <span class="math inline">\(j\)</span> to represent the different polls and we define <span class="math inline">\(\varepsilon_j\)</span> to be a random variable that explains the poll-to-poll variability introduced by sampling error. To do this, we assume its average is 0 and standard error is <span class="math inline">\(2 \sqrt{p(1-p)/N}\)</span>. If <span class="math inline">\(d\)</span> is 2.1 and the sample size for these polls is 2,000, we can simulate <span class="math inline">\(J=6\)</span> data points from this model like this:</p>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="models.html#cb611-1"></a><span class="kw">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb611-2"><a href="models.html#cb611-2"></a>J &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb611-3"><a href="models.html#cb611-3"></a>N &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb611-4"><a href="models.html#cb611-4"></a>d &lt;-<span class="st"> </span><span class="fl">.021</span></span>
<span id="cb611-5"><a href="models.html#cb611-5"></a>p &lt;-<span class="st"> </span>(d <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span></span>
<span id="cb611-6"><a href="models.html#cb611-6"></a>X &lt;-<span class="st"> </span>d <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(J, <span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>N))</span></code></pre></div>
<p>Now suppose we have <span class="math inline">\(J=6\)</span> data points from <span class="math inline">\(I=5\)</span> different pollsters. To represent this we now need two indexes, one for pollster and one for the polls each pollster takes. We use <span class="math inline">\(X_{ij}\)</span> with <span class="math inline">\(i\)</span> representing the pollster and <span class="math inline">\(j\)</span> representing the <span class="math inline">\(j\)</span>-th poll from that pollster. If we apply the same model, we write:</p>
<p><span class="math display">\[
X_{i,j} = d + \varepsilon_{i,j}
\]</span></p>
<p>To simulate data, we now have to loop through the pollsters:</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="models.html#cb612-1"></a>I &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb612-2"><a href="models.html#cb612-2"></a>J &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb612-3"><a href="models.html#cb612-3"></a>N &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb612-4"><a href="models.html#cb612-4"></a>X &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>I, <span class="cf">function</span>(i){</span>
<span id="cb612-5"><a href="models.html#cb612-5"></a>  d <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(J, <span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>N))</span>
<span id="cb612-6"><a href="models.html#cb612-6"></a>})</span></code></pre></div>
<p>The simulated data does not really seem to capture the features of the actual data:</p>
<p><img src="book_files/figure-html/simulated-data-without-bias-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The model above does not account for pollster-to-pollster variability. To fix this, we add a new term for the pollster effect. We will use <span class="math inline">\(h_i\)</span> to represent the house effect of the <span class="math inline">\(i\)</span>-th pollster. The model is now augmented to:</p>
<p><span class="math display">\[
X_{i,j} = d + h_i + \varepsilon_{i,j}
\]</span></p>
<p>To simulate data from a specific pollster, we now need to draw an <span class="math inline">\(h_i\)</span> and then add the <span class="math inline">\(\varepsilon\)</span>s. Here is how we would do it for one specific pollster. We assume <span class="math inline">\(\sigma_h\)</span> is 0.025:</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="models.html#cb613-1"></a>I &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb613-2"><a href="models.html#cb613-2"></a>J &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb613-3"><a href="models.html#cb613-3"></a>N &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb613-4"><a href="models.html#cb613-4"></a>d &lt;-<span class="st"> </span><span class="fl">.021</span></span>
<span id="cb613-5"><a href="models.html#cb613-5"></a>p &lt;-<span class="st"> </span>(d <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb613-6"><a href="models.html#cb613-6"></a>h &lt;-<span class="st"> </span><span class="kw">rnorm</span>(I, <span class="dv">0</span>, <span class="fl">0.025</span>)</span>
<span id="cb613-7"><a href="models.html#cb613-7"></a>X &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>I, <span class="cf">function</span>(i){</span>
<span id="cb613-8"><a href="models.html#cb613-8"></a>  d <span class="op">+</span><span class="st"> </span>h[i] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(J, <span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>N))</span>
<span id="cb613-9"><a href="models.html#cb613-9"></a>})</span></code></pre></div>
<p>The simulated data now looks more like the actual data:</p>
<p><img src="book_files/figure-html/simulated-pollster-data-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>Note that <span class="math inline">\(h_i\)</span> is common to all the observed spreads from a specific pollster. Different pollsters have a different <span class="math inline">\(h_i\)</span>, which explains why we can see the groups of points shift up and down from pollster to pollster.</p>
<p>Now, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is <span class="math inline">\(\sigma_h\)</span>. But historically we see that every election has a general bias affecting all polls.
We can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another term to account for this variability:
<span class="math display">\[
X_{i,j} = d + b + h_i + \varepsilon_{i,j}.
\]</span></p>
<p>Here <span class="math inline">\(b\)</span> is a random variable that accounts for the election-to-election variability. This random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within on election. This is why it does not have indexes. This implies that all the random variables <span class="math inline">\(X_{i,j}\)</span> for an election year are correlated since they all have <span class="math inline">\(b\)</span> in common.</p>
<p>One way to interpret <span class="math inline">\(b\)</span> is as the difference between the average of all polls from all pollsters and the actual result of the election. Because we don’t know the actual result until after the election, we can’t estimate <span class="math inline">\(b\)</span> until after the election. However, we can estimate <span class="math inline">\(b\)</span> from previous elections and study the distribution of these values. Based on this approach we assume that, across election years, <span class="math inline">\(b\)</span> has expected value 0 and the standard error is about <span class="math inline">\(\sigma_b = 0.025\)</span>.</p>
<p>An implication of adding this term to the model is that the standard deviation for <span class="math inline">\(X_{i,j}\)</span> is actually higher than what we earlier called <span class="math inline">\(\sigma\)</span>, which combines the pollster variability and the sample in variability, and was estimated with:</p>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="models.html#cb614-1"></a><span class="kw">sd</span>(one_poll_per_pollster<span class="op">$</span>spread)</span>
<span id="cb614-2"><a href="models.html#cb614-2"></a><span class="co">#&gt; [1] 0.0242</span></span></code></pre></div>
<p>This estimate does not include the variability introduced by <span class="math inline">\(b\)</span>. Note that because</p>
<p><span class="math display">\[
\bar{X} = d + b + \frac{1}{N}\sum_{i=1}^N X_i,
\]</span></p>
<p>the standard deviation of <span class="math inline">\(\bar{X}\)</span> is:</p>
<p><span class="math display">\[
\sqrt{\sigma^2/N + \sigma_b^2}.
\]</span>
Since the same <span class="math inline">\(b\)</span> is in every measurement, the average does not reduce the variability introduced by the <span class="math inline">\(b\)</span> term. This is an important point: it does not matter how many polls you take, this bias does not get reduced.</p>
<p>If we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s:</p>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb615-1"><a href="models.html#cb615-1"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb615-2"><a href="models.html#cb615-2"></a>tau &lt;-<span class="st"> </span><span class="fl">0.035</span></span>
<span id="cb615-3"><a href="models.html#cb615-3"></a>sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(results<span class="op">$</span>se<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">.025</span><span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb615-4"><a href="models.html#cb615-4"></a>Y &lt;-<span class="st"> </span>results<span class="op">$</span>avg</span>
<span id="cb615-5"><a href="models.html#cb615-5"></a>B &lt;-<span class="st"> </span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>tau<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb615-6"><a href="models.html#cb615-6"></a></span>
<span id="cb615-7"><a href="models.html#cb615-7"></a>posterior_mean &lt;-<span class="st"> </span>B<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>B)<span class="op">*</span>Y</span>
<span id="cb615-8"><a href="models.html#cb615-8"></a>posterior_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>tau<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb615-9"><a href="models.html#cb615-9"></a></span>
<span id="cb615-10"><a href="models.html#cb615-10"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">0</span>, posterior_mean, posterior_se)</span>
<span id="cb615-11"><a href="models.html#cb615-11"></a><span class="co">#&gt; [1] 0.817</span></span></code></pre></div>
</div>
<div id="predicting-the-electoral-college" class="section level3">
<h3><span class="header-section-number">16.8.4</span> Predicting the electoral college</h3>
<p>Up to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes in 2016.</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="models.html#cb616-1"></a>results_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">top_n</span>(<span class="dv">5</span>, electoral_votes)</span>
<span id="cb616-2"><a href="models.html#cb616-2"></a><span class="co">#&gt;          state electoral_votes clinton trump others</span></span>
<span id="cb616-3"><a href="models.html#cb616-3"></a><span class="co">#&gt; 1   California              55    61.7  31.6    6.7</span></span>
<span id="cb616-4"><a href="models.html#cb616-4"></a><span class="co">#&gt; 2        Texas              38    43.2  52.2    4.5</span></span>
<span id="cb616-5"><a href="models.html#cb616-5"></a><span class="co">#&gt; 3      Florida              29    47.8  49.0    3.2</span></span>
<span id="cb616-6"><a href="models.html#cb616-6"></a><span class="co">#&gt; 4     New York              29    59.0  36.5    4.5</span></span>
<span id="cb616-7"><a href="models.html#cb616-7"></a><span class="co">#&gt; 5     Illinois              20    55.8  38.8    5.4</span></span>
<span id="cb616-8"><a href="models.html#cb616-8"></a><span class="co">#&gt; 6 Pennsylvania              20    47.9  48.6    3.6</span></span></code></pre></div>
<p>With some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you win California by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000, and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election. Nonetheless, many people in the US consider the electoral college unfair and would like to see it abolished.</p>
<p>We are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election. We use the <code>str_detect</code>, a function we introduce later in Section <a href="string-processing.html#stringr">24.1</a>, to remove polls that are not for entire states.</p>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="models.html#cb617-1"></a>results &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span></span>
<span id="cb617-2"><a href="models.html#cb617-2"></a><span class="st">  </span><span class="kw">filter</span>(state<span class="op">!=</span><span class="st">&quot;U.S.&quot;</span> <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb617-3"><a href="models.html#cb617-3"></a><span class="st">           </span><span class="op">!</span><span class="kw">str_detect</span>(state, <span class="st">&quot;CD&quot;</span>) <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb617-4"><a href="models.html#cb617-4"></a><span class="st">           </span>enddate <span class="op">&gt;=</span><span class="st">&quot;2016-10-31&quot;</span> <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb617-5"><a href="models.html#cb617-5"></a><span class="st">           </span>(grade <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A+&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;A-&quot;</span>,<span class="st">&quot;B+&quot;</span>) <span class="op">|</span><span class="st"> </span><span class="kw">is.na</span>(grade))) <span class="op">%&gt;%</span></span>
<span id="cb617-6"><a href="models.html#cb617-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb617-7"><a href="models.html#cb617-7"></a><span class="st">  </span><span class="kw">group_by</span>(state) <span class="op">%&gt;%</span></span>
<span id="cb617-8"><a href="models.html#cb617-8"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(spread), <span class="dt">sd =</span> <span class="kw">sd</span>(spread), <span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb617-9"><a href="models.html#cb617-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">as.character</span>(state))</span>
<span id="cb617-10"><a href="models.html#cb617-10"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p>Here are the five closest races according to the polls:</p>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="models.html#cb618-1"></a>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">abs</span>(avg))</span>
<span id="cb618-2"><a href="models.html#cb618-2"></a><span class="co">#&gt; # A tibble: 47 x 4</span></span>
<span id="cb618-3"><a href="models.html#cb618-3"></a><span class="co">#&gt;   state               avg     sd     n</span></span>
<span id="cb618-4"><a href="models.html#cb618-4"></a><span class="co">#&gt;   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;</span></span>
<span id="cb618-5"><a href="models.html#cb618-5"></a><span class="co">#&gt; 1 Florida         0.00356 0.0163     7</span></span>
<span id="cb618-6"><a href="models.html#cb618-6"></a><span class="co">#&gt; 2 North Carolina -0.00730 0.0306     9</span></span>
<span id="cb618-7"><a href="models.html#cb618-7"></a><span class="co">#&gt; 3 Ohio           -0.0104  0.0252     6</span></span>
<span id="cb618-8"><a href="models.html#cb618-8"></a><span class="co">#&gt; 4 Nevada          0.0169  0.0441     7</span></span>
<span id="cb618-9"><a href="models.html#cb618-9"></a><span class="co">#&gt; 5 Iowa           -0.0197  0.0437     3</span></span>
<span id="cb618-10"><a href="models.html#cb618-10"></a><span class="co">#&gt; # … with 42 more rows</span></span></code></pre></div>
<p>We now introduce the command <code>left_join</code> that will let us easily add the number of electoral votes for each state from the dataset <code>us_electoral_votes_2016</code>. We will describe this function in detail in the Wrangling chapter. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first:</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="models.html#cb619-1"></a>results &lt;-<span class="st"> </span><span class="kw">left_join</span>(results, results_us_election_<span class="dv">2016</span>, <span class="dt">by =</span> <span class="st">&quot;state&quot;</span>)</span></code></pre></div>
<p>Notice that some states have no polls because the winner is pretty much known:</p>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="models.html#cb620-1"></a>results_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="op">!</span>state <span class="op">%in%</span><span class="st"> </span>results<span class="op">$</span>state) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb620-2"><a href="models.html#cb620-2"></a><span class="st">  </span><span class="kw">pull</span>(state)</span>
<span id="cb620-3"><a href="models.html#cb620-3"></a><span class="co">#&gt; [1] &quot;Rhode Island&quot;         &quot;Alaska&quot;               &quot;Wyoming&quot;             </span></span>
<span id="cb620-4"><a href="models.html#cb620-4"></a><span class="co">#&gt; [4] &quot;District of Columbia&quot;</span></span></code></pre></div>
<p>No polls were conducted in DC, Rhode Island, Alaska, and Wyoming because Democrats are sure to win in the first two and Republicans in the last two.</p>
<p>Because we can’t estimate the standard deviation for states with just one poll, we will estimate it as the median of the standard deviations estimated for states with more than one poll:</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="models.html#cb621-1"></a>results &lt;-<span class="st"> </span>results <span class="op">%&gt;%</span></span>
<span id="cb621-2"><a href="models.html#cb621-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sd =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(sd), <span class="kw">median</span>(results<span class="op">$</span>sd, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>), sd))</span></code></pre></div>
<p>To make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day <span class="math inline">\(d\)</span>. We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or <span class="math inline">\(\tau=0.02\)</span>. For now, we will assume, incorrectly, that the poll results from each state are independent. The code for the Bayesian calculation under these assumptions looks like this:</p>
<pre><code>#&gt; # A tibble: 47 x 12
#&gt;   state     avg      sd     n electoral_votes clinton trump others
#&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
#&gt; 1 Alab… -0.149  2.53e-2     3               9    34.4  62.1    3.6
#&gt; 2 Ariz… -0.0326 2.70e-2     9              11    45.1  48.7    6.2
#&gt; 3 Arka… -0.151  9.90e-4     2               6    33.7  60.6    5.8
#&gt; 4 Cali…  0.260  3.87e-2     5              55    61.7  31.6    6.7
#&gt; 5 Colo…  0.0452 2.95e-2     7               9    48.2  43.3    8.6
#&gt; # … with 42 more rows, and 4 more variables: sigma &lt;dbl&gt;, B &lt;dbl&gt;,
#&gt; #   posterior_mean &lt;dbl&gt;, posterior_se &lt;dbl&gt;</code></pre>
<p>The estimates based on posterior do move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results:</p>
<p><img src="book_files/figure-html/posterior-versus-original-estimates-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep track of the total number of electoral votes for Clinton. Remember that Trump gets 270 minus the votes for Clinton. Also note that the reason we add 7 in the code is to account for Rhode Island and D.C.:</p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="models.html#cb623-1"></a>B &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb623-2"><a href="models.html#cb623-2"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb623-3"><a href="models.html#cb623-3"></a>tau &lt;-<span class="st"> </span><span class="fl">0.02</span></span>
<span id="cb623-4"><a href="models.html#cb623-4"></a>clinton_EV &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, {</span>
<span id="cb623-5"><a href="models.html#cb623-5"></a>  results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sigma =</span> sd<span class="op">/</span><span class="kw">sqrt</span>(n), </span>
<span id="cb623-6"><a href="models.html#cb623-6"></a>                   <span class="dt">B =</span> sigma<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>tau<span class="op">^</span><span class="dv">2</span>),</span>
<span id="cb623-7"><a href="models.html#cb623-7"></a>                   <span class="dt">posterior_mean =</span> B <span class="op">*</span><span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B) <span class="op">*</span><span class="st"> </span>avg,</span>
<span id="cb623-8"><a href="models.html#cb623-8"></a>                   <span class="dt">posterior_se =</span> <span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>tau<span class="op">^</span><span class="dv">2</span>)),</span>
<span id="cb623-9"><a href="models.html#cb623-9"></a>                   <span class="dt">result =</span> <span class="kw">rnorm</span>(<span class="kw">length</span>(posterior_mean), </span>
<span id="cb623-10"><a href="models.html#cb623-10"></a>                                  posterior_mean, posterior_se),</span>
<span id="cb623-11"><a href="models.html#cb623-11"></a>                   <span class="dt">clinton =</span> <span class="kw">ifelse</span>(result <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, electoral_votes, <span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb623-12"><a href="models.html#cb623-12"></a><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">clinton =</span> <span class="kw">sum</span>(clinton)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb623-13"><a href="models.html#cb623-13"></a><span class="st">    </span><span class="kw">pull</span>(clinton) <span class="op">+</span><span class="st"> </span><span class="dv">7</span></span>
<span id="cb623-14"><a href="models.html#cb623-14"></a>})</span>
<span id="cb623-15"><a href="models.html#cb623-15"></a></span>
<span id="cb623-16"><a href="models.html#cb623-16"></a><span class="kw">mean</span>(clinton_EV <span class="op">&gt;</span><span class="st"> </span><span class="dv">269</span>)</span>
<span id="cb623-17"><a href="models.html#cb623-17"></a><span class="co">#&gt; [1] 0.998</span></span></code></pre></div>
<p>This model gives Clinton over 99% chance of winning.
<!--Here is a histogram of the Monte Carlo outcomes:

<img src="book_files/figure-html/election-forecast-posterior-no-bias-1.png" width="70%" style="display: block; margin: auto;" />
-->
A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened?</p>
<p>The model above ignores the general bias and assumes the results from different states are independent. After the election, we realized that the general bias in 2016 was not that big: it was between 1 and 2%. But because the election was close in several big states and these states had a large number of polls, pollsters that ignored the general bias greatly underestimated the standard error. Using the notation we introduce, they assumed the standard error was <span class="math inline">\(\sqrt{\sigma^2/N}\)</span> which with large N is quite smaller than the more accurate estimate
<span class="math inline">\(\sqrt{\sigma^2/N + \sigma_b^2}\)</span>. FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at <span class="math inline">\(\sigma_b = 0.03\)</span>:</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="models.html#cb624-1"></a>tau &lt;-<span class="st"> </span><span class="fl">0.02</span></span>
<span id="cb624-2"><a href="models.html#cb624-2"></a>bias_sd &lt;-<span class="st"> </span><span class="fl">0.03</span></span>
<span id="cb624-3"><a href="models.html#cb624-3"></a>clinton_EV_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">1000</span>, {</span>
<span id="cb624-4"><a href="models.html#cb624-4"></a>  results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sigma =</span> <span class="kw">sqrt</span>(sd<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n  <span class="op">+</span><span class="st"> </span>bias_sd<span class="op">^</span><span class="dv">2</span>),  </span>
<span id="cb624-5"><a href="models.html#cb624-5"></a>                   <span class="dt">B =</span> sigma<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>tau<span class="op">^</span><span class="dv">2</span>),</span>
<span id="cb624-6"><a href="models.html#cb624-6"></a>                   <span class="dt">posterior_mean =</span> B<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>B)<span class="op">*</span>avg,</span>
<span id="cb624-7"><a href="models.html#cb624-7"></a>                   <span class="dt">posterior_se =</span> <span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>tau<span class="op">^</span><span class="dv">2</span>)),</span>
<span id="cb624-8"><a href="models.html#cb624-8"></a>                   <span class="dt">result =</span> <span class="kw">rnorm</span>(<span class="kw">length</span>(posterior_mean), </span>
<span id="cb624-9"><a href="models.html#cb624-9"></a>                                  posterior_mean, posterior_se),</span>
<span id="cb624-10"><a href="models.html#cb624-10"></a>                   <span class="dt">clinton =</span> <span class="kw">ifelse</span>(result<span class="op">&gt;</span><span class="dv">0</span>, electoral_votes, <span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb624-11"><a href="models.html#cb624-11"></a><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">clinton =</span> <span class="kw">sum</span>(clinton) <span class="op">+</span><span class="st"> </span><span class="dv">7</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb624-12"><a href="models.html#cb624-12"></a><span class="st">    </span><span class="kw">pull</span>(clinton)</span>
<span id="cb624-13"><a href="models.html#cb624-13"></a>})</span>
<span id="cb624-14"><a href="models.html#cb624-14"></a><span class="kw">mean</span>(clinton_EV_<span class="dv">2</span> <span class="op">&gt;</span><span class="st"> </span><span class="dv">269</span>)</span>
<span id="cb624-15"><a href="models.html#cb624-15"></a><span class="co">#&gt; [1] 0.848</span></span></code></pre></div>
<p>This gives us a much more sensible estimate. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results.</p>
<p><img src="book_files/figure-html/comparison-forecast-with-and-without-bias-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>FiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. One way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution. FiveThirtyEight predicted a probability of 71%.</p>
</div>
<div id="forecasting" class="section level3">
<h3><span class="header-section-number">16.8.5</span> Forecasting</h3>
<p>Forecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election about the actual election? Here we study the variability of poll results across time.</p>
<p>To make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="models.html#cb625-1"></a>one_pollster &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb625-2"><a href="models.html#cb625-2"></a><span class="st">  </span><span class="kw">filter</span>(pollster <span class="op">==</span><span class="st"> &quot;Ipsos&quot;</span> <span class="op">&amp;</span><span class="st"> </span>state <span class="op">==</span><span class="st"> &quot;U.S.&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb625-3"><a href="models.html#cb625-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>)</span></code></pre></div>
<p>Since there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here:</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="models.html#cb626-1"></a>se &lt;-<span class="st"> </span>one_pollster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb626-2"><a href="models.html#cb626-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">empirical =</span> <span class="kw">sd</span>(spread), </span>
<span id="cb626-3"><a href="models.html#cb626-3"></a>            <span class="dt">theoretical =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(spread) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(spread)) <span class="op">/</span></span>
<span id="cb626-4"><a href="models.html#cb626-4"></a><span class="st">                                     </span><span class="kw">min</span>(samplesize)))</span>
<span id="cb626-5"><a href="models.html#cb626-5"></a>se</span>
<span id="cb626-6"><a href="models.html#cb626-6"></a><span class="co">#&gt;   empirical theoretical</span></span>
<span id="cb626-7"><a href="models.html#cb626-7"></a><span class="co">#&gt; 1    0.0403      0.0326</span></span></code></pre></div>
<p>But the empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict:</p>
<p><img src="book_files/figure-html/time-trend-variability-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes <span class="math inline">\(p\)</span> is fixed:</p>
<pre><code>#&gt; `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="book_files/figure-html/time-trend-estimate-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Some of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see the peaks and valleys are consistent across several pollsters:</p>
<pre><code>#&gt; `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="book_files/figure-html/time-trend-estimate-several-pollsters-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This implies that, if we are going to forecast, our model must include a term to accounts for the time effect. We need to write a model including a bias term for time:</p>
<p><span class="math display">\[
Y_{i,j,t} = d + b + h_j + b_t + \varepsilon_{i,j,t}
\]</span></p>
<p>The standard deviation of <span class="math inline">\(b_t\)</span> would depend on <span class="math inline">\(t\)</span> since the closer we get to election day, the closer to 0 this bias term should be.</p>
<p>Pollsters also try to estimate trends from these data and incorporate these into their predictions. We can model the time trend with a function <span class="math inline">\(f(t)\)</span> and rewrite the model like this:
The blue lines in the plots above:</p>
<p><span class="math display">\[
Y_{i,j,t} = d + b + h_j + b_t + f(t) + \varepsilon_{i,jt,}
\]</span></p>
<p>We usually see the estimated <span class="math inline">\(f(t)\)</span> not for the difference, but for the actual percentages for each candidate like this:</p>
<p><img src="book_files/figure-html/trend-estimate-for-all-pollsters-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Once a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There is a variety of methods for estimating trends <span class="math inline">\(f(t)\)</span> which we discuss in the Machine Learning part.</p>
</div>
</div>
<div id="exercises-32" class="section level2">
<h2><span class="header-section-number">16.9</span> Exercises</h2>
<p>1. Create this table:</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="models.html#cb629-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb629-2"><a href="models.html#cb629-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb629-3"><a href="models.html#cb629-3"></a><span class="kw">data</span>(<span class="st">&quot;polls_us_election_2016&quot;</span>)</span>
<span id="cb629-4"><a href="models.html#cb629-4"></a>polls &lt;-<span class="st"> </span>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb629-5"><a href="models.html#cb629-5"></a><span class="st">  </span><span class="kw">filter</span>(state <span class="op">!=</span><span class="st"> &quot;U.S.&quot;</span> <span class="op">&amp;</span><span class="st"> </span>enddate <span class="op">&gt;=</span><span class="st"> &quot;2016-10-31&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb629-6"><a href="models.html#cb629-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>)</span></code></pre></div>
<p>Now for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the <code>select</code> function to keep the columns <code>state, startdate, end date, pollster, grade, spread, lower, upper</code>.</p>
<p>2. You can add the final result to the <code>cis</code> table you just created using the <code>right_join</code> function like this:</p>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb630-1"><a href="models.html#cb630-1"></a>add &lt;-<span class="st"> </span>results_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb630-2"><a href="models.html#cb630-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">actual_spread =</span> clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>trump<span class="op">/</span><span class="dv">100</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb630-3"><a href="models.html#cb630-3"></a><span class="st">  </span><span class="kw">select</span>(state, actual_spread)</span>
<span id="cb630-4"><a href="models.html#cb630-4"></a>cis &lt;-<span class="st"> </span>cis <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb630-5"><a href="models.html#cb630-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">as.character</span>(state)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb630-6"><a href="models.html#cb630-6"></a><span class="st">  </span><span class="kw">left_join</span>(add, <span class="dt">by =</span> <span class="st">&quot;state&quot;</span>)</span></code></pre></div>
<p>Now determine how often the 95% confidence interval includes the actual result.</p>
<p>3. Repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use <code>n=n(), grade = grade[1]</code> in the call to summarize.</p>
<p>4. Repeat exercise 3, but instead of pollster, stratify by state. Note that here we can’t show grades.</p>
<p>5. Make a barplot based on the result of exercise 4. Use <code>coord_flip</code>.</p>
<p>6. Add two columns to the <code>cis</code> table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column <code>hit</code> that is true if the signs are the same. Hint: use the function <code>sign</code>. Call the object <code>resids</code>.</p>
<p>7. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed.</p>
<p>8. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors?</p>
<p>9. We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use <code>filter(grade %in% c("A+","A","A-","B+") | is.na(grade)))</code> to only include pollsters with high grades.</p>
<p>10. Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use <code>group_by</code>, <code>filter</code> then <code>ungroup</code>. You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Note that some pollsters may now be modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models.</p>

</div>
<div id="t-dist" class="section level2">
<h2><span class="header-section-number">16.10</span> The t-distribution</h2>
<p>Above we made use of the CLT with a sample size of 15. Because we are estimating a second parameters <span class="math inline">\(\sigma\)</span>, further variability is introduced into our confidence interval which results in intervals that are too small. For very large sample sizes this extra variability is negligible, but, in general, for values smaller than 30 we need to be cautious about using the CLT.</p>
<p>However, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of <span class="math inline">\(\sigma\)</span>. Using this theory, we can construct confidence intervals for any <span class="math inline">\(N\)</span>. But again, this works only if <strong>the data in the urn is known to follow a normal distribution</strong>. So for the 0, 1 data of our previous urn model, this theory definitely does not apply.</p>
<p>The statistic on which confidence intervals for <span class="math inline">\(d\)</span> are based is</p>
<p><span class="math display">\[
Z = \frac{\bar{X} - d}{\sigma/\sqrt{N}}
\]</span></p>
<p>CLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know <span class="math inline">\(\sigma\)</span> so we use:</p>
<p><span class="math display">\[
Z = \frac{\bar{X} - d}{s/\sqrt{N}}
\]</span></p>
<p>By substituting <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(s\)</span> we introduce some variability. The theory tells us that <span class="math inline">\(Z\)</span> follows a t-distribution with <span class="math inline">\(N-1\)</span> <em>degrees of freedom</em>. The degrees of freedom is a parameter that controls the variability via fatter tails:</p>
<p><img src="book_files/figure-html/t-distribution-examples-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>If we are willing to assume the pollster effect data is normally distributed, based on the sample data <span class="math inline">\(X_1, \dots, X_N\)</span>,</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="models.html#cb631-1"></a>one_poll_per_pollster <span class="op">%&gt;%</span></span>
<span id="cb631-2"><a href="models.html#cb631-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">sample=</span>spread)) <span class="op">+</span><span class="st"> </span><span class="kw">stat_qq</span>()</span></code></pre></div>
<p><img src="book_files/figure-html/poll-spread-qq-1.png" width="70%" style="display: block; margin: auto;" />
then <span class="math inline">\(Z\)</span> follows a t-distribution with <span class="math inline">\(N-1\)</span> degrees of freedom. So perhaps a better confidence interval for <span class="math inline">\(d\)</span> is:</p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="models.html#cb632-1"></a>z &lt;-<span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>,  <span class="kw">nrow</span>(one_poll_per_pollster)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb632-2"><a href="models.html#cb632-2"></a>one_poll_per_pollster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb632-3"><a href="models.html#cb632-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(spread), <span class="dt">moe =</span> z<span class="op">*</span><span class="kw">sd</span>(spread)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">length</span>(spread))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb632-4"><a href="models.html#cb632-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">start =</span> avg <span class="op">-</span><span class="st"> </span>moe, <span class="dt">end =</span> avg <span class="op">+</span><span class="st"> </span>moe) </span>
<span id="cb632-5"><a href="models.html#cb632-5"></a><span class="co">#&gt; # A tibble: 1 x 4</span></span>
<span id="cb632-6"><a href="models.html#cb632-6"></a><span class="co">#&gt;      avg    moe  start    end</span></span>
<span id="cb632-7"><a href="models.html#cb632-7"></a><span class="co">#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;</span></span>
<span id="cb632-8"><a href="models.html#cb632-8"></a><span class="co">#&gt; 1 0.0290 0.0134 0.0156 0.0424</span></span></code></pre></div>
<p>A bit larger than the one using normal is</p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="models.html#cb633-1"></a><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dv">14</span>)</span>
<span id="cb633-2"><a href="models.html#cb633-2"></a><span class="co">#&gt; [1] 2.14</span></span></code></pre></div>
<p>is bigger than</p>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="models.html#cb634-1"></a><span class="kw">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb634-2"><a href="models.html#cb634-2"></a><span class="co">#&gt; [1] 1.96</span></span></code></pre></div>
<p>The t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously saw. Fivethirtyeight uses the t-distribution to generate errors that better model the deviations we see in election data. For example, in Wisconsin the average of six polls was 7% in favor of Clinton with a standard deviation of 1%, but Trump won by 0.7%. Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="models.html#cb635-1"></a><span class="kw">data</span>(<span class="st">&quot;polls_us_election_2016&quot;</span>)</span>
<span id="cb635-2"><a href="models.html#cb635-2"></a>polls_us_election_<span class="dv">2016</span> <span class="op">%&gt;%</span></span>
<span id="cb635-3"><a href="models.html#cb635-3"></a><span class="st">  </span><span class="kw">filter</span>(state <span class="op">==</span><span class="st">&quot;Wisconsin&quot;</span> <span class="op">&amp;</span></span>
<span id="cb635-4"><a href="models.html#cb635-4"></a><span class="st">           </span>enddate <span class="op">&gt;=</span><span class="st">&quot;2016-10-31&quot;</span> <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb635-5"><a href="models.html#cb635-5"></a><span class="st">           </span>(grade <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A+&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;A-&quot;</span>,<span class="st">&quot;B+&quot;</span>) <span class="op">|</span><span class="st"> </span><span class="kw">is.na</span>(grade))) <span class="op">%&gt;%</span></span>
<span id="cb635-6"><a href="models.html#cb635-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">spread =</span> rawpoll_clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>rawpoll_trump<span class="op">/</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb635-7"><a href="models.html#cb635-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">as.character</span>(state)) <span class="op">%&gt;%</span></span>
<span id="cb635-8"><a href="models.html#cb635-8"></a><span class="st">  </span><span class="kw">left_join</span>(results_us_election_<span class="dv">2016</span>, <span class="dt">by =</span> <span class="st">&quot;state&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb635-9"><a href="models.html#cb635-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">actual =</span> clinton<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span>trump<span class="op">/</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb635-10"><a href="models.html#cb635-10"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">actual =</span> <span class="kw">first</span>(actual), <span class="dt">avg =</span> <span class="kw">mean</span>(spread), </span>
<span id="cb635-11"><a href="models.html#cb635-11"></a>            <span class="dt">sd =</span> <span class="kw">sd</span>(spread), <span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb635-12"><a href="models.html#cb635-12"></a><span class="st">  </span><span class="kw">select</span>(actual, avg, sd, n)</span>
<span id="cb635-13"><a href="models.html#cb635-13"></a><span class="co">#&gt;   actual    avg     sd n</span></span>
<span id="cb635-14"><a href="models.html#cb635-14"></a><span class="co">#&gt; 1 -0.007 0.0711 0.0104 6</span></span></code></pre></div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="58">
<li id="fn58"><p><a href="https://www.youtube.com/watch?v=TbKkjm-gheY" class="uri">https://www.youtube.com/watch?v=TbKkjm-gheY</a><a href="models.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p><a href="https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html" class="uri">https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html</a><a href="models.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p><a href="https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/" class="uri">https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/</a><a href="models.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p><a href="https://projects.fivethirtyeight.com/2016-election-forecast/" class="uri">https://projects.fivethirtyeight.com/2016-election-forecast/</a><a href="models.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p><a href="https://en.wikipedia.org/wiki/Sally_Clark" class="uri">https://en.wikipedia.org/wiki/Sally_Clark</a><a href="models.html#fnref62" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/inference/models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
