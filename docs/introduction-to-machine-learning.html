<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 27 Introduction to machine learning | Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.20.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 27 Introduction to machine learning | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 27 Introduction to machine learning | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2020-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="text-mining.html"/>
<link rel="next" href="smoothing.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i>Case studies</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who will find this book useful?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i>What does this book cover?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i>What is not covered by this book?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started with R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>1.2</b> The R console</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> Scripts</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>1.4.1</b> The panes</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>1.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>1.5</b> Installing R packages</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> R basics</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>2.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>2.2</b> The very basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>2.2.2</b> The workspace</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>2.2.3</b> Functions</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>2.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>2.2.5</b> Variable names</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>2.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>2.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>2.2.8</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>2.4</b> Data types</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> Data frames</a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>2.4.2</b> Examining an object</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>2.4.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>2.4.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>2.4.6</b> Lists</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectors</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>2.6.1</b> Creating vectors</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>2.6.2</b> Names</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>2.6.3</b> Sequences</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>2.6.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>2.7</b> Coercion</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>2.7.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> Sorting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>2.9.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>2.11</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>2.11.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>2.11.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>2.12</b> Exercises</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>2.13</b> Indexing</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>2.13.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>2.13.2</b> Logical operators</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>2.14</b> Exercises</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>2.15</b> Basic plots</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#exercises-6"><i class="fa fa-check"></i><b>2.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>3</b> Programming basics</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="3.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>3.2</b> Defining functions</a></li>
<li class="chapter" data-level="3.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> Namespaces</a></li>
<li class="chapter" data-level="3.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>3.4</b> For-loops</a></li>
<li class="chapter" data-level="3.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="3.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-7"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> The tidyverse</a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Tidy data</a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#exercises-8"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#manipulating-data-frames"><i class="fa fa-check"></i><b>4.3</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#subsetting-with-filter"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>4.3.3</b> Selecting columns with <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#exercises-9"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>4.5</b> The pipe: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#exercises-10"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#summarizing-data"><i class="fa fa-check"></i><b>4.7</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Group then summarize with <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#sorting-data-frames"><i class="fa fa-check"></i><b>4.8</b> Sorting data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#nested-sorting"><i class="fa fa-check"></i><b>4.8.1</b> Nested sorting</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#the-top-n"><i class="fa fa-check"></i><b>4.8.2</b> The top <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#exercises-11"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> Tibbles</a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>4.10.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>4.10.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>4.10.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#create-a-tibble-using-tibble-instead-of-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Create a tibble using <code>tibble</code> instead of <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>4.11</b> The dot operator</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>4.13</b> The <strong>purrr</strong> package</a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-conditionals"><i class="fa fa-check"></i><b>4.14</b> Tidyverse conditionals</a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#exercises-12"><i class="fa fa-check"></i><b>4.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importing data</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>5.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>5.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>5.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>5.1.3</b> The working directory</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>5.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>5.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>5.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#exercises-13"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>5.4</b> Downloading files</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>5.5</b> R-base importing functions</a></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>5.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>5.8</b> Organizing data with spreadsheets</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#exercises-14"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>6</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>7.1</b> The components of a graph</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects"><i class="fa fa-check"></i><b>7.2</b> <code>ggplot</code> objects</a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>7.3</b> Geometries</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>7.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>7.5</b> Layers</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>7.5.1</b> Tinkering with arguments</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>7.6</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>7.7</b> Scales</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>7.8</b> Labels and titles</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>7.9</b> Categories as colors</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>7.10</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Add-on packages</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>7.12</b> Putting it all together</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>7.14</b> Grids of plots</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#exercises-15"><i class="fa fa-check"></i><b>7.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Visualizing data distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>8.1</b> Variable types</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#case-study-describing-student-heights"><i class="fa fa-check"></i><b>8.2</b> Case study: describing student heights</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>8.3</b> Distribution function</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>8.5</b> Histograms</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>8.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>8.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>8.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#exercises-16"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> The normal distribution</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#standard-units"><i class="fa fa-check"></i><b>8.9</b> Standard units</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>8.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>8.12</b> Boxplots</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Stratification</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Case study: describing student heights (continued)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#exercises-17"><i class="fa fa-check"></i><b>8.15</b> Exercises</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#barplots"><i class="fa fa-check"></i><b>8.16.1</b> Barplots</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histograms-1"><i class="fa fa-check"></i><b>8.16.2</b> Histograms</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#density-plots"><i class="fa fa-check"></i><b>8.16.3</b> Density plots</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#boxplots-1"><i class="fa fa-check"></i><b>8.16.4</b> Boxplots</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#qq-plots"><i class="fa fa-check"></i><b>8.16.5</b> QQ-plots</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#images"><i class="fa fa-check"></i><b>8.16.6</b> Images</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#quick-plots"><i class="fa fa-check"></i><b>8.16.7</b> Quick plots</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#exercises-18"><i class="fa fa-check"></i><b>8.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#case-study-new-insights-on-poverty"><i class="fa fa-check"></i><b>9.1</b> Case study: new insights on poverty</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>9.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>9.2</b> Scatterplots</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>9.3</b> Faceting</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>9.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>9.4</b> Time series plots</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>9.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#data-transformations"><i class="fa fa-check"></i><b>9.5</b> Data transformations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>9.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>9.5.2</b> Which base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>9.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#visualizing-multimodal-distributions"><i class="fa fa-check"></i><b>9.6</b> Visualizing multimodal distributions</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#comparing-multiple-distributions-with-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>9.7</b> Comparing multiple distributions with boxplots and ridge plots</a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-2"><i class="fa fa-check"></i><b>9.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>9.7.2</b> Ridge plots</a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>9.7.3</b> Example: 1970 versus 2010 income distributions</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>9.7.4</b> Accessing computed variables</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>9.7.5</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#the-ecological-fallacy-and-importance-of-showing-the-data"><i class="fa fa-check"></i><b>9.8</b> The ecological fallacy and importance of showing the data</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>9.8.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>10</b> Data visualization principles</a><ul>
<li class="chapter" data-level="10.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>10.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="10.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>10.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="10.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>10.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="10.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-categories-by-a-meaningful-value"><i class="fa fa-check"></i><b>10.4</b> Order categories by a meaningful value</a></li>
<li class="chapter" data-level="10.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>10.5</b> Show the data</a></li>
<li class="chapter" data-level="10.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons"><i class="fa fa-check"></i><b>10.6</b> Ease comparisons</a><ul>
<li class="chapter" data-level="10.6.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-common-axes"><i class="fa fa-check"></i><b>10.6.1</b> Use common axes</a></li>
<li class="chapter" data-level="10.6.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>10.6.2</b> Align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="10.6.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>10.6.3</b> Consider transformations</a></li>
<li class="chapter" data-level="10.6.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>10.6.4</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="10.6.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>10.7</b> Think of the color blind</a></li>
<li class="chapter" data-level="10.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#plots-for-two-variables"><i class="fa fa-check"></i><b>10.8</b> Plots for two variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>10.8.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>10.9</b> Encoding a third variable</a></li>
<li class="chapter" data-level="10.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>10.10</b> Avoid pseudo-three-dimensional plots</a></li>
<li class="chapter" data-level="10.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>10.11</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="10.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>10.12</b> Know your audience</a></li>
<li class="chapter" data-level="10.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-19"><i class="fa fa-check"></i><b>10.13</b> Exercises</a></li>
<li class="chapter" data-level="10.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Case study: vaccines and infectious diseases</a></li>
<li class="chapter" data-level="10.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-20"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Robust summaries</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>11.1</b> Outliers</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>11.2</b> Median</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>11.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>11.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>11.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-21"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>11.7</b> Case study: self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Statistics with R</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>12</b> Introduction to statistics with R</a></li>
<li class="chapter" data-level="13" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>13</b> Probability</a><ul>
<li class="chapter" data-level="13.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>13.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>13.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="13.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>13.1.2</b> Notation</a></li>
<li class="chapter" data-level="13.1.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>13.1.3</b> Probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-categorical-data"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo simulations for categorical data</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>13.2.1</b> Setting the random seed</a></li>
<li class="chapter" data-level="13.2.2" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i><b>13.2.2</b> With and without replacement</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>13.3</b> Independence</a></li>
<li class="chapter" data-level="13.4" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>13.4</b> Conditional probabilities</a></li>
<li class="chapter" data-level="13.5" data-path="probability.html"><a href="probability.html#addition-and-multiplication-rules"><i class="fa fa-check"></i><b>13.5</b> Addition and multiplication rules</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>13.5.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="13.5.2" data-path="probability.html"><a href="probability.html#multiplication-rule-under-independence"><i class="fa fa-check"></i><b>13.5.2</b> Multiplication rule under independence</a></li>
<li class="chapter" data-level="13.5.3" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>13.5.3</b> Addition rule</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>13.6</b> Combinations and permutations</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>13.6.1</b> Monte Carlo example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probability.html"><a href="probability.html#examples"><i class="fa fa-check"></i><b>13.7</b> Examples</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probability.html"><a href="probability.html#monty-hall-problem"><i class="fa fa-check"></i><b>13.7.1</b> Monty Hall problem</a></li>
<li class="chapter" data-level="13.7.2" data-path="probability.html"><a href="probability.html#birthday-problem"><i class="fa fa-check"></i><b>13.7.2</b> Birthday problem</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probability.html"><a href="probability.html#infinity-in-practice"><i class="fa fa-check"></i><b>13.8</b> Infinity in practice</a></li>
<li class="chapter" data-level="13.9" data-path="probability.html"><a href="probability.html#exercises-22"><i class="fa fa-check"></i><b>13.9</b> Exercises</a></li>
<li class="chapter" data-level="13.10" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>13.10</b> Continuous probability</a></li>
<li class="chapter" data-level="13.11" data-path="probability.html"><a href="probability.html#theoretical-continuous-distributions"><i class="fa fa-check"></i><b>13.11</b> Theoretical continuous distributions</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>13.11.1</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="13.11.2" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>13.11.2</b> The probability density</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>13.12</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="13.13" data-path="probability.html"><a href="probability.html#continuous-distributions"><i class="fa fa-check"></i><b>13.13</b> Continuous distributions</a></li>
<li class="chapter" data-level="13.14" data-path="probability.html"><a href="probability.html#exercises-23"><i class="fa fa-check"></i><b>13.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>14</b> Random variables</a><ul>
<li class="chapter" data-level="14.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>14.1</b> Random variables</a></li>
<li class="chapter" data-level="14.2" data-path="random-variables.html"><a href="random-variables.html#sampling-models"><i class="fa fa-check"></i><b>14.2</b> Sampling models</a></li>
<li class="chapter" data-level="14.3" data-path="random-variables.html"><a href="random-variables.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>14.3</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="14.4" data-path="random-variables.html"><a href="random-variables.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>14.4</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="14.5" data-path="random-variables.html"><a href="random-variables.html#notation-for-random-variables"><i class="fa fa-check"></i><b>14.5</b> Notation for random variables</a></li>
<li class="chapter" data-level="14.6" data-path="random-variables.html"><a href="random-variables.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>14.6</b> The expected value and standard error</a><ul>
<li class="chapter" data-level="14.6.1" data-path="random-variables.html"><a href="random-variables.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>14.6.1</b> Population SD versus the sample SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="random-variables.html"><a href="random-variables.html#central-limit-theorem"><i class="fa fa-check"></i><b>14.7</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="14.7.1" data-path="random-variables.html"><a href="random-variables.html#how-large-is-large-in-the-central-limit-theorem"><i class="fa fa-check"></i><b>14.7.1</b> How large is large in the Central Limit Theorem?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="random-variables.html"><a href="random-variables.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>14.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="14.9" data-path="random-variables.html"><a href="random-variables.html#law-of-large-numbers"><i class="fa fa-check"></i><b>14.9</b> Law of large numbers</a><ul>
<li class="chapter" data-level="14.9.1" data-path="random-variables.html"><a href="random-variables.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>14.9.1</b> Misinterpreting law of averages</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="random-variables.html"><a href="random-variables.html#exercises-24"><i class="fa fa-check"></i><b>14.10</b> Exercises</a></li>
<li class="chapter" data-level="14.11" data-path="random-variables.html"><a href="random-variables.html#case-study-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="random-variables.html"><a href="random-variables.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>14.11.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="14.11.2" data-path="random-variables.html"><a href="random-variables.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="random-variables.html"><a href="random-variables.html#exercises-25"><i class="fa fa-check"></i><b>14.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Statistical inference</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#polls"><i class="fa fa-check"></i><b>15.1</b> Polls</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>15.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>15.2</b> Populations, samples, parameters, and estimates</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#the-sample-average"><i class="fa fa-check"></i><b>15.2.1</b> The sample average</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parameters"><i class="fa fa-check"></i><b>15.2.2</b> Parameters</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>15.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>15.2.4</b> Properties of our estimate: expected value and standard error</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#exercises-26"><i class="fa fa-check"></i><b>15.3</b> Exercises</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>15.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#the-spread"><i class="fa fa-check"></i><b>15.4.2</b> The spread</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>15.4.3</b> Bias: why not run a very large poll?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#exercises-27"><i class="fa fa-check"></i><b>15.5</b> Exercises</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>15.6</b> Confidence intervals</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>15.6.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#the-correct-language"><i class="fa fa-check"></i><b>15.6.2</b> The correct language</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#exercises-28"><i class="fa fa-check"></i><b>15.7</b> Exercises</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#power"><i class="fa fa-check"></i><b>15.8</b> Power</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>15.9</b> p-values</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Association tests</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>15.10.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#chi-square-test"><i class="fa fa-check"></i><b>15.10.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> The odds ratio</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>15.10.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#small-count-correction"><i class="fa fa-check"></i><b>15.10.6</b> Small count correction</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>15.10.7</b> Large samples, small p-values</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#exercises-29"><i class="fa fa-check"></i><b>15.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Statistical models</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#poll-aggregators"><i class="fa fa-check"></i><b>16.1</b> Poll aggregators</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#poll-data"><i class="fa fa-check"></i><b>16.1.1</b> Poll data</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#pollster-bias"><i class="fa fa-check"></i><b>16.1.2</b> Pollster bias</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Data-driven models</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#exercises-30"><i class="fa fa-check"></i><b>16.3</b> Exercises</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#bayes-theorem"><i class="fa fa-check"></i><b>16.4.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>16.5</b> Bayes theorem simulation</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-in-practice"><i class="fa fa-check"></i><b>16.5.1</b> Bayes in practice</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#hierarchical-models"><i class="fa fa-check"></i><b>16.6</b> Hierarchical models</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#exercises-31"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Case study: election forecasting</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#the-general-bias"><i class="fa fa-check"></i><b>16.8.2</b> The general bias</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>16.8.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>16.8.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#forecasting"><i class="fa fa-check"></i><b>16.8.5</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#exercises-32"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>17.1</b> Case study: is height hereditary?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>17.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>17.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Conditional expectations</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>17.4</b> The regression line</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>17.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>17.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>17.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>17.4.4</b> Warning: there are two regression lines</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#exercises-33"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>18</b> Linear models</a><ul>
<li class="chapter" data-level="18.1" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball"><i class="fa fa-check"></i><b>18.1</b> Case study: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="linear-models.html"><a href="linear-models.html#sabermetics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetics</a></li>
<li class="chapter" data-level="18.1.2" data-path="linear-models.html"><a href="linear-models.html#baseball-basics"><i class="fa fa-check"></i><b>18.1.2</b> Baseball basics</a></li>
<li class="chapter" data-level="18.1.3" data-path="linear-models.html"><a href="linear-models.html#no-awards-for-bb"><i class="fa fa-check"></i><b>18.1.3</b> No awards for BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="linear-models.html"><a href="linear-models.html#base-on-balls-or-stolen-bases"><i class="fa fa-check"></i><b>18.1.4</b> Base on balls or stolen bases?</a></li>
<li class="chapter" data-level="18.1.5" data-path="linear-models.html"><a href="linear-models.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>18.1.5</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="linear-models.html"><a href="linear-models.html#confounding"><i class="fa fa-check"></i><b>18.2</b> Confounding</a><ul>
<li class="chapter" data-level="18.2.1" data-path="linear-models.html"><a href="linear-models.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>18.2.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="18.2.2" data-path="linear-models.html"><a href="linear-models.html#multivariate-regression"><i class="fa fa-check"></i><b>18.2.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="linear-models.html"><a href="linear-models.html#lse"><i class="fa fa-check"></i><b>18.3</b> Least squares estimates</a><ul>
<li class="chapter" data-level="18.3.1" data-path="linear-models.html"><a href="linear-models.html#interpreting-linear-models"><i class="fa fa-check"></i><b>18.3.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="18.3.2" data-path="linear-models.html"><a href="linear-models.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>18.3.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="linear-models.html"><a href="linear-models.html#the-lm-function"><i class="fa fa-check"></i><b>18.3.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="18.3.4" data-path="linear-models.html"><a href="linear-models.html#lse-are-random-variables"><i class="fa fa-check"></i><b>18.3.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="18.3.5" data-path="linear-models.html"><a href="linear-models.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>18.3.5</b> Predicted values are random variables</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="linear-models.html"><a href="linear-models.html#exercises-34"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
<li class="chapter" data-level="18.5" data-path="linear-models.html"><a href="linear-models.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="linear-models.html"><a href="linear-models.html#the-broom-package"><i class="fa fa-check"></i><b>18.5.1</b> The broom package</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="linear-models.html"><a href="linear-models.html#exercises-35"><i class="fa fa-check"></i><b>18.6</b> Exercises</a></li>
<li class="chapter" data-level="18.7" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>18.7</b> Case study: Moneyball (continued)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="linear-models.html"><a href="linear-models.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>18.7.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="18.7.2" data-path="linear-models.html"><a href="linear-models.html#picking-nine-players"><i class="fa fa-check"></i><b>18.7.2</b> Picking nine players</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="linear-models.html"><a href="linear-models.html#the-regression-fallacy"><i class="fa fa-check"></i><b>18.8</b> The regression fallacy</a></li>
<li class="chapter" data-level="18.9" data-path="linear-models.html"><a href="linear-models.html#measurement-error-models"><i class="fa fa-check"></i><b>18.9</b> Measurement error models</a></li>
<li class="chapter" data-level="18.10" data-path="linear-models.html"><a href="linear-models.html#exercises-36"><i class="fa fa-check"></i><b>18.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>19</b> Association is not causation</a><ul>
<li class="chapter" data-level="19.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>19.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="19.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>19.2</b> Outliers</a></li>
<li class="chapter" data-level="19.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>19.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="19.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>19.4</b> Confounders</a><ul>
<li class="chapter" data-level="19.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>19.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="19.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>19.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="19.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>19.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>19.5</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="19.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-37"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="20" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>20</b> Introduction to data wrangling</a></li>
<li class="chapter" data-level="21" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>21</b> Reshaping data</a><ul>
<li class="chapter" data-level="21.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-38"><i class="fa fa-check"></i><b>21.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>22</b> Joining tables</a><ul>
<li class="chapter" data-level="22.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>22.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="22.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>22.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>22.3</b> Set operators</a><ul>
<li class="chapter" data-level="22.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>22.3.1</b> Intersect</a></li>
<li class="chapter" data-level="22.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>22.3.2</b> Union</a></li>
<li class="chapter" data-level="22.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-39"><i class="fa fa-check"></i><b>22.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>23</b> Web scraping</a><ul>
<li class="chapter" data-level="23.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>23.2</b> The rvest package</a></li>
<li class="chapter" data-level="23.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> CSS selectors</a></li>
<li class="chapter" data-level="23.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-40"><i class="fa fa-check"></i><b>23.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>24</b> String processing</a><ul>
<li class="chapter" data-level="24.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>24.1</b> The stringr package</a></li>
<li class="chapter" data-level="24.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>24.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="24.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>24.3</b> Case study 2: self-reported heights</a></li>
<li class="chapter" data-level="24.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>24.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="24.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>24.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="24.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>24.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="24.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>24.5.2</b> Special characters</a></li>
<li class="chapter" data-level="24.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>24.5.3</b> Character classes</a></li>
<li class="chapter" data-level="24.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>24.5.4</b> Anchors</a></li>
<li class="chapter" data-level="24.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>24.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="24.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>24.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>24.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>24.5.8</b> Not</a></li>
<li class="chapter" data-level="24.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>24.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="24.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>24.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>24.7</b> Testing and improving</a></li>
<li class="chapter" data-level="24.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>24.8</b> Trimming</a></li>
<li class="chapter" data-level="24.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>24.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="24.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>24.10</b> Case study 2: self-reported heights (continued)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>24.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="24.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>24.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>24.11</b> String splitting</a></li>
<li class="chapter" data-level="24.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>24.12</b> Case study 3: extracting tables from a PDF</a></li>
<li class="chapter" data-level="24.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recoding</a></li>
<li class="chapter" data-level="24.14" data-path="string-processing.html"><a href="string-processing.html#exercises-41"><i class="fa fa-check"></i><b>24.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>25</b> Parsing dates and times</a><ul>
<li class="chapter" data-level="25.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>25.1</b> The date data type</a></li>
<li class="chapter" data-level="25.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> The lubridate package</a></li>
<li class="chapter" data-level="25.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-42"><i class="fa fa-check"></i><b>25.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>26</b> Text mining</a><ul>
<li class="chapter" data-level="26.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>26.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="26.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>26.2</b> Text as data</a></li>
<li class="chapter" data-level="26.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>26.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="26.4" data-path="text-mining.html"><a href="text-mining.html#exercises-43"><i class="fa fa-check"></i><b>26.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introduction to machine learning</a><ul>
<li class="chapter" data-level="27.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>27.1</b> Notation</a></li>
<li class="chapter" data-level="27.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>27.2</b> An example</a></li>
<li class="chapter" data-level="27.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-44"><i class="fa fa-check"></i><b>27.3</b> Exercises</a></li>
<li class="chapter" data-level="27.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>27.4</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>27.4.1</b> Training and test sets</a></li>
<li class="chapter" data-level="27.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>27.4.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="27.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>27.4.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="27.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>27.4.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="27.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>27.4.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="27.4.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>27.4.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="27.4.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>27.4.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="27.4.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-45"><i class="fa fa-check"></i><b>27.5</b> Exercises</a></li>
<li class="chapter" data-level="27.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>27.6</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>27.6.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="27.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>27.6.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="27.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>27.6.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-46"><i class="fa fa-check"></i><b>27.7</b> Exercises</a></li>
<li class="chapter" data-level="27.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Case study: is it a 2 or a 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>28</b> Smoothing</a><ul>
<li class="chapter" data-level="28.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>28.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="28.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>28.3</b> Local weighted regression (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>28.3.1</b> Fitting parabolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="smoothing.html"><a href="smoothing.html#beware-of-default-smoothing-parameters"><i class="fa fa-check"></i><b>28.3.2</b> Beware of default smoothing parameters</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="smoothing.html"><a href="smoothing.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Connecting smoothing to machine learning</a></li>
<li class="chapter" data-level="28.5" data-path="smoothing.html"><a href="smoothing.html#exercises-47"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Cross validation</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#over-training"><i class="fa fa-check"></i><b>29.1.1</b> Over-training</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#over-smoothing"><i class="fa fa-check"></i><b>29.1.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>29.1.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>29.2</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>29.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-48"><i class="fa fa-check"></i><b>29.4</b> Exercises</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#exercises-49"><i class="fa fa-check"></i><b>29.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> The caret package</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>30.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Cross validation</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>30.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>31</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="31.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>31.1</b> Linear regression</a><ul>
<li class="chapter" data-level="31.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>31.1.1</b> The <code>predict</code> function</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-50"><i class="fa fa-check"></i><b>31.2</b> Exercises</a></li>
<li class="chapter" data-level="31.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>31.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="31.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>31.3.1</b> Generalized linear models</a></li>
<li class="chapter" data-level="31.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression-with-more-than-one-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Logistic regression with more than one predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-51"><i class="fa fa-check"></i><b>31.4</b> Exercises</a></li>
<li class="chapter" data-level="31.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="31.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-52"><i class="fa fa-check"></i><b>31.6</b> Exercises</a></li>
<li class="chapter" data-level="31.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>31.7</b> Generative models</a><ul>
<li class="chapter" data-level="31.7.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>31.7.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="31.7.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.3</b> Quadratic discriminant analysis</a></li>
<li class="chapter" data-level="31.7.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>31.7.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="31.7.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>31.7.5</b> Connection to distance</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>31.8</b> Case study: more than three classes</a></li>
<li class="chapter" data-level="31.9" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-53"><i class="fa fa-check"></i><b>31.9</b> Exercises</a></li>
<li class="chapter" data-level="31.10" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>31.10</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>31.10.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="31.10.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>31.10.2</b> CART motivation</a></li>
<li class="chapter" data-level="31.10.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>31.10.3</b> Regression trees</a></li>
<li class="chapter" data-level="31.10.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>31.10.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>31.11</b> Random forests</a></li>
<li class="chapter" data-level="31.12" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-54"><i class="fa fa-check"></i><b>31.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>32</b> Machine learning in practice</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>32.1</b> Preprocessing</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest"><i class="fa fa-check"></i><b>32.2</b> k-nearest neighbor and random forest</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>32.3</b> Variable importance</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>32.4</b> Visual assessments</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>32.5</b> Ensembles</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-55"><i class="fa fa-check"></i><b>32.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>33</b> Large datasets</a><ul>
<li class="chapter" data-level="33.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="33.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>33.1.1</b> Notation</a></li>
<li class="chapter" data-level="33.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>33.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="33.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>33.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="33.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>33.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="33.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>33.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="33.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>33.1.9</b> Matrix algebra operations</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>33.2</b> Exercises</a></li>
<li class="chapter" data-level="33.3" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>33.3</b> Distance</a><ul>
<li class="chapter" data-level="33.3.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>33.3.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="33.3.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>33.3.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="33.3.3" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance-example"><i class="fa fa-check"></i><b>33.3.3</b> Euclidean distance example</a></li>
<li class="chapter" data-level="33.3.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Predictor space</a></li>
<li class="chapter" data-level="33.3.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>33.3.5</b> Distance between predictors</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>33.4</b> Exercises</a></li>
<li class="chapter" data-level="33.5" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>33.5</b> Dimension reduction</a><ul>
<li class="chapter" data-level="33.5.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>33.5.1</b> Preserving distance</a></li>
<li class="chapter" data-level="33.5.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advanced"><i class="fa fa-check"></i><b>33.5.2</b> Linear transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogonal-transformations-advanced"><i class="fa fa-check"></i><b>33.5.3</b> Orthogonal transformations (advanced)</a></li>
<li class="chapter" data-level="33.5.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>33.5.4</b> Principal component analysis</a></li>
<li class="chapter" data-level="33.5.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>33.5.5</b> Iris example</a></li>
<li class="chapter" data-level="33.5.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>33.5.6</b> MNIST example</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>33.6</b> Exercises</a></li>
<li class="chapter" data-level="33.7" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>33.7</b> Recommendation systems</a><ul>
<li class="chapter" data-level="33.7.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>33.7.1</b> Movielens data</a></li>
<li class="chapter" data-level="33.7.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>33.7.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="33.7.3" data-path="large-datasets.html"><a href="large-datasets.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Loss function</a></li>
<li class="chapter" data-level="33.7.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>33.7.4</b> A first model</a></li>
<li class="chapter" data-level="33.7.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>33.7.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="33.7.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>33.7.6</b> User effects</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="large-datasets.html"><a href="large-datasets.html#exercises-59"><i class="fa fa-check"></i><b>33.8</b> Exercises</a></li>
<li class="chapter" data-level="33.9" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>33.9</b> Regularization</a><ul>
<li class="chapter" data-level="33.9.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>33.9.1</b> Motivation</a></li>
<li class="chapter" data-level="33.9.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>33.9.2</b> Penalized least squares</a></li>
<li class="chapter" data-level="33.9.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>33.9.3</b> Choosing the penalty terms</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-60"><i class="fa fa-check"></i><b>33.10</b> Exercises</a></li>
<li class="chapter" data-level="33.11" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>33.11</b> Matrix factorization</a><ul>
<li class="chapter" data-level="33.11.1" data-path="large-datasets.html"><a href="large-datasets.html#factors-analysis"><i class="fa fa-check"></i><b>33.11.1</b> Factors analysis</a></li>
<li class="chapter" data-level="33.11.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>33.11.2</b> Connection to SVD and PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="large-datasets.html"><a href="large-datasets.html#exercises-61"><i class="fa fa-check"></i><b>33.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Clustering</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>34.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>34.3</b> Heatmaps</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>34.4</b> Filtering features</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#exercises-62"><i class="fa fa-check"></i><b>34.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity Tools</b></span></li>
<li class="chapter" data-level="35" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>35</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>36.1</b> Installing R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>36.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html"><i class="fa fa-check"></i><b>37</b> Accessing the terminal and installing Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>37.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>37.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#naming-convention"><i class="fa fa-check"></i><b>38.1</b> Naming convention</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> The terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> The filesystem</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>38.3.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>38.3.2</b> The home directory</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Working directory</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>38.4</b> Unix commands</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>38.5</b> Some examples</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>38.6</b> More Unix commands</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>38.8</b> Advanced Unix</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>38.8.1</b> Arguments</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>38.8.2</b> Getting help</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>38.8.3</b> Pipes</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>38.8.4</b> Wild cards</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>38.8.5</b> Environment variables</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> Shells</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>38.8.7</b> Executables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>38.8.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>38.8.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>38.8.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git and GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>39.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#github-accounts"><i class="fa fa-check"></i><b>39.2</b> GitHub accounts</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> GitHub repositories</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Overview of Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>39.4.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Reproducible projects with RStudio and R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#rstudio-projects"><i class="fa fa-check"></i><b>40.1</b> RStudio projects</a></li>
<li class="chapter" data-level="40.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#the-header"><i class="fa fa-check"></i><b>40.2.1</b> The header</a></li>
<li class="chapter" data-level="40.2.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>40.2.2</b> R code chunks</a></li>
<li class="chapter" data-level="40.2.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#global-options"><i class="fa fa-check"></i><b>40.2.3</b> Global options</a></li>
<li class="chapter" data-level="40.2.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> More on R markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizing a data science project</a><ul>
<li class="chapter" data-level="40.3.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-directories-in-unix"><i class="fa fa-check"></i><b>40.3.1</b> Create directories in Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-an-rstudio-project"><i class="fa fa-check"></i><b>40.3.2</b> Create an RStudio project</a></li>
<li class="chapter" data-level="40.3.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#edit-some-r-scripts"><i class="fa fa-check"></i><b>40.3.3</b> Edit some R scripts</a></li>
<li class="chapter" data-level="40.3.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-some-more-directories-using-unix"><i class="fa fa-check"></i><b>40.3.4</b> Create some more directories using Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-a-readme-file"><i class="fa fa-check"></i><b>40.3.5</b> Add a README file</a></li>
<li class="chapter" data-level="40.3.6" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#initializing-a-git-directory"><i class="fa fa-check"></i><b>40.3.6</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="40.3.7" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-commit-and-push-files-using-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Add, commit, and push files using RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-machine-learning" class="section level1">
<h1><span class="header-section-number">Chapter 27</span> Introduction to machine learning</h1>
<p>Perhaps the most popular data science methodologies come from the field of <em>machine learning</em>. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars. Although today Artificial Intelligence and machine learning are often used interchangeably, we make the following distinction: while the first artificial intelligence algorithms, such as those used by chess playing machines, implemented decision making based on programmable rules derived from theory or first principles, in machine learning decisions are based on algorithms <strong>built with data</strong>.</p>
<div id="notation-1" class="section level2">
<h2><span class="header-section-number">27.1</span> Notation</h2>
<p>In machine learning, data comes in the form of:</p>
<ol style="list-style-type: decimal">
<li>the <em>outcome</em> we want to predict and</li>
<li>the <em>features</em> that we will use to predict the outcome</li>
</ol>
<p>We want to build an algorithm that takes feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to <em>train</em> an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome.</p>
<p>Here we will use <span class="math inline">\(Y\)</span> to denote the outcome and <span class="math inline">\(X_1, \dots, X_p\)</span> to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms.</p>
<p>Prediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, <span class="math inline">\(Y\)</span> can be any one of <span class="math inline">\(K\)</span> classes. The number of classes can vary greatly across applications.
For example, in the digit reader data, <span class="math inline">\(K=10\)</span> with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the <span class="math inline">\(K\)</span> categories with indexes <span class="math inline">\(k=1,\dots,K\)</span>. However, for binary data we will use <span class="math inline">\(k=0,1\)</span> for mathematical conveniences that we demonstrate later.</p>
<p>The general setup is as follows. We have a series of features and an unknown outcome we want to predict:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
outcome
</th>
<th style="text-align:center;">
feature 1
</th>
<th style="text-align:center;">
feature 2
</th>
<th style="text-align:center;">
feature 3
</th>
<th style="text-align:center;">
feature 4
</th>
<th style="text-align:center;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
?
</td>
<td style="text-align:center;">
<span class="math inline">\(X_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_2\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_4\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_5\)</span>
</td>
</tr>
</tbody>
</table>
<p>To <em>build a model</em> that provides a prediction for any set of observed values <span class="math inline">\(X_1=x_1, X_2=x_2, \dots X_5=x_5\)</span>, we collect data for which we know the outcome:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
outcome
</th>
<th style="text-align:left;">
feature 1
</th>
<th style="text-align:left;">
feature 2
</th>
<th style="text-align:left;">
feature 3
</th>
<th style="text-align:left;">
feature 4
</th>
<th style="text-align:left;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_n\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,5}\)</span>
</td>
</tr>
</tbody>
</table>
<p>When the output is continuous we refer to the machine learning task as <em>prediction</em>, and the main output of the model is a function <span class="math inline">\(f\)</span> that automatically produces a prediction, denoted with <span class="math inline">\(\hat{y}\)</span>, for any set of predictors: <span class="math inline">\(\hat{y} = f(x_1, x_2, \dots, x_p)\)</span>. We use the term <em>actual outcome</em> to denote what we ended up observing. So we want the prediction <span class="math inline">\(\hat{y}\)</span> to match the actual outcome <span class="math inline">\(y\)</span> as well as possible. Because our outcome is continuous, our predictions <span class="math inline">\(\hat{y}\)</span> will not be either exactly right or wrong, but instead we will determine an <em>error</em> defined as the difference between the prediction and the actual outcome <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>When the outcome is categorical, we refer to the machine learning task as <em>classification</em>, and the main output of the model will be a <em>decision rule</em> which prescribes which of the <span class="math inline">\(K\)</span> classes we should predict. In this scenario, most models provide functions of the predictors for each class <span class="math inline">\(k\)</span>, <span class="math inline">\(f_k(x_1, x_2, \dots, x_p)\)</span>, that are used to make this decision. When the data is binary a typical decision rules looks like this: if <span class="math inline">\(f_1(x_1, x_2, \dots, x_p) &gt; C\)</span>, predict category 1, if not the other category, with <span class="math inline">\(C\)</span> a predetermined cutoff. Because the outcomes are categorical, our predictions will be either right or wrong.</p>
<p>Notice that these terms vary among courses, text books, and other publications. Often <em>prediction</em> is used for both categorical and continuous outcomes, and the term <em>regression</em> can be used for the continuous case. Here we avoid using <em>regression</em> to avoid confusion with our previous use of the term <em>linear regression</em>. In most cases it will be clear if our outcomes are categorical or continuous, so we will avoid using these terms when possible.</p>
</div>
<div id="an-example" class="section level2">
<h2><span class="header-section-number">27.2</span> An example</h2>
<p>Let’s consider the zip code reader example. The first step in handling mail received in the post office is sorting letters by zip code:</p>
<p><img src="ml/img/how-to-write-a-address-on-an-envelope-how-to-write-the-address-on-an-envelope-write-address-on-envelope-india-finishedenvelope-x69070.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Originally, humans had to sort these by hand. To do this, they had to read the zip codes on each letter. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. In this part of the book, we will learn how to build algorithms that can read a digit.</p>
<p>The first step in building an algorithm is to understand
what are the outcomes and features. Below are three images of written digits. These have already been read by a human and assigned an outcome <span class="math inline">\(Y\)</span>. These are considered known and serve as the training set.</p>
<p><img src="book_files/figure-html/digit-images-example-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The images are converted into <span class="math inline">\(28 \times 28 = 784\)</span> pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black), which we consider continuous for now. The following plot shows the individual features for each image:</p>
<p><img src="book_files/figure-html/example-images-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>For each digitized image <span class="math inline">\(i\)</span>, we have a categorical outcome <span class="math inline">\(Y_i\)</span> which can be one of 10 values (<span class="math inline">\(0,1,2,3,4,5,6,7,8,9\)</span>), and features <span class="math inline">\(X_{i,1}, \dots, X_{i,784}\)</span>. We use bold face <span class="math inline">\(\mathbf{X}_i = (X_{i,1}, \dots, X_{i,784})\)</span> to distinguish the vector of predictors from the individual predictors. When referring to an arbitrary set of features rather than a specific image in our dataset, we drop the index <span class="math inline">\(i\)</span> and use <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X} = (X_{1}, \dots, X_{784})\)</span>. We use upper case variables because, in general, we think of the predictors as random variables. We use lower case, for example <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, to denote observed values. When we code we stick to lower case.</p>
<p>The machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features. Here, we will learn several approaches to building these algorithms. Although at this point it might seem impossible to achieve this, we will start with simple examples and build up our knowledge until we can attack more complex ones. In fact, we start with an artificially simple example with just one predictor and then move on to a slightly more realistic example with two predictors. Once we understand these, we will attack real-world machine learning challenges involving many predictors.</p>
</div>
<div id="exercises-44" class="section level2">
<h2><span class="header-section-number">27.3</span> Exercises</h2>
<p>1. For each of the following, determine if the outcome is continuous or categorical:</p>
<ol style="list-style-type: lower-alpha">
<li>Digit reader</li>
<li>Movie recommendations</li>
<li>Spam filter</li>
<li>Hospitalizations</li>
<li>Siri (speech recognition)</li>
</ol>
<p>2. How many features are available to us for prediction in the digits dataset?</p>
<p>3. In the digit reader example, the outcomes are stored here:</p>
<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb996-1"><a href="introduction-to-machine-learning.html#cb996-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb996-2"><a href="introduction-to-machine-learning.html#cb996-2"></a>mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</span>
<span id="cb996-3"><a href="introduction-to-machine-learning.html#cb996-3"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels</span></code></pre></div>
<p>Do the following operations have a practical meaning?</p>
<div class="sourceCode" id="cb997"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb997-1"><a href="introduction-to-machine-learning.html#cb997-1"></a>y[<span class="dv">5</span>] <span class="op">+</span><span class="st"> </span>y[<span class="dv">6</span>]</span>
<span id="cb997-2"><a href="introduction-to-machine-learning.html#cb997-2"></a>y[<span class="dv">5</span>] <span class="op">&gt;</span><span class="st"> </span>y[<span class="dv">6</span>]</span></code></pre></div>
<p>Pick the best answer:</p>
<ol style="list-style-type: lower-alpha">
<li>Yes, because <span class="math inline">\(9 + 2 = 11\)</span> and <span class="math inline">\(9 &gt; 2\)</span>.</li>
<li>No, because <code>y</code> is not a numeric vector.</li>
<li>No, because 11 is not a digit. It’s two digits.</li>
<li>No, because these are labels representing a category not a number. A <code>9</code> represents a class not the number 9.</li>
</ol>

</div>
<div id="evaluation-metrics" class="section level2">
<h2><span class="header-section-number">27.4</span> Evaluation metrics</h2>
<p>Before we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better”.</p>
<p>For our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain machine learning step by step, this example will let us set down the first building block. Soon enough, we will be attacking more interesting challenges. We use the <strong>caret</strong> package, which has several useful functions for building and assessing machine learning methods and we introduce in more detail in Section <a href="caret.html#caret">30</a>.</p>
<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb998-1"><a href="introduction-to-machine-learning.html#cb998-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb998-2"><a href="introduction-to-machine-learning.html#cb998-2"></a><span class="kw">library</span>(caret)</span></code></pre></div>
<p>For a first example, we use the height data in dslabs:</p>
<div class="sourceCode" id="cb999"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb999-1"><a href="introduction-to-machine-learning.html#cb999-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb999-2"><a href="introduction-to-machine-learning.html#cb999-2"></a><span class="kw">data</span>(heights)</span></code></pre></div>
<p>We start by defining the outcome and predictors.</p>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="introduction-to-machine-learning.html#cb1000-1"></a>y &lt;-<span class="st"> </span>heights<span class="op">$</span>sex</span>
<span id="cb1000-2"><a href="introduction-to-machine-learning.html#cb1000-2"></a>x &lt;-<span class="st"> </span>heights<span class="op">$</span>height</span></code></pre></div>
<p>In this case, we have only one predictor, height, and <code>y</code> is clearly a categorical outcome since observed values are either <code>Male</code> or <code>Female</code>.
We know that we will not be able to predict <span class="math inline">\(Y\)</span> very accurately based on <span class="math inline">\(X\)</span> because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of better.</p>
<div id="training-and-test-sets" class="section level3">
<h3><span class="header-section-number">27.4.1</span> Training and test sets</h3>
<p>Ultimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only <em>after</em> we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the <em>training</em> set. We refer to the group for which we pretend we don’t know the outcome as the <em>test</em> set.</p>
<p>A standard way of generating the training and test sets is by randomly splitting the data. The <strong>caret</strong> package includes the function <code>createDataPartition</code> that helps us generates indexes for randomly splitting the data into training and test sets:</p>
<div class="sourceCode" id="cb1001"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1001-1"><a href="introduction-to-machine-learning.html#cb1001-1"></a><span class="kw">set.seed</span>(<span class="dv">2007</span>)</span>
<span id="cb1001-2"><a href="introduction-to-machine-learning.html#cb1001-2"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>The argument <code>times</code> is used to define how many random samples of indexes to return, the argument <code>p</code> is used to define what proportion of the data is represented by the index, and the argument <code>list</code> is used to decide if we want the indexes returned as a list or not.
We can use the result of the <code>createDataPartition</code> function call to define the training and test sets like this:</p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="introduction-to-machine-learning.html#cb1002-1"></a>test_set &lt;-<span class="st"> </span>heights[test_index, ]</span>
<span id="cb1002-2"><a href="introduction-to-machine-learning.html#cb1002-2"></a>train_set &lt;-<span class="st"> </span>heights[<span class="op">-</span>test_index, ]</span></code></pre></div>
<p>We will now develop an algorithm using <strong>only</strong> the training set. Once we are done developing the algorithm, we will <em>freeze</em> it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted <strong>in the test set</strong>. This metric is usually referred to as <em>overall accuracy</em>.</p>
</div>
<div id="overall-accuracy" class="section level3">
<h3><span class="header-section-number">27.4.2</span> Overall accuracy</h3>
<p>To demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.</p>
<p>Let’s start by developing the simplest possible machine algorithm: guessing the outcome.</p>
<div class="sourceCode" id="cb1003"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1003-1"><a href="introduction-to-machine-learning.html#cb1003-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Note that we are completely ignoring the predictor and simply guessing the sex.</p>
<p>In machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the <strong>caret</strong> package, require or recommend that categorical outcomes be coded as factors. So convert <code>y_hat</code> to factors using the <code>factor</code> function:</p>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="introduction-to-machine-learning.html#cb1004-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1004-2"><a href="introduction-to-machine-learning.html#cb1004-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>The <em>overall accuracy</em> is simply defined as the overall proportion that is predicted correctly:</p>
<div class="sourceCode" id="cb1005"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1005-1"><a href="introduction-to-machine-learning.html#cb1005-1"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1005-2"><a href="introduction-to-machine-learning.html#cb1005-2"></a><span class="co">#&gt; [1] 0.51</span></span></code></pre></div>
<p>Not surprisingly, our accuracy is about 50%. We are guessing!</p>
<p>Can we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females:</p>
<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1006-1"><a href="introduction-to-machine-learning.html#cb1006-1"></a>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="kw">mean</span>(height), <span class="kw">sd</span>(height))</span>
<span id="cb1006-2"><a href="introduction-to-machine-learning.html#cb1006-2"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1006-3"><a href="introduction-to-machine-learning.html#cb1006-3"></a><span class="co">#&gt; # A tibble: 2 x 3</span></span>
<span id="cb1006-4"><a href="introduction-to-machine-learning.html#cb1006-4"></a><span class="co">#&gt;   sex    `mean(height)` `sd(height)`</span></span>
<span id="cb1006-5"><a href="introduction-to-machine-learning.html#cb1006-5"></a><span class="co">#&gt;   &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;</span></span>
<span id="cb1006-6"><a href="introduction-to-machine-learning.html#cb1006-6"></a><span class="co">#&gt; 1 Female           64.9         3.76</span></span>
<span id="cb1006-7"><a href="introduction-to-machine-learning.html#cb1006-7"></a><span class="co">#&gt; 2 Male             69.3         3.61</span></span></code></pre></div>
<p>But how do we make use of this insight? Let’s try another simple approach: predict <code>Male</code> if height is within two standard deviations from the average male:</p>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="introduction-to-machine-learning.html#cb1007-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">62</span>, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1007-2"><a href="introduction-to-machine-learning.html#cb1007-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>The accuracy goes up from 0.50 to about 0.80:</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="introduction-to-machine-learning.html#cb1008-1"></a><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> </span>y_hat)</span>
<span id="cb1008-2"><a href="introduction-to-machine-learning.html#cb1008-2"></a><span class="co">#&gt; [1] 0.793</span></span></code></pre></div>
<p>But can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, <strong>it is important that we optimize the cutoff using only the training set</strong>: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to <em>overfitting</em>, which often results in dangerously over-optimistic assessments.</p>
<p>Here we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:</p>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="introduction-to-machine-learning.html#cb1009-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1009-2"><a href="introduction-to-machine-learning.html#cb1009-2"></a>accuracy &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1009-3"><a href="introduction-to-machine-learning.html#cb1009-3"></a>  y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1009-4"><a href="introduction-to-machine-learning.html#cb1009-4"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1009-5"><a href="introduction-to-machine-learning.html#cb1009-5"></a>  <span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>train_set<span class="op">$</span>sex)</span>
<span id="cb1009-6"><a href="introduction-to-machine-learning.html#cb1009-6"></a>})</span></code></pre></div>
<p>We can make a plot showing the accuracy obtained on the training set for males and females:</p>
<p><img src="book_files/figure-html/accuracy-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see that the maximum value is:</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="introduction-to-machine-learning.html#cb1010-1"></a><span class="kw">max</span>(accuracy)</span>
<span id="cb1010-2"><a href="introduction-to-machine-learning.html#cb1010-2"></a><span class="co">#&gt; [1] 0.85</span></span></code></pre></div>
<p>which is much higher than 0.5. The cutoff resulting in this accuracy is:</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="introduction-to-machine-learning.html#cb1011-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(accuracy)]</span>
<span id="cb1011-2"><a href="introduction-to-machine-learning.html#cb1011-2"></a>best_cutoff</span>
<span id="cb1011-3"><a href="introduction-to-machine-learning.html#cb1011-3"></a><span class="co">#&gt; [1] 64</span></span></code></pre></div>
<p>We can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="introduction-to-machine-learning.html#cb1012-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1012-2"><a href="introduction-to-machine-learning.html#cb1012-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1012-3"><a href="introduction-to-machine-learning.html#cb1012-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(y_hat)</span>
<span id="cb1012-4"><a href="introduction-to-machine-learning.html#cb1012-4"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1012-5"><a href="introduction-to-machine-learning.html#cb1012-5"></a><span class="co">#&gt; [1] 0.804</span></span></code></pre></div>
<p>We see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result.</p>
</div>
<div id="the-confusion-matrix" class="section level3">
<h3><span class="header-section-number">27.4.3</span> The confusion matrix</h3>
<p>The prediction rule we developed in the previous section predicts <code>Male</code> if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict <code>Female</code>?</p>
<p>Generally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the <em>confusion matrix</em>, which basically tabulates each combination of prediction and actual value. We can do this in R using the function <code>table</code>:</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="introduction-to-machine-learning.html#cb1013-1"></a><span class="kw">table</span>(<span class="dt">predicted =</span> y_hat, <span class="dt">actual =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1013-2"><a href="introduction-to-machine-learning.html#cb1013-2"></a><span class="co">#&gt;          actual</span></span>
<span id="cb1013-3"><a href="introduction-to-machine-learning.html#cb1013-3"></a><span class="co">#&gt; predicted Female Male</span></span>
<span id="cb1013-4"><a href="introduction-to-machine-learning.html#cb1013-4"></a><span class="co">#&gt;    Female     48   32</span></span>
<span id="cb1013-5"><a href="introduction-to-machine-learning.html#cb1013-5"></a><span class="co">#&gt;    Male       71  374</span></span></code></pre></div>
<p>If we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get:</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="introduction-to-machine-learning.html#cb1014-1"></a>test_set <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1014-2"><a href="introduction-to-machine-learning.html#cb1014-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span></span>
<span id="cb1014-3"><a href="introduction-to-machine-learning.html#cb1014-3"></a><span class="st">  </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1014-4"><a href="introduction-to-machine-learning.html#cb1014-4"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>sex))</span>
<span id="cb1014-5"><a href="introduction-to-machine-learning.html#cb1014-5"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1014-6"><a href="introduction-to-machine-learning.html#cb1014-6"></a><span class="co">#&gt; # A tibble: 2 x 2</span></span>
<span id="cb1014-7"><a href="introduction-to-machine-learning.html#cb1014-7"></a><span class="co">#&gt;   sex    accuracy</span></span>
<span id="cb1014-8"><a href="introduction-to-machine-learning.html#cb1014-8"></a><span class="co">#&gt;   &lt;fct&gt;     &lt;dbl&gt;</span></span>
<span id="cb1014-9"><a href="introduction-to-machine-learning.html#cb1014-9"></a><span class="co">#&gt; 1 Female    0.403</span></span>
<span id="cb1014-10"><a href="introduction-to-machine-learning.html#cb1014-10"></a><span class="co">#&gt; 2 Male      0.921</span></span></code></pre></div>
<p>There is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the <em>prevalence</em> of males in this dataset is high. These heights were collected from three data sciences courses, two of which had more males enrolled:</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="introduction-to-machine-learning.html#cb1015-1"></a>prev &lt;-<span class="st"> </span><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> &quot;Male&quot;</span>)</span>
<span id="cb1015-2"><a href="introduction-to-machine-learning.html#cb1015-2"></a>prev</span>
<span id="cb1015-3"><a href="introduction-to-machine-learning.html#cb1015-3"></a><span class="co">#&gt; [1] 0.773</span></span></code></pre></div>
<p>So when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. <strong>This can actually be a big problem in machine learning.</strong> If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.</p>
<p>There are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study <em>sensitivity</em> and <em>specificity</em> separately.</p>
</div>
<div id="sensitivity-and-specificity" class="section level3">
<h3><span class="header-section-number">27.4.4</span> Sensitivity and specificity</h3>
<p>To define sensitivity and specificity, we need a binary outcome. When the outcomes are categorical, we can define these terms for a specific category. In the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit. Once we specify a category of interest, then we can talk about positive outcomes, <span class="math inline">\(Y=1\)</span>, and negative outcomes, <span class="math inline">\(Y=0\)</span>.</p>
<p>In general, <em>sensitivity</em> is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: <span class="math inline">\(\hat{Y}=1\)</span> when <span class="math inline">\(Y=1\)</span>. Because an algorithm that calls everything positive (<span class="math inline">\(\hat{Y}=1\)</span> no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine <em>specificity</em>, which is generally defined as the ability of an algorithm to not predict a positive <span class="math inline">\(\hat{Y}=0\)</span> when the actual outcome is not a positive <span class="math inline">\(Y=0\)</span>. We can summarize in the following way:</p>
<ul>
<li>High sensitivity: <span class="math inline">\(Y=1 \implies \hat{Y}=1\)</span></li>
<li>High specificity: <span class="math inline">\(Y=0 \implies \hat{Y} = 0\)</span></li>
</ul>
<p>Although the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:</p>
<ul>
<li>High specificity: <span class="math inline">\(\hat{Y}=1 \implies Y=1\)</span>.</li>
</ul>
<p>To provide precise definitions, we name the four entries of the confusion matrix:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Actually Positive
</th>
<th style="text-align:left;">
Actually Negative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Predicted positive
</td>
<td style="text-align:left;">
True positives (TP)
</td>
<td style="text-align:left;">
False positives (FP)
</td>
</tr>
<tr>
<td style="text-align:left;">
Predicted negative
</td>
<td style="text-align:left;">
False negatives (FN)
</td>
<td style="text-align:left;">
True negatives (TN)
</td>
</tr>
</tbody>
</table>
<p>Sensitivity is typically quantified by <span class="math inline">\(TP/(TP+FN)\)</span>, the proportion of actual positives (the first column = <span class="math inline">\(TP+FN\)</span>) that are called positives (<span class="math inline">\(TP\)</span>). This quantity is referred to as the <em>true positive rate</em> (TPR) or <em>recall</em>.</p>
<p>Specificity is defined as <span class="math inline">\(TN/(TN+FP)\)</span> or the proportion of negatives (the second column = <span class="math inline">\(FP+TN\)</span>) that are called negatives (<span class="math inline">\(TN\)</span>). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is <span class="math inline">\(TP/(TP+FP)\)</span> or the proportion of outcomes called positives (the first row or <span class="math inline">\(TP+FP\)</span>) that are actually positives (<span class="math inline">\(TP\)</span>). This quantity is referred to as <em>positive predictive value (PPV)</em> and also as <em>precision</em>. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.</p>
<p>The multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.</p>
<table>
<colgroup>
<col width="18%" />
<col width="10%" />
<col width="20%" />
<col width="16%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Measure of</th>
<th>Name 1</th>
<th>Name 2</th>
<th>Definition</th>
<th>Probability representation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sensitivity</td>
<td>TPR</td>
<td>Recall</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=1 \mid Y=1)\)</span></td>
</tr>
<tr class="even">
<td>specificity</td>
<td>TNR</td>
<td>1-FPR</td>
<td><span class="math inline">\(\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=0 \mid Y=0)\)</span></td>
</tr>
<tr class="odd">
<td>specificity</td>
<td>PPV</td>
<td>Precision</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(Y=1 \mid \hat{Y}=1)\)</span></td>
</tr>
</tbody>
</table>
<p>Here TPR is True Positive Rate, FPR is False Positive Rate, and PPV is Positive Predictive Value.
The <strong>caret</strong> function <code>confusionMatrix</code> computes all these metrics for us once we define what category “positive” is. The function expects factors as input, and the first level is considered the positive outcome or <span class="math inline">\(Y=1\)</span>. In our example, <code>Female</code> is the first level because it comes before <code>Male</code> alphabetically. If you type this into R you will see several metrics including accuracy, sensitivity, specificity, and PPV.</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="introduction-to-machine-learning.html#cb1016-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span></code></pre></div>
<p>You can acceess these directly, for example, like this:</p>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="introduction-to-machine-learning.html#cb1017-1"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1017-2"><a href="introduction-to-machine-learning.html#cb1017-2"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1017-3"><a href="introduction-to-machine-learning.html#cb1017-3"></a><span class="co">#&gt;    0.804</span></span>
<span id="cb1017-4"><a href="introduction-to-machine-learning.html#cb1017-4"></a>cm<span class="op">$</span>byClass[<span class="kw">c</span>(<span class="st">&quot;Sensitivity&quot;</span>,<span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Prevalence&quot;</span>)]</span>
<span id="cb1017-5"><a href="introduction-to-machine-learning.html#cb1017-5"></a><span class="co">#&gt; Sensitivity Specificity  Prevalence </span></span>
<span id="cb1017-6"><a href="introduction-to-machine-learning.html#cb1017-6"></a><span class="co">#&gt;       0.403       0.921       0.227</span></span></code></pre></div>
<p>We can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same.</p>
</div>
<div id="balanced-accuracy-and-f_1-score" class="section level3">
<h3><span class="header-section-number">27.4.5</span> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</h3>
<p>Although we usually recommend studying both specificity and sensitivity, very often it is useful to have a one-number summary, for example for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as <em>balanced accuracy</em>. Because specificity and sensitivity are rates, it is more appropriate to compute the <em>harmonic</em> average. In fact, the <em><span class="math inline">\(F_1\)</span>-score</em>, a widely used one-number summary, is the harmonic average of precision and recall:</p>
<p><span class="math display">\[
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }
\]</span></p>
<p>Because it is easier to write, you often see this harmonic average rewritten as:</p>
<p><span class="math display">\[
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
\]</span></p>
<p>when defining <span class="math inline">\(F_1\)</span>.</p>
<p>Remember that, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The <span class="math inline">\(F_1\)</span>-score can be adapted to weigh specificity and sensitivity differently. To do this, we define <span class="math inline">\(\beta\)</span> to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:</p>
<p><span class="math display">\[
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} + 
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
\]</span></p>
<p>The <code>F_meas</code> function in the <strong>caret</strong> package computes this summary with <code>beta</code> defaulting to 1.</p>
<p>Let’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="introduction-to-machine-learning.html#cb1018-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1018-2"><a href="introduction-to-machine-learning.html#cb1018-2"></a>F_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1018-3"><a href="introduction-to-machine-learning.html#cb1018-3"></a>  y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1018-4"><a href="introduction-to-machine-learning.html#cb1018-4"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1018-5"><a href="introduction-to-machine-learning.html#cb1018-5"></a>  <span class="kw">F_meas</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> <span class="kw">factor</span>(train_set<span class="op">$</span>sex))</span>
<span id="cb1018-6"><a href="introduction-to-machine-learning.html#cb1018-6"></a>})</span></code></pre></div>
<p>As before, we can plot these <span class="math inline">\(F_1\)</span> measures versus the cutoffs:</p>
<p><img src="book_files/figure-html/f_1-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see that it is maximized at <span class="math inline">\(F_1\)</span> value of:</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="introduction-to-machine-learning.html#cb1019-1"></a><span class="kw">max</span>(F_<span class="dv">1</span>)</span>
<span id="cb1019-2"><a href="introduction-to-machine-learning.html#cb1019-2"></a><span class="co">#&gt; [1] 0.647</span></span></code></pre></div>
<p>This maximum is achieved when we use the following cutoff:</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="introduction-to-machine-learning.html#cb1020-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(F_<span class="dv">1</span>)]</span>
<span id="cb1020-2"><a href="introduction-to-machine-learning.html#cb1020-2"></a>best_cutoff</span>
<span id="cb1020-3"><a href="introduction-to-machine-learning.html#cb1020-3"></a><span class="co">#&gt; [1] 66</span></span></code></pre></div>
<p>A cutoff of 66 makes more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="introduction-to-machine-learning.html#cb1021-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1021-2"><a href="introduction-to-machine-learning.html#cb1021-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1021-3"><a href="introduction-to-machine-learning.html#cb1021-3"></a><span class="kw">sensitivity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1021-4"><a href="introduction-to-machine-learning.html#cb1021-4"></a><span class="co">#&gt; [1] 0.63</span></span>
<span id="cb1021-5"><a href="introduction-to-machine-learning.html#cb1021-5"></a><span class="kw">specificity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1021-6"><a href="introduction-to-machine-learning.html#cb1021-6"></a><span class="co">#&gt; [1] 0.833</span></span></code></pre></div>
<p>We now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm. It takes height as a predictor and predicts female if you are 65 inches or shorter.</p>
</div>
<div id="prevalence-matters-in-practice" class="section level3">
<h3><span class="header-section-number">27.4.6</span> Prevalence matters in practice</h3>
<p>A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease. The doctor shares data with you and you then develop an algorithm with very high sensitivity. You explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly. You also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: <span class="math inline">\(\mbox{Pr}(\hat{Y}=1)\)</span>. The doctor is neither concerned nor impressed and explains that what is important is the precision of the test: <span class="math inline">\(\mbox{Pr}(Y=1 | \hat{Y}=1)\)</span>. Using Bayes theorem, we can connect the two measures:</p>
<p><span class="math display">\[ \mbox{Pr}(Y = 1\mid \hat{Y}=1) = \mbox{Pr}(\hat{Y}=1 \mid Y=1) \frac{\mbox{Pr}(Y=1)}{\mbox{Pr}(\hat{Y}=1)}\]</span></p>
<p>The doctor knows that the prevalence of the disease is 5 in 1,000, which implies that <span class="math inline">\(\mbox{Pr}(Y=1) \, / \,\mbox{Pr}(\hat{Y}=1) = 1/100\)</span> and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm.</p>
</div>
<div id="roc-and-precision-recall-curves" class="section level3">
<h3><span class="header-section-number">27.4.7</span> ROC and precision-recall curves</h3>
<p>When comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and <span class="math inline">\(F_1\)</span>. The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Note that guessing <code>Male</code> with higher probability would give us higher accuracy due to the bias in the sample:</p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="introduction-to-machine-learning.html#cb1022-1"></a>p &lt;-<span class="st"> </span><span class="fl">0.9</span></span>
<span id="cb1022-2"><a href="introduction-to-machine-learning.html#cb1022-2"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(test_index)</span>
<span id="cb1022-3"><a href="introduction-to-machine-learning.html#cb1022-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1022-4"><a href="introduction-to-machine-learning.html#cb1022-4"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1022-5"><a href="introduction-to-machine-learning.html#cb1022-5"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1022-6"><a href="introduction-to-machine-learning.html#cb1022-6"></a><span class="co">#&gt; [1] 0.739</span></span></code></pre></div>
<p>But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.</p>
<p>Remember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.</p>
<p>A widely used plot that does this is the <em>receiver operating characteristic</em> (ROC) curve. If you are wondering where this name comes from, you can consult the
ROC Wikipedia page<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a>.</p>
<p>The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="introduction-to-machine-learning.html#cb1023-1"></a>probs &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb1023-2"><a href="introduction-to-machine-learning.html#cb1023-2"></a>guessing &lt;-<span class="st"> </span><span class="kw">map_df</span>(probs, <span class="cf">function</span>(p){</span>
<span id="cb1023-3"><a href="introduction-to-machine-learning.html#cb1023-3"></a>  y_hat &lt;-<span class="st"> </span></span>
<span id="cb1023-4"><a href="introduction-to-machine-learning.html#cb1023-4"></a><span class="st">    </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1023-5"><a href="introduction-to-machine-learning.html#cb1023-5"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span>
<span id="cb1023-6"><a href="introduction-to-machine-learning.html#cb1023-6"></a>  <span class="kw">list</span>(<span class="dt">method =</span> <span class="st">&quot;Guessing&quot;</span>,</span>
<span id="cb1023-7"><a href="introduction-to-machine-learning.html#cb1023-7"></a>       <span class="dt">FPR =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">specificity</span>(y_hat, test_set<span class="op">$</span>sex),</span>
<span id="cb1023-8"><a href="introduction-to-machine-learning.html#cb1023-8"></a>       <span class="dt">TPR =</span> <span class="kw">sensitivity</span>(y_hat, test_set<span class="op">$</span>sex))</span>
<span id="cb1023-9"><a href="introduction-to-machine-learning.html#cb1023-9"></a>})</span></code></pre></div>
<p>We can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:</p>
<!--We can construct an ROC curve for the height-based approach:-->
<!--
<img src="book_files/figure-html/roc-2-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p><img src="book_files/figure-html/roc-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method. Note that ROC curves for guessing always fall on the identiy line. Also note that when making ROC curves, it is often nice to add the cutoff associated with each point.</p>
<p>The packages <strong>pROC</strong> and <strong>plotROC</strong> are useful for generating these plots.</p>
<p>ROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall:</p>
<p><img src="book_files/figure-html/precision-recall-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From this plot we immediately see that the precision of guessing is not high. This is because the prevalence is low. We also see that if we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes.</p>
</div>
<div id="loss-function" class="section level3">
<h3><span class="header-section-number">27.4.8</span> The loss function</h3>
<p>Up to now we have described evaluation metrics that apply exclusively to categorical data.
Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and <span class="math inline">\(F_1\)</span> can be used as quantification. However, these metrics are not useful for continuous outcomes. In this section, we describe how the general approach to defining “best” in machine learning is to define a <em>loss function</em>, which can be applied to both categorical and continuous data.</p>
<p>The most commonly used loss function is the squared loss function. If <span class="math inline">\(\hat{y}\)</span> is our predictor and <span class="math inline">\(y\)</span> is the observed outcome, the squared loss function is simply:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Because we often have a test set with many observations, say <span class="math inline">\(N\)</span>, we use the mean squared error (MSE):</p>
<p><span class="math display">\[
\mbox{MSE} = \frac{1}{N} \mbox{RSS} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>In practice, we often report the root mean squared error (RMSE), which is <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, because it is in the same units as the outcomes. But doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.</p>
<p>If the outcomes are binary, both RMSE and MSE are equivalent to one minus accuracy, since <span class="math inline">\((\hat{y} - y)^2\)</span> is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.</p>
<p>Because our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this:</p>
<p><span class="math display">\[
\mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
\]</span></p>
<p>This is a theoretical concept because in practice we only have one dataset to work with. But in theory, we think of having a very large number of random samples (call it <span class="math inline">\(B\)</span>), apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as:</p>
<p><span class="math display">\[
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 
\]</span></p>
<p>with <span class="math inline">\(y_{i}^b\)</span> denoting the <span class="math inline">\(i\)</span>th observation in the <span class="math inline">\(b\)</span>th random sample and <span class="math inline">\(\hat{y}_i^b\)</span> the resulting prediction obtained from applying the exact same algorithm to the <span class="math inline">\(b\)</span>th random sample. Again, in practice we only observe one random sample, so the expected MSE is only theoretical. However, in Chapter <a href="cross-validation.html#cross-validation">29</a> we describe an approach to estimating the MSE that tries to mimic this theoretical quantity.</p>
<p>Note that there are loss functions other than the squared loss. For example, the <em>Mean Absolute Error</em> uses absolute values, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> instead of squaring the errors
<span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. However, in this book we focus on minimizing square loss since it is the most widely used.</p>
</div>
</div>
<div id="exercises-45" class="section level2">
<h2><span class="header-section-number">27.5</span> Exercises</h2>
<p>The <code>reported_height</code> and <code>height</code> datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the <code>reported_height</code> dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it <code>type</code>, to denote the type of student: <code>inclass</code> or <code>online</code>:</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="introduction-to-machine-learning.html#cb1024-1"></a><span class="kw">library</span>(lubridate)</span>
<span id="cb1024-2"><a href="introduction-to-machine-learning.html#cb1024-2"></a><span class="kw">data</span>(<span class="st">&quot;reported_heights&quot;</span>)</span>
<span id="cb1024-3"><a href="introduction-to-machine-learning.html#cb1024-3"></a>dat &lt;-<span class="st"> </span><span class="kw">mutate</span>(reported_heights, <span class="dt">date_time =</span> <span class="kw">ymd_hms</span>(time_stamp)) <span class="op">%&gt;%</span></span>
<span id="cb1024-4"><a href="introduction-to-machine-learning.html#cb1024-4"></a><span class="st">  </span><span class="kw">filter</span>(date_time <span class="op">&gt;=</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">01</span>, <span class="dv">25</span>) <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb1024-5"><a href="introduction-to-machine-learning.html#cb1024-5"></a><span class="st">           </span>date_time <span class="op">&lt;</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">02</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1024-6"><a href="introduction-to-machine-learning.html#cb1024-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">type =</span> <span class="kw">ifelse</span>(<span class="kw">day</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">25</span> <span class="op">&amp;</span><span class="st"> </span><span class="kw">hour</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">8</span> <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb1024-7"><a href="introduction-to-machine-learning.html#cb1024-7"></a><span class="st">                         </span><span class="kw">between</span>(<span class="kw">minute</span>(date_time), <span class="dv">15</span>, <span class="dv">30</span>),</span>
<span id="cb1024-8"><a href="introduction-to-machine-learning.html#cb1024-8"></a>                       <span class="st">&quot;inclass&quot;</span>, <span class="st">&quot;online&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(sex, type)</span>
<span id="cb1024-9"><a href="introduction-to-machine-learning.html#cb1024-9"></a>x &lt;-<span class="st"> </span>dat<span class="op">$</span>type</span>
<span id="cb1024-10"><a href="introduction-to-machine-learning.html#cb1024-10"></a>y &lt;-<span class="st"> </span><span class="kw">factor</span>(dat<span class="op">$</span>sex, <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span></code></pre></div>
<p>1. Show summary statistics that indicate that the <code>type</code> is predictive of sex.</p>
<p>2. Instead of using height to predict sex, use the <code>type</code> variable.</p>
<p>3. Show the confusion matrix.</p>
<p>4. Use the <code>confusionMatrix</code> function in the <strong>caret</strong> package to report accuracy.</p>
<p>5. Now use the <code>sensitivity</code> and <code>specificity</code> functions to report specificity and sensitivity.</p>
<p>6. What is the prevalence (% of females) in the <code>dat</code> dataset defined above?</p>

</div>
<div id="conditional-probabilities-and-expectations" class="section level2">
<h2><span class="header-section-number">27.6</span> Conditional probabilities and expectations</h2>
<p>In machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and your bank at times thinks your card was stolen when it was not. The most common reason for not being able to build perfect algorithms is that it is impossible. To see this, note that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes. Because our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height <span class="math inline">\(x\)</span>, you will have both males and females that are <span class="math inline">\(x\)</span> inches tall.</p>
<p>However, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in Section <a href="regression.html#conditional-expectation">17.3</a>. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data.</p>
<div id="conditional-probabilities-1" class="section level3">
<h3><span class="header-section-number">27.6.1</span> Conditional probabilities</h3>
<p>We use the notation <span class="math inline">\((X_1 = x_1,\dots,X_p=x_p)\)</span> to represent the fact that we have observed values <span class="math inline">\(x_1,\dots,x_p\)</span> for covariates <span class="math inline">\(X_1, \dots, X_p\)</span>. This does not imply that the outcome <span class="math inline">\(Y\)</span> will take a specific value. Instead, it implies a specific probability. In particular, we denote the <em>conditional probabilities</em> for each class <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
\]</span></p>
<p>To avoid writing out all the predictors, we will use the bold letters like this: <span class="math inline">\(\mathbf{X} \equiv (X_1,\dots,X_p)\)</span> and <span class="math inline">\(\mathbf{x} \equiv (x_1,\dots,x_p)\)</span>. We will also use the following notation for the conditional probability of being class <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<p>Note: We will be using the <span class="math inline">\(p(x)\)</span> notation to represent conditional probabilities as functions of the predictors. Do not confuse it with the <span class="math inline">\(p\)</span> that represents the number of predictors.</p>
<p>These probabilities guide the construction of an algorithm that makes the best prediction: for any given <span class="math inline">\(\mathbf{x}\)</span>, we will predict the class <span class="math inline">\(k\)</span> with the largest probability among <span class="math inline">\(p_1(x), p_2(x), \dots p_K(x)\)</span>. In mathematical notation, we write it like this: <span class="math inline">\(\hat{Y} = \max_k p_k(\mathbf{x})\)</span>.</p>
<p>In machine learning, we refer to this as <em>Bayes’ Rule</em>. But keep in mind that this is a theoretical rule since in practice we don’t know <span class="math inline">\(p_k(\mathbf{x}), k=1,\dots,K\)</span>. In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span>, the better our predictor:</p>
<p><span class="math display">\[\hat{Y} = \max_k \hat{p}_k(\mathbf{x})\]</span></p>
<p>So what we will predict depends on two things: 1) how close are the <span class="math inline">\(\max_k p_k(\mathbf{x})\)</span> to 1 or 0 (perfect certainty)
and 2) how close our estimates <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span> are to <span class="math inline">\(p_k(\mathbf{x})\)</span>. We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities. The first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others our success is restricted by the randomness of the process, with movie recommendations for example.</p>
<p>Before we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed above, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the <span class="math inline">\(p_k(x), k=1,\dots,K\)</span> will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired.</p>
</div>
<div id="conditional-expectations" class="section level3">
<h3><span class="header-section-number">27.6.2</span> Conditional expectations</h3>
<p>For binary data, you can think of the probability <span class="math inline">\(\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})\)</span> as the proportion of 1s in the stratum of the population for which <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>. Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between <em>conditional probabilities</em> and <em>conditional expectations</em>.</p>
<p>Because the expectation is the average of values <span class="math inline">\(y_1,\dots,y_n\)</span> in the population, in the case in which the <span class="math inline">\(y\)</span>s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}).
\]</span></p>
<p>As a result, we often only use the expectation to denote both the conditional probability and conditional expectation.</p>
<p>Just like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors.</p>
</div>
<div id="conditional-expectation-minimizes-squared-loss-function" class="section level3">
<h3><span class="header-section-number">27.6.3</span> Conditional expectation minimizes squared loss function</h3>
<p>Why do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions <span class="math inline">\(\hat{Y}\)</span>,</p>
<p><span class="math display">\[
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2  \mid  \mathbf{X}=\mathbf{x} \}
\]</span></p>
<p>Due to this property, a succinct description of the main task of machine learning is that we use data to estimate:</p>
<p><span class="math display">\[
f(\mathbf{x}) \equiv \mbox{E}( Y  \mid  \mathbf{X}=\mathbf{x} )
\]</span></p>
<p>for any set of features <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_p)\)</span>. Of course this is easier said than done, since this function can take any shape and <span class="math inline">\(p\)</span> can be very large. Consider a case in which we only have one predictor <span class="math inline">\(x\)</span>. The expectation <span class="math inline">\(\mbox{E}\{ Y \mid X=x \}\)</span> can be any function of <span class="math inline">\(x\)</span>: a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large <span class="math inline">\(p\)</span>, in which case <span class="math inline">\(f(\mathbf{x})\)</span> is a function of a multidimensional vector <span class="math inline">\(\mathbf{x}\)</span>. For example, in our digit reader example <span class="math inline">\(p = 784\)</span>! <strong>The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation.</strong></p>
</div>
</div>
<div id="exercises-46" class="section level2">
<h2><span class="header-section-number">27.7</span> Exercises</h2>
<p>1. Compute conditional probabilities for being Male for the <code>heights</code> dataset. Round the heights to the closest inch. Plot the estimated conditional probability <span class="math inline">\(P(x) = \mbox{Pr}(\mbox{Male} | \mbox{height}=x)\)</span> for each <span class="math inline">\(x\)</span>.</p>
<p>2. In the plot we just made, we see high variability for low values of height. This is because we have few data points in these strata. This time use the <code>quantile</code> function for quantiles <span class="math inline">\(0.1,0.2,\dots,0.9\)</span> and the <code>cut</code> function to assure each group has the same number of points. Hint: for any numeric vector <code>x</code>, you can create groups based on quantiles like this:</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="introduction-to-machine-learning.html#cb1025-1"></a><span class="kw">cut</span>(x, <span class="kw">quantile</span>(x, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)), <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>3. Generate data from a bivariate normal distribution using the <strong>MASS</strong> package like this:</p>
<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1026-1"><a href="introduction-to-machine-learning.html#cb1026-1"></a>Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1026-2"><a href="introduction-to-machine-learning.html#cb1026-2"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1026-3"><a href="introduction-to-machine-learning.html#cb1026-3"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</span></code></pre></div>
<p>You can make a quick plot of the data using <code>plot(dat)</code>. Use an approach similar to the previous exercise to estimate the conditional expectations and make a plot.</p>

</div>
<div id="two-or-seven" class="section level2">
<h2><span class="header-section-number">27.8</span> Case study: is it a 2 or a 7?</h2>
<p>In the two simple examples above, we only had one predictor. We actually do not consider these machine learning challenges, which are characterized by cases with many predictors. Let’s go back to the digits example in which we had 784 predictors. For illustrative purposes, we will start by simplifying this problem to one with two predictors and two classes. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the predictors. We are not quite ready to build algorithms with 784 predictors, so we will extract two simple predictors from the 784: the proportion of dark pixels that are in the upper left quadrant (<span class="math inline">\(X_1\)</span>) and the lower right quadrant (<span class="math inline">\(X_2\)</span>).</p>
<p>We then select a random sample of 1,000 digits, 500 in the training set and 500 in the test set. We provide this dataset in the <code>dslabs</code> package:</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="introduction-to-machine-learning.html#cb1027-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1027-2"><a href="introduction-to-machine-learning.html#cb1027-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1027-3"><a href="introduction-to-machine-learning.html#cb1027-3"></a><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span></code></pre></div>
<p>We can explore the data by plotting the two predictors and using colors to denote the labels:</p>
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="introduction-to-machine-learning.html#cb1028-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="book_files/figure-html/two-or-seven-scatter-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can immediately see some patterns. For example, if <span class="math inline">\(X_1\)</span> (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of <span class="math inline">\(X_1\)</span>, the 2s appear to be in the mid range values of <span class="math inline">\(X_2\)</span>.</p>
<p>These are the images of the digits with the largest and smallest values for <span class="math inline">\(X_1\)</span>:
And here are the original images corresponding to the largest and smallest value of <span class="math inline">\(X_2\)</span>:</p>
<p><img src="book_files/figure-html/two-or-seven-images-large-x1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.</p>
<p>We haven’t really learned any algorithms yet, so let’s try building an algorithm using regression. The model is simply:</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) = 
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>We fit it like this:</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="introduction-to-machine-learning.html#cb1029-1"></a>fit &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span></span>
<span id="cb1029-2"><a href="introduction-to-machine-learning.html#cb1029-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">ifelse</span>(y<span class="op">==</span><span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1029-3"><a href="introduction-to-machine-learning.html#cb1029-3"></a><span class="st">  </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x_<span class="dv">2</span>, <span class="dt">data =</span> .)</span></code></pre></div>
<p>We can now build a decision rule based on the estimate of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>:</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="introduction-to-machine-learning.html#cb1030-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1030-2"><a href="introduction-to-machine-learning.html#cb1030-2"></a>p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1030-3"><a href="introduction-to-machine-learning.html#cb1030-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(p_hat <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">2</span>))</span>
<span id="cb1030-4"><a href="introduction-to-machine-learning.html#cb1030-4"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]</span>
<span id="cb1030-5"><a href="introduction-to-machine-learning.html#cb1030-5"></a><span class="co">#&gt; [1] 0.75</span></span></code></pre></div>
<p>We get an accuracy well above 50%. Not bad for our first try. But can we do better?</p>
<p>Because we constructed the <code>mnist_27</code> example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the <em>true</em> conditional distribution <span class="math inline">\(p(x_1, x_2)\)</span>. Keep in mind that this is something we don’t have access to in practice, but we include it in this example because it permits the comparison of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> to the true <span class="math inline">\(p(x_1, x_2)\)</span>. This comparison teaches us the limitations of different algorithms. Let’s do that here. We have stored the true <span class="math inline">\(p(x_1,x_2)\)</span> in the <code>mnist_27</code> object and can plot the image using the <strong>ggplot2</strong> function <code>geom_raster()</code>.
We choose better colors and use the <code>stat_contour</code> function to draw a curve that separates pairs <span class="math inline">\((x_1,x_2)\)</span> for which <span class="math inline">\(p(x_1,x_2) &gt; 0.5\)</span> and pairs for which <span class="math inline">\(p(x_1,x_2) &lt; 0.5\)</span>:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="introduction-to-machine-learning.html#cb1031-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>true_p <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">z =</span> p, <span class="dt">fill =</span> p)) <span class="op">+</span></span>
<span id="cb1031-2"><a href="introduction-to-machine-learning.html#cb1031-2"></a><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></span>
<span id="cb1031-3"><a href="introduction-to-machine-learning.html#cb1031-3"></a><span class="st">  </span><span class="kw">scale_fill_gradientn</span>(<span class="dt">colors=</span><span class="kw">c</span>(<span class="st">&quot;#F8766D&quot;</span>, <span class="st">&quot;white&quot;</span>, <span class="st">&quot;#00BFC4&quot;</span>)) <span class="op">+</span></span>
<span id="cb1031-4"><a href="introduction-to-machine-learning.html#cb1031-4"></a><span class="st">  </span><span class="kw">stat_contour</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="fl">0.5</span>), <span class="dt">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="book_files/figure-html/true-p-better-colors-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Above you see a plot of the true <span class="math inline">\(p(x,y)\)</span>. To start understanding the limitations of logistic regression here, first note that with logistic regression <span class="math inline">\(\hat{p}(x,y)\)</span> has to be a plane, and as a result the boundary defined by the decision rule is given by:
<span class="math inline">\(\hat{p}(x,y) = 0.5\)</span>, which implies the boundary can’t be anything other than a straight line:</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5  \implies
x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2  -\hat{\beta}_1/\hat{\beta}_2 x_1
\]</span>
Note that for this boundary, <span class="math inline">\(x_2\)</span> is a linear function of <span class="math inline">\(x_1\)</span>. This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true <span class="math inline">\(p(x_1,x_2)\)</span>. Below is a visual representation of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>. We used the <code>squish</code> function from the <strong>scales</strong> package to constrain estimates to be between 0 and 1. We can see where the mistakes were made by also showing the data and the boundary. They mainly come from low values <span class="math inline">\(x_1\)</span> that have either high or low value of <span class="math inline">\(x_2\)</span>. Regression can’t catch this.</p>
<p><img src="book_files/figure-html/regression-p-hat-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We need something more flexible: a method that permits estimates with shapes other than a plane.</p>
<p>We are going to learn a few new algorithms based on different ideas and concepts. But what they all have in common is that they permit more flexible approaches. We will start by describing nearest neighbor and kernel approaches. To introduce the concepts behinds these approaches, we will again start with a simple one-dimensional example and describe the concept of <em>smoothing</em>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="99">
<li id="fn99"><p><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" class="uri">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a><a href="introduction-to-machine-learning.html#fnref99" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="text-mining.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smoothing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/ml/intro-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
