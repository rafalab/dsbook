<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 19 Linear Models | Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 19 Linear Models | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 19 Linear Models | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry">


<meta name="date" content="2019-04-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression.html">
<link rel="next" href="association-is-not-causation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i>Case studies</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who will find this book useful?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i>What does this book cover?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i>What is not covered by this book?</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>1</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>1.1</b> Installing R</a></li>
<li class="chapter" data-level="1.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>1.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Getting Started with R and RStudio</a><ul>
<li class="chapter" data-level="2.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>2.1</b> Why R?</a></li>
<li class="chapter" data-level="2.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>2.2</b> The R console</a></li>
<li class="chapter" data-level="2.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>2.3</b> Scripts</a></li>
<li class="chapter" data-level="2.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>2.4</b> RStudio</a><ul>
<li class="chapter" data-level="2.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>2.4.1</b> The panes</a></li>
<li class="chapter" data-level="2.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>2.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="2.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>2.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="2.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>2.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>2.5</b> Installing R packages</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>3.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="3.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>3.2</b> The very basics</a><ul>
<li class="chapter" data-level="3.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>3.2.1</b> Objects</a></li>
<li class="chapter" data-level="3.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>3.2.2</b> The workspace</a></li>
<li class="chapter" data-level="3.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>3.2.3</b> Functions</a></li>
<li class="chapter" data-level="3.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>3.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="3.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>3.2.5</b> Variable names</a></li>
<li class="chapter" data-level="3.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>3.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="3.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>3.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="3.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>3.2.8</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
<li class="chapter" data-level="3.4" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>3.4</b> Data types</a></li>
<li class="chapter" data-level="3.5" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>3.5</b> Data frames</a><ul>
<li class="chapter" data-level="3.5.1" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>3.5.1</b> Examining an object</a></li>
<li class="chapter" data-level="3.5.2" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>3.5.2</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="3.5.3" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>3.5.3</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="3.5.4" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>3.5.4</b> Factors</a></li>
<li class="chapter" data-level="3.5.5" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>3.5.5</b> Lists</a></li>
<li class="chapter" data-level="3.5.6" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>3.5.6</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
<li class="chapter" data-level="3.7" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>3.7</b> Vectors</a><ul>
<li class="chapter" data-level="3.7.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>3.7.1</b> Creating vectors</a></li>
<li class="chapter" data-level="3.7.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>3.7.2</b> Names</a></li>
<li class="chapter" data-level="3.7.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>3.7.3</b> Sequences</a></li>
<li class="chapter" data-level="3.7.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>3.7.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>3.8</b> Coercion</a><ul>
<li class="chapter" data-level="3.8.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>3.8.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
<li class="chapter" data-level="3.10" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>3.10</b> Sorting</a><ul>
<li class="chapter" data-level="3.10.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>3.10.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="3.10.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>3.10.2</b> <code>order</code></a></li>
<li class="chapter" data-level="3.10.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>3.10.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="3.10.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>3.10.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="3.10.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>3.10.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="r-basics.html"><a href="r-basics.html#exercise"><i class="fa fa-check"></i><b>3.11</b> Exercise</a></li>
<li class="chapter" data-level="3.12" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>3.12</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="3.12.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>3.12.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="3.12.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>3.12.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>3.13</b> Exercises</a></li>
<li class="chapter" data-level="3.14" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>3.14</b> Indexing</a><ul>
<li class="chapter" data-level="3.14.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>3.14.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="3.14.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>3.14.2</b> Logical operators</a></li>
<li class="chapter" data-level="3.14.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>3.14.3</b> <code>which</code></a></li>
<li class="chapter" data-level="3.14.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>3.14.4</b> <code>match</code></a></li>
<li class="chapter" data-level="3.14.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>3.14.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>3.15</b> Exercises</a></li>
<li class="chapter" data-level="3.16" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>3.16</b> Basic plots</a><ul>
<li class="chapter" data-level="3.16.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>3.16.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="3.16.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>3.16.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="3.16.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>3.16.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="3.16.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>3.16.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="3.17" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>3.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>4</b> Programming basics</a><ul>
<li class="chapter" data-level="4.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>4.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="4.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>4.2</b> Defining functions</a></li>
<li class="chapter" data-level="4.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>4.3</b> Namespaces</a></li>
<li class="chapter" data-level="4.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>4.4</b> For-loops</a></li>
<li class="chapter" data-level="4.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>4.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="4.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-6"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>5</b> The tidyverse</a><ul>
<li class="chapter" data-level="5.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>5.1</b> Tidy data</a></li>
<li class="chapter" data-level="5.2" data-path="tidyverse.html"><a href="tidyverse.html#exercises-7"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
<li class="chapter" data-level="5.3" data-path="tidyverse.html"><a href="tidyverse.html#manipulating-data-frames"><i class="fa fa-check"></i><b>5.3</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tidyverse.html"><a href="tidyverse.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>5.3.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="5.3.2" data-path="tidyverse.html"><a href="tidyverse.html#subsetting-with-filter"><i class="fa fa-check"></i><b>5.3.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="5.3.3" data-path="tidyverse.html"><a href="tidyverse.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>5.3.3</b> Selecting columns with <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tidyverse.html"><a href="tidyverse.html#exercises-8"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>5.5</b> The pipe: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="5.6" data-path="tidyverse.html"><a href="tidyverse.html#exercises-9"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
<li class="chapter" data-level="5.7" data-path="tidyverse.html"><a href="tidyverse.html#summarizing-data"><i class="fa fa-check"></i><b>5.7</b> Summarizing data</a><ul>
<li class="chapter" data-level="5.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>5.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="5.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>5.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="5.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>5.7.3</b> Group then summarize with <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="tidyverse.html"><a href="tidyverse.html#sorting-data-frames"><i class="fa fa-check"></i><b>5.8</b> Sorting data frames</a><ul>
<li class="chapter" data-level="5.8.1" data-path="tidyverse.html"><a href="tidyverse.html#nested-sorting"><i class="fa fa-check"></i><b>5.8.1</b> Nested sorting</a></li>
<li class="chapter" data-level="5.8.2" data-path="tidyverse.html"><a href="tidyverse.html#the-top-n"><i class="fa fa-check"></i><b>5.8.2</b> The top <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="tidyverse.html"><a href="tidyverse.html#exercises-10"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>5.10</b> Tibbles</a><ul>
<li class="chapter" data-level="5.10.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>5.10.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="5.10.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>5.10.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="5.10.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>5.10.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="5.10.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>5.10.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="5.10.5" data-path="tidyverse.html"><a href="tidyverse.html#create-a-tibble-using-data_frame-instead-of-data.frame"><i class="fa fa-check"></i><b>5.10.5</b> Create a tibble using <code>data_frame</code> instead of <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>5.11</b> The dot operator</a></li>
<li class="chapter" data-level="5.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>5.12</b> <code>do</code></a></li>
<li class="chapter" data-level="5.13" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>5.13</b> The <strong>purrr</strong> package</a></li>
<li class="chapter" data-level="5.14" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-conditionals"><i class="fa fa-check"></i><b>5.14</b> Tidyverse conditionals</a><ul>
<li class="chapter" data-level="5.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>5.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="5.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>5.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="tidyverse.html"><a href="tidyverse.html#exercises-11"><i class="fa fa-check"></i><b>5.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>6</b> Importing data</a><ul>
<li class="chapter" data-level="6.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>6.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="6.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>6.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="6.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>6.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="6.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>6.1.3</b> The working directory</a></li>
<li class="chapter" data-level="6.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>6.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="6.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>6.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>6.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="6.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>6.2.1</b> readr</a></li>
<li class="chapter" data-level="6.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>6.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="importing-data.html"><a href="importing-data.html#exercises-12"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
<li class="chapter" data-level="6.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>6.4</b> Downloading files</a></li>
<li class="chapter" data-level="6.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>6.5</b> R-base importing functions</a><ul>
<li class="chapter" data-level="6.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>6.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>6.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="6.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>6.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="6.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>6.8</b> Organizing Data with Spreadsheets</a></li>
<li class="chapter" data-level="6.9" data-path="importing-data.html"><a href="importing-data.html#exercises-13"><i class="fa fa-check"></i><b>6.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="7" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>7</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="8" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>8</b> ggplot2</a><ul>
<li class="chapter" data-level="8.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>8.1</b> The components of a graph</a></li>
<li class="chapter" data-level="8.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects"><i class="fa fa-check"></i><b>8.2</b> <code>ggplot</code> objects</a></li>
<li class="chapter" data-level="8.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>8.3</b> Geometries</a></li>
<li class="chapter" data-level="8.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>8.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="8.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>8.5</b> Layers</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>8.5.1</b> Tinkering with arguments</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>8.6</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="8.7" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>8.7</b> Scales</a></li>
<li class="chapter" data-level="8.8" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>8.8</b> Labels and titles</a></li>
<li class="chapter" data-level="8.9" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>8.9</b> Categories as colors</a></li>
<li class="chapter" data-level="8.10" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>8.10</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="8.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>8.11</b> Add-on packages</a></li>
<li class="chapter" data-level="8.12" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>8.12</b> Putting it all together</a></li>
<li class="chapter" data-level="8.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>8.13</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="8.14" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>8.14</b> Grids of plots</a></li>
<li class="chapter" data-level="8.15" data-path="ggplot2.html"><a href="ggplot2.html#exercises-14"><i class="fa fa-check"></i><b>8.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>9</b> Visualizing data distributions</a><ul>
<li class="chapter" data-level="9.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>9.1</b> Variable types</a></li>
<li class="chapter" data-level="9.2" data-path="distributions.html"><a href="distributions.html#case-study-describing-student-heights"><i class="fa fa-check"></i><b>9.2</b> Case study: describing student heights</a></li>
<li class="chapter" data-level="9.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>9.3</b> Distribution function</a></li>
<li class="chapter" data-level="9.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>9.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="9.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>9.5</b> Histograms</a></li>
<li class="chapter" data-level="9.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>9.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="9.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>9.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="9.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>9.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="distributions.html"><a href="distributions.html#exercises-15"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
<li class="chapter" data-level="9.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>9.8</b> The normal distribution</a></li>
<li class="chapter" data-level="9.9" data-path="distributions.html"><a href="distributions.html#standard-units"><i class="fa fa-check"></i><b>9.9</b> Standard units</a></li>
<li class="chapter" data-level="9.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>9.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="9.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>9.11</b> Percentiles</a></li>
<li class="chapter" data-level="9.12" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>9.12</b> Boxplots</a></li>
<li class="chapter" data-level="9.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>9.13</b> Stratification</a></li>
<li class="chapter" data-level="9.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>9.14</b> Case study: describing student heights (continued)</a></li>
<li class="chapter" data-level="9.15" data-path="distributions.html"><a href="distributions.html#exercises-16"><i class="fa fa-check"></i><b>9.15</b> Exercises</a></li>
<li class="chapter" data-level="9.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>9.16</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="9.16.1" data-path="distributions.html"><a href="distributions.html#barplots"><i class="fa fa-check"></i><b>9.16.1</b> Barplots</a></li>
<li class="chapter" data-level="9.16.2" data-path="distributions.html"><a href="distributions.html#histograms-1"><i class="fa fa-check"></i><b>9.16.2</b> Histograms</a></li>
<li class="chapter" data-level="9.16.3" data-path="distributions.html"><a href="distributions.html#density-plots"><i class="fa fa-check"></i><b>9.16.3</b> Density plots</a></li>
<li class="chapter" data-level="9.16.4" data-path="distributions.html"><a href="distributions.html#boxplots-1"><i class="fa fa-check"></i><b>9.16.4</b> Boxplots</a></li>
<li class="chapter" data-level="9.16.5" data-path="distributions.html"><a href="distributions.html#qq-plots"><i class="fa fa-check"></i><b>9.16.5</b> QQ-plots</a></li>
<li class="chapter" data-level="9.16.6" data-path="distributions.html"><a href="distributions.html#images"><i class="fa fa-check"></i><b>9.16.6</b> Images</a></li>
<li class="chapter" data-level="9.16.7" data-path="distributions.html"><a href="distributions.html#quick-plots"><i class="fa fa-check"></i><b>9.16.7</b> Quick plots</a></li>
</ul></li>
<li class="chapter" data-level="9.17" data-path="distributions.html"><a href="distributions.html#exercises-17"><i class="fa fa-check"></i><b>9.17</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>10</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="10.1" data-path="gapminder.html"><a href="gapminder.html#case-study-new-insights-on-poverty"><i class="fa fa-check"></i><b>10.1</b> Case study: new insights on poverty</a><ul>
<li class="chapter" data-level="10.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>10.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>10.2</b> Scatterplots</a></li>
<li class="chapter" data-level="10.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>10.3</b> Faceting</a><ul>
<li class="chapter" data-level="10.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>10.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="10.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>10.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>10.4</b> Time series plots</a><ul>
<li class="chapter" data-level="10.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>10.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="gapminder.html"><a href="gapminder.html#data-transformations"><i class="fa fa-check"></i><b>10.5</b> Data transformations</a><ul>
<li class="chapter" data-level="10.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>10.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="10.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>10.5.2</b> Which base?</a></li>
<li class="chapter" data-level="10.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>10.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="gapminder.html"><a href="gapminder.html#visualizing-multimodal-distributions"><i class="fa fa-check"></i><b>10.6</b> Visualizing multimodal distributions</a></li>
<li class="chapter" data-level="10.7" data-path="gapminder.html"><a href="gapminder.html#comparing-multiple-distributions-with-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>10.7</b> Comparing multiple distributions with boxplots and ridge plots</a><ul>
<li class="chapter" data-level="10.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-2"><i class="fa fa-check"></i><b>10.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="10.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>10.7.2</b> Ridge plots</a></li>
<li class="chapter" data-level="10.7.3" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>10.7.3</b> Example: 1970 versus 2010 income distributions</a></li>
<li class="chapter" data-level="10.7.4" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>10.7.4</b> Accessing computed variables</a></li>
<li class="chapter" data-level="10.7.5" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>10.7.5</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="gapminder.html"><a href="gapminder.html#the-ecological-fallacy-and-importance-of-showing-the-data"><i class="fa fa-check"></i><b>10.8</b> The ecological fallacy and importance of showing the data</a><ul>
<li class="chapter" data-level="10.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>10.8.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="10.8.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>10.8.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>11</b> Data visualization principles</a><ul>
<li class="chapter" data-level="11.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>11.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="11.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>11.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="11.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>11.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="11.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-categories-by-a-meaningful-value"><i class="fa fa-check"></i><b>11.4</b> Order categories by a meaningful value</a></li>
<li class="chapter" data-level="11.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>11.5</b> Show the data</a></li>
<li class="chapter" data-level="11.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons"><i class="fa fa-check"></i><b>11.6</b> Ease comparisons</a><ul>
<li class="chapter" data-level="11.6.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-common-axes"><i class="fa fa-check"></i><b>11.6.1</b> Use common axes</a></li>
<li class="chapter" data-level="11.6.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>11.6.2</b> Align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="11.6.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>11.6.3</b> Consider transformations</a></li>
<li class="chapter" data-level="11.6.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>11.6.4</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="11.6.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#use-color"><i class="fa fa-check"></i><b>11.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>11.7</b> Think of the color blind</a></li>
<li class="chapter" data-level="11.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#plots-for-two-variables"><i class="fa fa-check"></i><b>11.8</b> Plots for two variables</a><ul>
<li class="chapter" data-level="11.8.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>11.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="11.8.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>11.8.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>11.9</b> Encoding a third variable</a></li>
<li class="chapter" data-level="11.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>11.10</b> Avoid pseudo-three-dimensional plots</a></li>
<li class="chapter" data-level="11.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>11.11</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="11.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>11.12</b> Know your audience</a></li>
<li class="chapter" data-level="11.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-18"><i class="fa fa-check"></i><b>11.13</b> Exercises</a></li>
<li class="chapter" data-level="11.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#vaccines"><i class="fa fa-check"></i><b>11.14</b> Case study: impact of vaccines on battling infectious diseases</a></li>
<li class="chapter" data-level="11.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-19"><i class="fa fa-check"></i><b>11.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>12</b> Robust summaries</a><ul>
<li class="chapter" data-level="12.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>12.1</b> Outliers</a></li>
<li class="chapter" data-level="12.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>12.2</b> Median</a></li>
<li class="chapter" data-level="12.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>12.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="12.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>12.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="12.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>12.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="12.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-20"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
<li class="chapter" data-level="12.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>12.7</b> Case study: self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Statistics with R</b></span></li>
<li class="chapter" data-level="13" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>13</b> Introduction to Statistics with R</a></li>
<li class="chapter" data-level="14" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>14</b> Probability</a><ul>
<li class="chapter" data-level="14.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>14.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="14.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>14.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="14.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>14.1.2</b> Notation</a></li>
<li class="chapter" data-level="14.1.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>14.1.3</b> Probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-categorical-data"><i class="fa fa-check"></i><b>14.2</b> Monte Carlo simulations for categorical data</a><ul>
<li class="chapter" data-level="14.2.1" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>14.2.1</b> Setting the random seed</a></li>
<li class="chapter" data-level="14.2.2" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i><b>14.2.2</b> With and without replacement</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>14.3</b> Independence</a></li>
<li class="chapter" data-level="14.4" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>14.4</b> Conditional probabilities</a></li>
<li class="chapter" data-level="14.5" data-path="probability.html"><a href="probability.html#addition-and-multiplication-rules"><i class="fa fa-check"></i><b>14.5</b> Addition and multiplication rules</a><ul>
<li class="chapter" data-level="14.5.1" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>14.5.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="14.5.2" data-path="probability.html"><a href="probability.html#multiplication-rule-under-indepedence"><i class="fa fa-check"></i><b>14.5.2</b> Multiplication rule under indepedence</a></li>
<li class="chapter" data-level="14.5.3" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>14.5.3</b> Addition rule</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>14.6</b> Combinations and permutations</a><ul>
<li class="chapter" data-level="14.6.1" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>14.6.1</b> Monte Carlo example</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="probability.html"><a href="probability.html#examples"><i class="fa fa-check"></i><b>14.7</b> Examples</a><ul>
<li class="chapter" data-level="14.7.1" data-path="probability.html"><a href="probability.html#monty-hall-problem"><i class="fa fa-check"></i><b>14.7.1</b> Monty Hall problem</a></li>
<li class="chapter" data-level="14.7.2" data-path="probability.html"><a href="probability.html#birthday-problem"><i class="fa fa-check"></i><b>14.7.2</b> Birthday problem</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="probability.html"><a href="probability.html#infinity-in-practice"><i class="fa fa-check"></i><b>14.8</b> Infinity in practice</a></li>
<li class="chapter" data-level="14.9" data-path="probability.html"><a href="probability.html#exercises-21"><i class="fa fa-check"></i><b>14.9</b> Exercises</a></li>
<li class="chapter" data-level="14.10" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>14.10</b> Continuous probability</a></li>
<li class="chapter" data-level="14.11" data-path="probability.html"><a href="probability.html#theoretical-continuous-distributions"><i class="fa fa-check"></i><b>14.11</b> Theoretical continuous distributions</a><ul>
<li class="chapter" data-level="14.11.1" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>14.11.1</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="14.11.2" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>14.11.2</b> The probability density</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>14.12</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="14.13" data-path="probability.html"><a href="probability.html#continuous-distributions"><i class="fa fa-check"></i><b>14.13</b> Continuous distributions</a></li>
<li class="chapter" data-level="14.14" data-path="probability.html"><a href="probability.html#exercises-22"><i class="fa fa-check"></i><b>14.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>15</b> Random variables</a><ul>
<li class="chapter" data-level="15.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>15.1</b> Random variables</a></li>
<li class="chapter" data-level="15.2" data-path="random-variables.html"><a href="random-variables.html#sampling-models"><i class="fa fa-check"></i><b>15.2</b> Sampling models</a></li>
<li class="chapter" data-level="15.3" data-path="random-variables.html"><a href="random-variables.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>15.3</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="15.4" data-path="random-variables.html"><a href="random-variables.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>15.4</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="15.5" data-path="random-variables.html"><a href="random-variables.html#notation-for-random-variables"><i class="fa fa-check"></i><b>15.5</b> Notation for random variables</a></li>
<li class="chapter" data-level="15.6" data-path="random-variables.html"><a href="random-variables.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>15.6</b> The expected value and standard error</a><ul>
<li class="chapter" data-level="15.6.1" data-path="random-variables.html"><a href="random-variables.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>15.6.1</b> Population SD versus the sample SD</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="random-variables.html"><a href="random-variables.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.7</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="15.7.1" data-path="random-variables.html"><a href="random-variables.html#how-large-is-large-in-the-central-limit-theorem"><i class="fa fa-check"></i><b>15.7.1</b> How large is large in the Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="random-variables.html"><a href="random-variables.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>15.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="15.9" data-path="random-variables.html"><a href="random-variables.html#law-of-large-numbers"><i class="fa fa-check"></i><b>15.9</b> Law of large numbers</a><ul>
<li class="chapter" data-level="15.9.1" data-path="random-variables.html"><a href="random-variables.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>15.9.1</b> Misinterpreting law of averages</a></li>
</ul></li>
<li class="chapter" data-level="15.10" data-path="random-variables.html"><a href="random-variables.html#exercises-23"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li>
<li class="chapter" data-level="15.11" data-path="random-variables.html"><a href="random-variables.html#case-study-the-big-short"><i class="fa fa-check"></i><b>15.11</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="15.11.1" data-path="random-variables.html"><a href="random-variables.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>15.11.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="15.11.2" data-path="random-variables.html"><a href="random-variables.html#the-big-short"><i class="fa fa-check"></i><b>15.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="15.12" data-path="random-variables.html"><a href="random-variables.html#exercises-24"><i class="fa fa-check"></i><b>15.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>16</b> Statistical Inference</a><ul>
<li class="chapter" data-level="16.1" data-path="inference.html"><a href="inference.html#polls"><i class="fa fa-check"></i><b>16.1</b> Polls</a><ul>
<li class="chapter" data-level="16.1.1" data-path="inference.html"><a href="inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>16.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="inference.html"><a href="inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>16.2</b> Populations, samples, parameters and estimates</a><ul>
<li class="chapter" data-level="16.2.1" data-path="inference.html"><a href="inference.html#the-sample-average"><i class="fa fa-check"></i><b>16.2.1</b> The sample average</a></li>
<li class="chapter" data-level="16.2.2" data-path="inference.html"><a href="inference.html#parameters"><i class="fa fa-check"></i><b>16.2.2</b> Parameters</a></li>
<li class="chapter" data-level="16.2.3" data-path="inference.html"><a href="inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>16.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="16.2.4" data-path="inference.html"><a href="inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>16.2.4</b> Properties of our estimate: expected value and standard error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="inference.html"><a href="inference.html#exercises-25"><i class="fa fa-check"></i><b>16.3</b> Exercises</a></li>
<li class="chapter" data-level="16.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>16.4</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="16.4.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>16.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="16.4.2" data-path="inference.html"><a href="inference.html#the-spread"><i class="fa fa-check"></i><b>16.4.2</b> The spread</a></li>
<li class="chapter" data-level="16.4.3" data-path="inference.html"><a href="inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>16.4.3</b> Bias: why not run a very large poll?</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="inference.html"><a href="inference.html#exercises-26"><i class="fa fa-check"></i><b>16.5</b> Exercises</a></li>
<li class="chapter" data-level="16.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>16.6</b> Confidence intervals</a><ul>
<li class="chapter" data-level="16.6.1" data-path="inference.html"><a href="inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>16.6.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="16.6.2" data-path="inference.html"><a href="inference.html#the-correct-language"><i class="fa fa-check"></i><b>16.6.2</b> The correct language</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="inference.html"><a href="inference.html#exercises-27"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
<li class="chapter" data-level="16.8" data-path="inference.html"><a href="inference.html#power"><i class="fa fa-check"></i><b>16.8</b> Power</a></li>
<li class="chapter" data-level="16.9" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>16.9</b> p-values</a></li>
<li class="chapter" data-level="16.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>16.10</b> Association Tests</a><ul>
<li class="chapter" data-level="16.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>16.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="16.10.2" data-path="inference.html"><a href="inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>16.10.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="16.10.3" data-path="inference.html"><a href="inference.html#chi-square-test"><i class="fa fa-check"></i><b>16.10.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="16.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>16.10.4</b> The odds ratio</a></li>
<li class="chapter" data-level="16.10.5" data-path="inference.html"><a href="inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>16.10.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="16.10.6" data-path="inference.html"><a href="inference.html#small-count-correction"><i class="fa fa-check"></i><b>16.10.6</b> Small count correction</a></li>
<li class="chapter" data-level="16.10.7" data-path="inference.html"><a href="inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>16.10.7</b> Large samples, small p-values</a></li>
</ul></li>
<li class="chapter" data-level="16.11" data-path="inference.html"><a href="inference.html#exercises-28"><i class="fa fa-check"></i><b>16.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>17</b> Statistical models</a><ul>
<li class="chapter" data-level="17.1" data-path="models.html"><a href="models.html#poll-aggregators"><i class="fa fa-check"></i><b>17.1</b> Poll aggregators</a><ul>
<li class="chapter" data-level="17.1.1" data-path="models.html"><a href="models.html#poll-data"><i class="fa fa-check"></i><b>17.1.1</b> Poll data</a></li>
<li class="chapter" data-level="17.1.2" data-path="models.html"><a href="models.html#pollster-bias"><i class="fa fa-check"></i><b>17.1.2</b> Pollster bias</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>17.2</b> Data driven models</a></li>
<li class="chapter" data-level="17.3" data-path="models.html"><a href="models.html#exercises-29"><i class="fa fa-check"></i><b>17.3</b> Exercises</a></li>
<li class="chapter" data-level="17.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>17.4</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="17.4.1" data-path="models.html"><a href="models.html#bayes-theorem"><i class="fa fa-check"></i><b>17.4.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="models.html"><a href="models.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>17.5</b> Bayes Theorem simulation</a><ul>
<li class="chapter" data-level="17.5.1" data-path="models.html"><a href="models.html#bayes-in-practice"><i class="fa fa-check"></i><b>17.5.1</b> Bayes in practice</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="models.html"><a href="models.html#hierarchical-models"><i class="fa fa-check"></i><b>17.6</b> Hierarchical models</a></li>
<li class="chapter" data-level="17.7" data-path="models.html"><a href="models.html#exercises-30"><i class="fa fa-check"></i><b>17.7</b> Exercises</a></li>
<li class="chapter" data-level="17.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>17.8</b> Case study: Election forecasting</a><ul>
<li class="chapter" data-level="17.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>17.8.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="17.8.2" data-path="models.html"><a href="models.html#the-general-bias"><i class="fa fa-check"></i><b>17.8.2</b> The general bias</a></li>
<li class="chapter" data-level="17.8.3" data-path="models.html"><a href="models.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>17.8.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="17.8.4" data-path="models.html"><a href="models.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>17.8.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="17.8.5" data-path="models.html"><a href="models.html#forecasting"><i class="fa fa-check"></i><b>17.8.5</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="models.html"><a href="models.html#exercise-1"><i class="fa fa-check"></i><b>17.9</b> Exercise</a></li>
<li class="chapter" data-level="17.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>17.10</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>18</b> Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>18.1</b> Case study: is height hereditary?</a></li>
<li class="chapter" data-level="18.2" data-path="regression.html"><a href="regression.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>18.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="18.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>18.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="18.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>18.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>18.3</b> Conditional expectations</a></li>
<li class="chapter" data-level="18.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>18.4</b> The regression line</a><ul>
<li class="chapter" data-level="18.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>18.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="18.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>18.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="18.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>18.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="18.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>18.4.4</b> Warning: there are two regression lines</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="regression.html"><a href="regression.html#exercises-31"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Models</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball"><i class="fa fa-check"></i><b>19.1</b> Case Study: Moneyball</a><ul>
<li class="chapter" data-level="19.1.1" data-path="linear-models.html"><a href="linear-models.html#sabermetics"><i class="fa fa-check"></i><b>19.1.1</b> Sabermetics</a></li>
<li class="chapter" data-level="19.1.2" data-path="linear-models.html"><a href="linear-models.html#baseball-basics"><i class="fa fa-check"></i><b>19.1.2</b> Baseball basics</a></li>
<li class="chapter" data-level="19.1.3" data-path="linear-models.html"><a href="linear-models.html#no-awards-for-bb"><i class="fa fa-check"></i><b>19.1.3</b> No awards for BB</a></li>
<li class="chapter" data-level="19.1.4" data-path="linear-models.html"><a href="linear-models.html#base-on-balls-or-stolen-bases"><i class="fa fa-check"></i><b>19.1.4</b> Base on Balls or Stolen Bases?</a></li>
<li class="chapter" data-level="19.1.5" data-path="linear-models.html"><a href="linear-models.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>19.1.5</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="linear-models.html"><a href="linear-models.html#confounding"><i class="fa fa-check"></i><b>19.2</b> Confounding</a><ul>
<li class="chapter" data-level="19.2.1" data-path="linear-models.html"><a href="linear-models.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>19.2.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="19.2.2" data-path="linear-models.html"><a href="linear-models.html#multivariate-regression"><i class="fa fa-check"></i><b>19.2.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="linear-models.html"><a href="linear-models.html#lse"><i class="fa fa-check"></i><b>19.3</b> Least Squared Estimates</a><ul>
<li class="chapter" data-level="19.3.1" data-path="linear-models.html"><a href="linear-models.html#interpreting-linear-models"><i class="fa fa-check"></i><b>19.3.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="19.3.2" data-path="linear-models.html"><a href="linear-models.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>19.3.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="19.3.3" data-path="linear-models.html"><a href="linear-models.html#the-lm-function"><i class="fa fa-check"></i><b>19.3.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="19.3.4" data-path="linear-models.html"><a href="linear-models.html#lse-are-random-variables"><i class="fa fa-check"></i><b>19.3.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="19.3.5" data-path="linear-models.html"><a href="linear-models.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>19.3.5</b> Predicted values are random variables</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="linear-models.html"><a href="linear-models.html#exercises-32"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
<li class="chapter" data-level="19.5" data-path="linear-models.html"><a href="linear-models.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>19.5</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="19.5.1" data-path="linear-models.html"><a href="linear-models.html#the-broom-package"><i class="fa fa-check"></i><b>19.5.1</b> The broom package</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="linear-models.html"><a href="linear-models.html#exercises-33"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
<li class="chapter" data-level="19.7" data-path="linear-models.html"><a href="linear-models.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>19.7</b> Case study: Moneyball (continued)</a><ul>
<li class="chapter" data-level="19.7.1" data-path="linear-models.html"><a href="linear-models.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>19.7.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="19.7.2" data-path="linear-models.html"><a href="linear-models.html#picking-9-players"><i class="fa fa-check"></i><b>19.7.2</b> Picking 9 players</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="linear-models.html"><a href="linear-models.html#the-regression-fallacy"><i class="fa fa-check"></i><b>19.8</b> The regression fallacy</a></li>
<li class="chapter" data-level="19.9" data-path="linear-models.html"><a href="linear-models.html#measurement-error-models"><i class="fa fa-check"></i><b>19.9</b> Measurement error models</a></li>
<li class="chapter" data-level="19.10" data-path="linear-models.html"><a href="linear-models.html#exercises-34"><i class="fa fa-check"></i><b>19.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>20</b> Association is not causation</a><ul>
<li class="chapter" data-level="20.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>20.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="20.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>20.2</b> Outliers</a></li>
<li class="chapter" data-level="20.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>20.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="20.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>20.4</b> Confounders</a><ul>
<li class="chapter" data-level="20.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>20.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="20.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>20.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="20.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>20.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>20.5</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="20.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-35"><i class="fa fa-check"></i><b>20.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="21" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>21</b> Introduction to Data Wrangling</a></li>
<li class="chapter" data-level="22" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>22</b> Reshaping data</a><ul>
<li class="chapter" data-level="22.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>22.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="22.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>22.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="22.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>22.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="22.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>22.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="22.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-36"><i class="fa fa-check"></i><b>22.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>23</b> Joining tables</a><ul>
<li class="chapter" data-level="23.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>23.1</b> Joins</a><ul>
<li class="chapter" data-level="23.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>23.1.1</b> Left join</a></li>
<li class="chapter" data-level="23.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>23.1.2</b> Right join</a></li>
<li class="chapter" data-level="23.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>23.1.3</b> Inner join</a></li>
<li class="chapter" data-level="23.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>23.1.4</b> Full join</a></li>
<li class="chapter" data-level="23.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>23.1.5</b> Semi join</a></li>
<li class="chapter" data-level="23.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>23.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>23.2</b> Binding</a><ul>
<li class="chapter" data-level="23.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>23.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="23.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>23.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>23.3</b> Set operators</a><ul>
<li class="chapter" data-level="23.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>23.3.1</b> Intersect</a></li>
<li class="chapter" data-level="23.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>23.3.2</b> Union</a></li>
<li class="chapter" data-level="23.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>23.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="23.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>23.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-37"><i class="fa fa-check"></i><b>23.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>24</b> Web Scraping</a><ul>
<li class="chapter" data-level="24.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>24.1</b> HTML</a></li>
<li class="chapter" data-level="24.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>24.2</b> The rvest package</a></li>
<li class="chapter" data-level="24.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>24.3</b> CSS selectors</a></li>
<li class="chapter" data-level="24.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>24.4</b> JSON</a></li>
<li class="chapter" data-level="24.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-38"><i class="fa fa-check"></i><b>24.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>25</b> String Processing</a><ul>
<li class="chapter" data-level="25.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>25.1</b> The stringr package</a></li>
<li class="chapter" data-level="25.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>25.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="25.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>25.3</b> Case study 2: self reported heights</a></li>
<li class="chapter" data-level="25.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>25.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="25.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>25.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="25.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>25.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="25.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>25.5.2</b> Special characters</a></li>
<li class="chapter" data-level="25.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>25.5.3</b> Character classes</a></li>
<li class="chapter" data-level="25.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>25.5.4</b> Anchors</a></li>
<li class="chapter" data-level="25.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>25.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="25.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>25.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="25.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>25.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="25.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>25.5.8</b> Not</a></li>
<li class="chapter" data-level="25.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>25.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>25.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="25.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>25.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>25.7</b> Testing and improving</a></li>
<li class="chapter" data-level="25.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>25.8</b> Trimming</a></li>
<li class="chapter" data-level="25.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>25.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="25.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>25.10</b> Case study 2: self reported heights (continued)</a><ul>
<li class="chapter" data-level="25.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>25.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="25.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>25.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="25.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>25.11</b> String splitting</a></li>
<li class="chapter" data-level="25.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>25.12</b> Case study 3: extracting tables from a PDF</a></li>
<li class="chapter" data-level="25.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>25.13</b> Recoding</a></li>
<li class="chapter" data-level="25.14" data-path="string-processing.html"><a href="string-processing.html#exercises-39"><i class="fa fa-check"></i><b>25.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>26</b> Parsing Dates and Times</a><ul>
<li class="chapter" data-level="26.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>26.1</b> The date data type</a></li>
<li class="chapter" data-level="26.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>26.2</b> The lubridate package</a></li>
<li class="chapter" data-level="26.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-40"><i class="fa fa-check"></i><b>26.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>27</b> Text mining</a><ul>
<li class="chapter" data-level="27.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>27.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="27.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>27.2</b> Text as data</a></li>
<li class="chapter" data-level="27.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>27.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="27.4" data-path="text-mining.html"><a href="text-mining.html#exercises-41"><i class="fa fa-check"></i><b>27.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="28" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>28</b> Introduction to Machine Learning</a><ul>
<li class="chapter" data-level="28.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>28.1</b> Notation</a></li>
<li class="chapter" data-level="28.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>28.2</b> An example</a></li>
<li class="chapter" data-level="28.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-42"><i class="fa fa-check"></i><b>28.3</b> Exercises</a></li>
<li class="chapter" data-level="28.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>28.4</b> Evaluation Metrics</a><ul>
<li class="chapter" data-level="28.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>28.4.1</b> Training and test sets</a></li>
<li class="chapter" data-level="28.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>28.4.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="28.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>28.4.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="28.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>28.4.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="28.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>28.4.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="28.4.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>28.4.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="28.4.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>28.4.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="28.4.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>28.4.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="28.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-43"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
<li class="chapter" data-level="28.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>28.6</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="28.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>28.6.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="28.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>28.6.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="28.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>28.6.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="28.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-44"><i class="fa fa-check"></i><b>28.7</b> Exercises</a></li>
<li class="chapter" data-level="28.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>28.8</b> Case study: is it a 2 or a 7?</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>29</b> Smoothing</a><ul>
<li class="chapter" data-level="29.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>29.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="29.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>29.2</b> Kernels</a></li>
<li class="chapter" data-level="29.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>29.3</b> Local weighted regression (loess)</a><ul>
<li class="chapter" data-level="29.3.1" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>29.3.1</b> Fitting parabolas</a></li>
<li class="chapter" data-level="29.3.2" data-path="smoothing.html"><a href="smoothing.html#beware-of-default-smoothing-parameters"><i class="fa fa-check"></i><b>29.3.2</b> Beware of default smoothing parameters</a></li>
</ul></li>
<li class="chapter" data-level="29.4" data-path="smoothing.html"><a href="smoothing.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>29.4</b> Connecting smoothing to machine learning</a></li>
<li class="chapter" data-level="29.5" data-path="smoothing.html"><a href="smoothing.html#exercises-45"><i class="fa fa-check"></i><b>29.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>30</b> Cross validation</a><ul>
<li class="chapter" data-level="30.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>30.1</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="30.1.1" data-path="cross-validation.html"><a href="cross-validation.html#over-training"><i class="fa fa-check"></i><b>30.1.1</b> Over-training</a></li>
<li class="chapter" data-level="30.1.2" data-path="cross-validation.html"><a href="cross-validation.html#over-smoothing"><i class="fa fa-check"></i><b>30.1.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="30.1.3" data-path="cross-validation.html"><a href="cross-validation.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>30.1.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="cross-validation.html"><a href="cross-validation.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>30.2</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="30.3" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>30.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="30.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-46"><i class="fa fa-check"></i><b>30.4</b> Exercises</a></li>
<li class="chapter" data-level="30.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>30.5</b> Bootstrap</a></li>
<li class="chapter" data-level="30.6" data-path="cross-validation.html"><a href="cross-validation.html#exercises-47"><i class="fa fa-check"></i><b>30.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>31</b> The caret package</a><ul>
<li class="chapter" data-level="31.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>31.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="31.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>31.2</b> Cross validation</a></li>
<li class="chapter" data-level="31.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>31.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>32</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="32.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>32.1</b> Linear regression</a><ul>
<li class="chapter" data-level="32.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>32.1.1</b> The <code>predict</code> function</a></li>
</ul></li>
<li class="chapter" data-level="32.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-48"><i class="fa fa-check"></i><b>32.2</b> Exercises</a></li>
<li class="chapter" data-level="32.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>32.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="32.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>32.3.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="32.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression-with-more-than-one-predictor"><i class="fa fa-check"></i><b>32.3.2</b> Logistic regression with more than one predictor</a></li>
</ul></li>
<li class="chapter" data-level="32.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-49"><i class="fa fa-check"></i><b>32.4</b> Exercises</a></li>
<li class="chapter" data-level="32.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>32.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="32.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-50"><i class="fa fa-check"></i><b>32.6</b> Exercises</a></li>
<li class="chapter" data-level="32.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>32.7</b> Generative models</a><ul>
<li class="chapter" data-level="32.7.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>32.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="32.7.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>32.7.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="32.7.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>32.7.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="32.7.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>32.7.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="32.7.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>32.7.5</b> Connection to distance</a></li>
<li class="chapter" data-level="32.7.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>32.7.6</b> Case study: more than three classes</a></li>
</ul></li>
<li class="chapter" data-level="32.8" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-51"><i class="fa fa-check"></i><b>32.8</b> Exercises</a></li>
<li class="chapter" data-level="32.9" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>32.9</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="32.9.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>32.9.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="32.9.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>32.9.2</b> CART motivation</a></li>
<li class="chapter" data-level="32.9.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>32.9.3</b> Regression trees</a></li>
<li class="chapter" data-level="32.9.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>32.9.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="32.10" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>32.10</b> Random Forests</a></li>
<li class="chapter" data-level="32.11" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-52"><i class="fa fa-check"></i><b>32.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>33</b> Machine learning in practice</a><ul>
<li class="chapter" data-level="33.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>33.1</b> Preprocessing</a></li>
<li class="chapter" data-level="33.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest"><i class="fa fa-check"></i><b>33.2</b> k-Nearest Neighbor and Random Forest</a></li>
<li class="chapter" data-level="33.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>33.3</b> Variable importance</a></li>
<li class="chapter" data-level="33.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>33.4</b> Visual assessments</a></li>
<li class="chapter" data-level="33.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>33.5</b> Ensembles</a></li>
<li class="chapter" data-level="33.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-53"><i class="fa fa-check"></i><b>33.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>34</b> Large datasets</a><ul>
<li class="chapter" data-level="34.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>34.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="34.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>34.1.1</b> Notation</a></li>
<li class="chapter" data-level="34.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>34.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="34.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>34.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="34.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>34.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="34.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>34.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="34.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>34.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="34.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>34.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="34.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>34.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="34.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>34.1.9</b> Matrix algebra operations</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="large-datasets.html"><a href="large-datasets.html#exercises-54"><i class="fa fa-check"></i><b>34.2</b> Exercises</a></li>
<li class="chapter" data-level="34.3" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>34.3</b> Distance</a><ul>
<li class="chapter" data-level="34.3.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>34.3.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="34.3.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>34.3.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="34.3.3" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance-example"><i class="fa fa-check"></i><b>34.3.3</b> Euclidean distance example</a></li>
<li class="chapter" data-level="34.3.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>34.3.4</b> Predictor Space</a></li>
<li class="chapter" data-level="34.3.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>34.3.5</b> Distance between predictors</a></li>
</ul></li>
<li class="chapter" data-level="34.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-55"><i class="fa fa-check"></i><b>34.4</b> Exercises</a></li>
<li class="chapter" data-level="34.5" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>34.5</b> Dimension reduction</a><ul>
<li class="chapter" data-level="34.5.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>34.5.1</b> Preserving distance</a></li>
<li class="chapter" data-level="34.5.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advanced"><i class="fa fa-check"></i><b>34.5.2</b> Linear transformations (advanced)</a></li>
<li class="chapter" data-level="34.5.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogonal-transformations-advanced"><i class="fa fa-check"></i><b>34.5.3</b> Orthogonal transformations (advanced)</a></li>
<li class="chapter" data-level="34.5.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>34.5.4</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="34.5.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>34.5.5</b> Iris Example</a></li>
<li class="chapter" data-level="34.5.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>34.5.6</b> MNIST Example</a></li>
</ul></li>
<li class="chapter" data-level="34.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>34.6</b> Exercises</a></li>
<li class="chapter" data-level="34.7" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>34.7</b> Recommendation systems</a><ul>
<li class="chapter" data-level="34.7.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>34.7.1</b> Movielens data</a></li>
<li class="chapter" data-level="34.7.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>34.7.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="34.7.3" data-path="large-datasets.html"><a href="large-datasets.html#netflix-loss-function"><i class="fa fa-check"></i><b>34.7.3</b> Loss function</a></li>
<li class="chapter" data-level="34.7.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>34.7.4</b> A first model</a></li>
<li class="chapter" data-level="34.7.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>34.7.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="34.7.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>34.7.6</b> User effects</a></li>
</ul></li>
<li class="chapter" data-level="34.8" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>34.8</b> Exercises</a></li>
<li class="chapter" data-level="34.9" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>34.9</b> Regularization</a><ul>
<li class="chapter" data-level="34.9.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>34.9.1</b> Motivation</a></li>
<li class="chapter" data-level="34.9.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>34.9.2</b> Penalized Least Squares</a></li>
<li class="chapter" data-level="34.9.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>34.9.3</b> Choosing the penalty terms</a></li>
</ul></li>
<li class="chapter" data-level="34.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>34.10</b> Exercises</a></li>
<li class="chapter" data-level="34.11" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>34.11</b> Matrix factorization</a><ul>
<li class="chapter" data-level="34.11.1" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>34.11.1</b> Factors</a></li>
<li class="chapter" data-level="34.11.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>34.11.2</b> Connection to SVD and PCA</a></li>
</ul></li>
<li class="chapter" data-level="34.12" data-path="large-datasets.html"><a href="large-datasets.html#exercises-59"><i class="fa fa-check"></i><b>34.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>35</b> Clustering</a><ul>
<li class="chapter" data-level="35.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>35.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="35.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>35.2</b> k-means</a></li>
<li class="chapter" data-level="35.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>35.3</b> Heatmaps</a></li>
<li class="chapter" data-level="35.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>35.4</b> Filtering features</a></li>
<li class="chapter" data-level="35.5" data-path="clustering.html"><a href="clustering.html#exercises-60"><i class="fa fa-check"></i><b>35.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity tools</b></span></li>
<li class="chapter" data-level="36" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>36</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="37" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html"><i class="fa fa-check"></i><b>37</b> Accessing the terminal and installing Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>37.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>37.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accessing-the-terminal-and-installing-git.html"><a href="accessing-the-terminal-and-installing-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#naming-convention"><i class="fa fa-check"></i><b>38.1</b> Naming convention</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> The terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> The filesystem</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>38.3.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>38.3.2</b> The home directory</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Working directory</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>38.4</b> Unix commands</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: Navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>38.5</b> Some examples</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>38.6</b> More Unix commands</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>38.8</b> Advanced Unix</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>38.8.1</b> Arguments</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>38.8.2</b> Getting help</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>38.8.3</b> Pipes</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>38.8.4</b> Wild cards</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>38.8.5</b> Environment variables</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> Shells</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>38.8.7</b> Executables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>38.8.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>38.8.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>38.8.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git and GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>39.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#github-accounts"><i class="fa fa-check"></i><b>39.2</b> GitHub accounts</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> GitHub repositories</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Overview of Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>39.4.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Initializing a Git directory</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Reproducible projects with RStudio and R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#rstudio-projects"><i class="fa fa-check"></i><b>40.1</b> RStudio projects</a></li>
<li class="chapter" data-level="40.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#the-header"><i class="fa fa-check"></i><b>40.2.1</b> The header</a></li>
<li class="chapter" data-level="40.2.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>40.2.2</b> R code chunks</a></li>
<li class="chapter" data-level="40.2.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#global-options"><i class="fa fa-check"></i><b>40.2.3</b> Global options</a></li>
<li class="chapter" data-level="40.2.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> More on R markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizing a data science project</a><ul>
<li class="chapter" data-level="40.3.1" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-directories-in-unix"><i class="fa fa-check"></i><b>40.3.1</b> Create directories in Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-an-rstudio-project"><i class="fa fa-check"></i><b>40.3.2</b> Create an RStudio project</a></li>
<li class="chapter" data-level="40.3.3" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#edit-some-r-scripts"><i class="fa fa-check"></i><b>40.3.3</b> Edit some R Scripts</a></li>
<li class="chapter" data-level="40.3.4" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#create-some-more-directories-using-unix"><i class="fa fa-check"></i><b>40.3.4</b> Create some more directories using Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-a-readme-file"><i class="fa fa-check"></i><b>40.3.5</b> Add a README file</a></li>
<li class="chapter" data-level="40.3.6" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#initilazing-a-git-directory"><i class="fa fa-check"></i><b>40.3.6</b> Initilazing a Git directory</a></li>
<li class="chapter" data-level="40.3.7" data-path="reproducible-projects-with-rstudio-and-r-markdown.html"><a href="reproducible-projects-with-rstudio-and-r-markdown.html#add-commit-and-push-files-using-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Add, commit and push files using RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-models" class="section level1">
<h1><span class="header-section-number">Chapter 19</span> Linear Models</h1>
<p>Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.</p>
<p>When we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this chapter we learn how linear models can help with such situations and can be used to describe how one or more variables affect an outcome varialbe.</p>
<div id="case-study-moneyball" class="section level2">
<h2><span class="header-section-number">19.1</span> Case Study: Moneyball</h2>
<p><em>Moneyball: The Art of Winning an Unfair Game</em> is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.</p>
<p>Traditionally, baseball teams use <em>scouts</em> to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.</p>
<p>From 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.</p>
<p>As motivation for this chapter, we will pretend it is 2002 and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:</p>
<p><img src="book_files/figure-html/mlb-2002-payroll-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div id="sabermetics" class="section level3">
<h3><span class="header-section-number">19.1.1</span> Sabermetics</h3>
<p>Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the <strong>Lahman</strong> library, goes back to the 19th century. For example, a summary statistics we will describe soon, the <em>batting average</em>, has been used for decades to summarize a batter’s success. <a href="http://mlb.mlb.com/stats/league_leaders.jsp">Other statistics</a> such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.</p>
<p>This changed with <a href="https://en.wikipedia.org/wiki/Bill_James">Bill James</a>. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win <a href="https://en.wikipedia.org/wiki/Sabermetrics"><em>sabermetrics</em></a>. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.</p>
<p>In this chapter, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.</p>
</div>
<div id="baseball-basics" class="section level3">
<h3><span class="header-section-number">19.1.2</span> Baseball basics</h3>
<p>To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.</p>
<p>The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s <em>pitcher</em> throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an <em>out</em> (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as <em>innings</em>, to score runs and each inning ends after three outs (three failures).</p>
<p>Here is a video showing a <a href="https://www.youtube.com/watch?v=HL-XjMCPfio">success</a> and here one showing a <a href="https://www.youtube.com/watch?v=NeloljCx-1g">failure</a>. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.</p>
<p>Now there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many <em>bases</em> as possible. There are four bases with the fourth one called <em>home plate</em>. Home plate is where batters start by trying to hit, so the bases form a cycle.</p>
<p><img src="regression/img/Baseball_Diamond1.png" width="50%" style="display: block; margin: auto;" /> (Source: <a href="https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg">Wikipedia Commons</a>)</p>
<p>A batter who <em>goes around the bases</em> and arrives home, scores a run.</p>
<p>We are simplifying a bit, but there are five ways a batter can succeed, that is not make an out:</p>
<ul>
<li>Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.</li>
<li>Single - Batter hits the ball and gets to first base.</li>
<li>Double (2B) - Batter hits the ball and gets to second base.</li>
<li>Triple (3B) - Batter hits the ball and gets to third base.</li>
<li>Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.</li>
</ul>
<p><a href="https://www.youtube.com/watch?v=xYxSZJ9GZ-w">Here</a> is an example of a HR.</p>
<p>If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is <em>on base</em>, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. <a href="https://www.youtube.com/watch?v=JSE5kfxkzfk">Here</a> is an example of a stolen base.</p>
<p>All these events are kept track of during the season and are available to us through the <strong>Lahman</strong> package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.</p>
</div>
<div id="no-awards-for-bb" class="section level3">
<h3><span class="header-section-number">19.1.3</span> No awards for BB</h3>
<p><img src="regression/img/JumboTron.png" width="70%" style="display: block; margin: auto;" /></p>
<p>(Source: <a href="https://twitter.com/jvrCTV/status/640230776806768640">John Vennavally-Rao</a>)</p>
<p>Historically, the <em>batting average</em> has been considered the most important offensive statistic. To define this average, we define a <em>hit</em> (H) and an <em>at bat</em> (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it <em>batting 280</em>.</p>
<p>One of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the <em>on base percentage</em> (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OPB as an important statistic. In contrast, total stolen bases were considered important and an <a href="http://www.baseball-almanac.com/awards/lou_brock_award.shtml">award</a> given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?</p>
</div>
<div id="base-on-balls-or-stolen-bases" class="section level3">
<h3><span class="header-section-number">19.1.4</span> Base on Balls or Stolen Bases?</h3>
<p>One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.</p>
<p>Let’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lahman)

Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_per_game =</span> HR <span class="op">/</span><span class="st"> </span>G, <span class="dt">R_per_game =</span> R <span class="op">/</span><span class="st"> </span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(HR_per_game, R_per_game)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/runs-vs-hrs-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">SB_per_game =</span> SB <span class="op">/</span><span class="st"> </span>G, <span class="dt">R_per_game =</span> R <span class="op">/</span><span class="st"> </span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(SB_per_game, R_per_game)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/runs-vs-sb-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BB_per_game =</span> BB<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(BB_per_game, R_per_game)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/runs-vs-bb-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Here again we see a clear association. But does this mean that increasing a team’s BBs <strong>causes</strong> an increase in runs? One of the most important lessons you learn in this book is that <strong>assocation is not causation.</strong></p>
<p>In fact, it looks like BB and HRs are also associated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_per_game =</span> HR<span class="op">/</span>G, <span class="dt">BB_per_game =</span> BB<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(HR_per_game, BB_per_game)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/bb-vs-hrs-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We know that HR cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is <em>confounding</em>, an important concept we will learn more about throughout this chapter.</p>
<p>Linear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.</p>
</div>
<div id="regression-applied-to-baseball-statistics" class="section level3">
<h3><span class="header-section-number">19.1.5</span> Regression applied to baseball statistics</h3>
<p>Can we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We save the plot into the object <code>p</code> as we will use it again later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lahman)
p &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_per_game =</span> HR<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(HR_per_game, R_per_game)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)
p</code></pre></div>
<p><img src="book_files/figure-html/hr-runs-bivariate-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The qq-plots confirm that the normal approximation is useful here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">z_HR =</span> <span class="kw">round</span>((HR <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(HR))<span class="op">/</span><span class="kw">sd</span>(HR)), 
         <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(z_HR <span class="op">%in%</span><span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">:</span><span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">stat_qq</span>(<span class="kw">aes</span>(<span class="dt">sample=</span>R_per_game)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>z_HR) </code></pre></div>
<p><img src="book_files/figure-html/hr-by-runs-qq-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Now we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">summary_stats &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_per_game =</span> HR<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg_HR =</span> <span class="kw">mean</span>(HR_per_game),
            <span class="dt">s_HR =</span> <span class="kw">sd</span>(HR_per_game),
            <span class="dt">avg_R =</span> <span class="kw">mean</span>(R_per_game),
            <span class="dt">s_R =</span> <span class="kw">sd</span>(R_per_game),
            <span class="dt">r =</span> <span class="kw">cor</span>(HR_per_game, R_per_game))
summary_stats
<span class="co">#&gt;   avg_HR  s_HR avg_R   s_R     r</span>
<span class="co">#&gt; 1  0.855 0.243  4.36 0.589 0.762</span></code></pre></div>
<p>and use the formulas given above to create the regression lines:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg_line &lt;-<span class="st"> </span>summary_stats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">slope =</span> r<span class="op">*</span>s_R<span class="op">/</span>s_HR,
                            <span class="dt">intercept =</span> avg_R <span class="op">-</span><span class="st"> </span>slope<span class="op">*</span>avg_HR)

p <span class="op">+</span><span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> reg_line<span class="op">$</span>intercept, <span class="dt">slope =</span> reg_line<span class="op">$</span>slope)</code></pre></div>
<p><img src="book_files/figure-html/hr-versus-runs-regression-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Soon we will learn R functions, such as <code>lm</code>, that make fitting regression lines much easier. Another example is the <strong>ggplot2</strong> function <code>geom_smooth</code> which computes and adds a regression line to plot along with confidence intervals, which we also learn about later. We use the argument <code>method = &quot;lm&quot;</code> which stands for <em>linear model</em>, the title of an upcoming section. So we can simplify the code above like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/hr-versus-runs-regression-easy-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the example above, the slope is 1.845. So this tells us that teams that hit 1 more HR per game than the average team, score 1.845 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.</p>

</div>
</div>
<div id="confounding" class="section level2">
<h2><span class="header-section-number">19.2</span> Confounding</h2>
<p>Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(Lahman)
bb_slope &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BB_per_game =</span> BB<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">lm</span>(R_per_game <span class="op">~</span><span class="st"> </span>BB_per_game, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>coef <span class="op">%&gt;%</span>
<span class="st">  </span>.[<span class="dv">2</span>]

bb_slope 
<span class="co">#&gt; BB_per_game </span>
<span class="co">#&gt;       0.735</span></code></pre></div>
<p>So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?</p>
<p>We are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.</p>
<p>Note that if we compute the regression line slope for singles we get:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">singles_slope &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Singles_per_game =</span> (H<span class="op">-</span>HR<span class="op">-</span>X2B<span class="op">-</span>X3B)<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">lm</span>(R_per_game <span class="op">~</span><span class="st"> </span>Singles_per_game, <span class="dt">data =</span> .) <span class="op">%&gt;%</span>
<span class="st">  </span>.<span class="op">$</span>coef  <span class="op">%&gt;%</span>
<span class="st">  </span>.[<span class="dv">2</span>]

singles_slope 
<span class="co">#&gt; Singles_per_game </span>
<span class="co">#&gt;            0.449</span></code></pre></div>
<p>which is a lower value than what we obtain for BB.</p>
<p>Also, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Singles =</span> (H<span class="op">-</span>HR<span class="op">-</span>X2B<span class="op">-</span>X3B)<span class="op">/</span>G, <span class="dt">BB =</span> BB<span class="op">/</span>G, <span class="dt">HR =</span> HR<span class="op">/</span>G) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">cor</span>(BB, HR), <span class="kw">cor</span>(Singles, HR), <span class="kw">cor</span>(BB,Singles))
<span class="co">#&gt;   cor(BB, HR) cor(Singles, HR) cor(BB, Singles)</span>
<span class="co">#&gt; 1       0.404           -0.174          -0.0561</span></code></pre></div>
<p>It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BB and a team with many HR will also have more BB. Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are <em>confounded</em> with HR. Nonetheless, could it be that BB still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.</p>
<div id="understanding-confounding-through-stratification" class="section level3">
<h3><span class="header-section-number">19.2.1</span> Understanding confounding through stratification</h3>
<p>A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_strata =</span> <span class="kw">round</span>(HR<span class="op">/</span>G, <span class="dv">1</span>), 
         <span class="dt">BB_per_game =</span> BB <span class="op">/</span><span class="st"> </span>G,
         <span class="dt">R_per_game =</span> R <span class="op">/</span><span class="st"> </span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(HR_strata <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.4</span> <span class="op">&amp;</span><span class="st"> </span>HR_strata <span class="op">&lt;=</span><span class="fl">1.2</span>) </code></pre></div>
<p>and then make a scatterplot for each strata:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(BB_per_game, R_per_game)) <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>( <span class="op">~</span><span class="st"> </span>HR_strata) </code></pre></div>
<p><img src="book_files/figure-html/runs-vs-bb-by-hr-strata-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Remember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR_strata) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">slope =</span> <span class="kw">cor</span>(BB_per_game, R_per_game)<span class="op">*</span><span class="kw">sd</span>(R_per_game)<span class="op">/</span><span class="kw">sd</span>(BB_per_game))
<span class="co">#&gt; # A tibble: 9 x 2</span>
<span class="co">#&gt;   HR_strata slope</span>
<span class="co">#&gt;       &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1       0.4 0.734</span>
<span class="co">#&gt; 2       0.5 0.566</span>
<span class="co">#&gt; 3       0.6 0.412</span>
<span class="co">#&gt; 4       0.7 0.285</span>
<span class="co">#&gt; 5       0.8 0.365</span>
<span class="co">#&gt; 6       0.9 0.261</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>The slopes are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as previously thought. In fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.</p>
<p>Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:</p>
<p><img src="book_files/figure-html/runs-vs-hr-by-bb-strata-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In this case, the slopes do not change much from the original:</p>
<pre><code>#&gt; # A tibble: 12 x 2
#&gt;   BB_strata slope
#&gt;       &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1       2.8  1.53
#&gt; 2       2.9  1.57
#&gt; 3       3    1.52
#&gt; 4       3.1  1.49
#&gt; 5       3.2  1.58
#&gt; 6       3.3  1.56
#&gt; # ... with 6 more rows</code></pre>
<p>They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hr_slope &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span> ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR_per_game =</span> HR<span class="op">/</span>G, <span class="dt">R_per_game =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">lm</span>(R_per_game <span class="op">~</span><span class="st"> </span>HR_per_game, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>coef  <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.[<span class="dv">2</span>]

hr_slope
<span class="co">#&gt; HR_per_game </span>
<span class="co">#&gt;        1.84</span></code></pre></div>
<p>Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.</p>
</div>
<div id="multivariate-regression" class="section level3">
<h3><span class="header-section-number">19.2.2</span> Multivariate regression</h3>
<p>It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
\]</span></p>
<p>with the slopes for <span class="math inline">\(x_1\)</span> changing for different values of <span class="math inline">\(x_2\)</span> and vice versa. But is there an easier approach?</p>
<p>If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that <span class="math inline">\(\beta_1(x_2)\)</span> and <span class="math inline">\(\beta_2(x_1)\)</span> are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>This model suggests that if the number of HR is fixed at <span class="math inline">\(x_2\)</span>, we observe a linear relationship between runs and BB with an intercept of <span class="math inline">\(\beta_0 + \beta_2 x_2\)</span>. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by <span class="math inline">\(\beta_1 x_1\)</span>.</p>
<p>In this analysis, referred to as <em>multivariate regression</em>, you will often hear people say that the BB slope <span class="math inline">\(\beta_1\)</span> is <em>adjusted</em> for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> from the data? For this, we learn about linear models and least squares estimates.</p>
</div>
</div>
<div id="lse" class="section level2">
<h2><span class="header-section-number">19.3</span> Least Squared Estimates</h2>
<p>We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a <em>linear model</em>.</p>
<p>We note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combinations of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, <span class="math inline">\(3x - 4y + 5z\)</span> is a linear combination of <span class="math inline">\(x, y\)</span> and <span class="math inline">\(z\)</span>. We can also add a constant so <span class="math inline">\(2 + 3x - 4y + 5z\)</span> is also linear combination of <span class="math inline">\(x, y\)</span> and <span class="math inline">\(z\)</span>.</p>
<p>So <span class="math inline">\(\beta_0 + \beta_1 x_1 + \beta_2 x_2\)</span>, is a linear combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The simplest linear model is a constant <span class="math inline">\(\beta_0\)</span>; the second simplest is a line <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. If we were to specify a linear model for Galton’s data, we would denote the <span class="math inline">\(N\)</span> observed father heights with <span class="math inline">\(x_1, \dots, x_n\)</span>, then we model the <span class="math inline">\(N\)</span> son heights we are trying to predict with:</p>
<p><span class="math display">\[ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \, i=1,\dots,N. 
\]</span></p>
<p>Here <span class="math inline">\(x_i\)</span> is the father’s height, which is fixed (not random) due to the conditioning, and <span class="math inline">\(Y_i\)</span> is the random son’s height that we want to predict. We further assume that <span class="math inline">\(\varepsilon_i\)</span> are independent from each other, have expected value 0 and the standard deviation, call it <span class="math inline">\(\sigma\)</span>, does not depend on <span class="math inline">\(i\)</span>.</p>
<p>In the above model, we know the <span class="math inline">\(x_i\)</span>, but to have a useful model for prediction, we need <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height <span class="math inline">\(x\)</span>. We show how to do this in the next section.</p>
<p>Note that if we further assume that the <span class="math inline">\(\varepsilon\)</span> is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the <span class="math inline">\(\varepsilon\)</span>s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.</p>
<div id="interpreting-linear-models" class="section level3">
<h3><span class="header-section-number">19.3.1</span> Interpreting linear models</h3>
<p>One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by <span class="math inline">\(\beta_1\)</span> for each inch we increase the father’s height <span class="math inline">\(x\)</span>. Because not all sons with fathers of height <span class="math inline">\(x\)</span> are of equal height, we need the term <span class="math inline">\(\varepsilon\)</span>, which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.</p>
<p>Given how we wrote the model above, the intercept <span class="math inline">\(\beta_0\)</span> is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:</p>
<p><span class="math display">\[ 
Y_i = \beta_0 + \beta_1 (x_i - \bar{x}) + \varepsilon_i, \, i=1,\dots,N 
\]</span></p>
<p>with <span class="math inline">\(\bar{x} = 1/N \sum_{i=1}^N x_i\)</span> the average of the <span class="math inline">\(x\)</span>. In this case <span class="math inline">\(\beta_0\)</span> represents the height when <span class="math inline">\(x_i = \bar{x}\)</span>, which is the height of the son of an average father.</p>
</div>
<div id="least-squares-estimates-lse" class="section level3">
<h3><span class="header-section-number">19.3.2</span> Least Squares Estimates (LSE)</h3>
<p>For linear models to be useful, we have to estimate the unknown <span class="math inline">\(\beta\)</span>s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this chapter. For Galton’s data, we would write:</p>
<p><span class="math display">\[ 
RSS = \sum_{i=1}^n \left\{  y_i - \left(\beta_0 + \beta_1 x_i \right)\right\}^2 
\]</span></p>
<p>This quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Let’s demonstrate this with the previously defined dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(HistData)
<span class="kw">data</span>(<span class="st">&quot;GaltonFamilies&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1983</span>)
galton_heights &lt;-<span class="st"> </span>GaltonFamilies <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(gender <span class="op">==</span><span class="st"> &quot;male&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(father, childHeight) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">son =</span> childHeight)</code></pre></div>
<p>Let’s write a function that computes the RSS for any pair of values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rss &lt;-<span class="st"> </span><span class="cf">function</span>(beta0, beta1, data){
  resid &lt;-<span class="st"> </span>galton_heights<span class="op">$</span>son <span class="op">-</span><span class="st"> </span>(beta0<span class="op">+</span>beta1<span class="op">*</span>galton_heights<span class="op">$</span>father)
  <span class="kw">return</span>(<span class="kw">sum</span>(resid<span class="op">^</span><span class="dv">2</span>))
}</code></pre></div>
<p>So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of <span class="math inline">\(\beta_1\)</span> when we keep the <span class="math inline">\(\beta_0\)</span> fixed at 25.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">len=</span><span class="kw">nrow</span>(galton_heights))
results &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">beta1 =</span> beta1,
                      <span class="dt">rss =</span> <span class="kw">sapply</span>(beta1, rss, <span class="dt">beta0 =</span> <span class="dv">25</span>))
results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(beta1, rss)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(beta1, rss))</code></pre></div>
<p><img src="book_files/figure-html/rss-versus-estimate-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see a clear minimum for <span class="math inline">\(\beta_1\)</span> at around 0.65. However, this minimum for <span class="math inline">\(\beta_1\)</span> is for when <span class="math inline">\(\beta_0 = 25\)</span>, a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.</p>
<p>Trial and error is not going to work in this case. Instead, we will use calculus: take the partial derivatives, set them to 0 and solve for <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.</p>
</div>
<div id="the-lm-function" class="section level3">
<h3><span class="header-section-number">19.3.3</span> The <code>lm</code> function</h3>
<p>In R, we can obtain the least squares estimates using the the <code>lm</code> function. To fit the model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> the son’s height and <span class="math inline">\(x_i\)</span> the father’s height, we write:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> galton_heights)</code></pre></div>
<p>and obtain the least squares estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = son ~ father, data = galton_heights)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)       father  </span>
<span class="co">#&gt;      38.765        0.441</span></code></pre></div>
<p>The most common way we use <code>lm</code> is by using the character <code>~</code> to let <code>lm</code> know which is the variable we are predicting (left of <code>~</code>) and which we are using to predict (right of <code>~</code>). The intercept is added automatically to the model that will be fit.</p>
<p>The object <code>fit</code> includes more information about the fit. We can use the function <code>summary</code> to extract more of this information:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = son ~ father, data = galton_heights)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;    Min     1Q Median     3Q    Max </span>
<span class="co">#&gt; -9.423 -1.702  0.033  1.567  9.357 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)  38.7646     5.4109    7.16  2.0e-11 ***</span>
<span class="co">#&gt; father        0.4411     0.0783    5.64  6.7e-08 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 2.66 on 177 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.152,  Adjusted R-squared:  0.147 </span>
<span class="co">#&gt; F-statistic: 31.8 on 1 and 177 DF,  p-value: 6.72e-08</span></code></pre></div>
<p>To understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables</p>
</div>
<div id="lse-are-random-variables" class="section level3">
<h3><span class="header-section-number">19.3.4</span> LSE are random variables</h3>
<p>The LSE is derived from the data <span class="math inline">\(y_1,\dots,y_N\)</span>, which are a realization of random variables <span class="math inline">\(Y_1, \dots, Y_N\)</span>. This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size <span class="math inline">\(N=50\)</span> and compute the regression slope coefficient for each one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
N &lt;-<span class="st"> </span><span class="dv">50</span>
lse &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, {
  <span class="kw">sample_n</span>(galton_heights, N, <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>.<span class="op">$</span>coef 
})

lse &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">beta_0 =</span> lse[<span class="dv">1</span>,], <span class="dt">beta_1 =</span> lse[<span class="dv">2</span>,]) </code></pre></div>
<p>We can see the variability of the estimates by plotting their distributions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra)
p1 &lt;-<span class="st"> </span>lse <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(beta_<span class="dv">0</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) 
p2 &lt;-<span class="st"> </span>lse <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(beta_<span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="fl">0.1</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) 
<span class="kw">grid.arrange</span>(p1, p2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="book_files/figure-html/lse-distributions-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The reason these look normal is because the central limit theorem applies here as well: for large enough <span class="math inline">\(N\)</span>, the least squares estimates will be approximately normal with expected value <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the <code>lm</code> function. Here it is for one of our simulated data sets:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> <span class="kw">sample_n</span>(galton_heights, N, <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>summary <span class="op">%&gt;%</span>
<span class="st">  </span>.<span class="op">$</span>coef
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)</span>
<span class="co">#&gt; (Intercept)   34.473      8.602    4.01 0.000213</span>
<span class="co">#&gt; father         0.499      0.124    4.02 0.000203</span></code></pre></div>
<p>You can see that the standard errors estimates reported by the <code>summary</code> are close to the standard errors from the simulation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lse <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">se_0 =</span> <span class="kw">sd</span>(beta_<span class="dv">0</span>), <span class="dt">se_1 =</span> <span class="kw">sd</span>(beta_<span class="dv">1</span>))
<span class="co">#&gt;   se_0  se_1</span>
<span class="co">#&gt; 1 9.68 0.141</span></code></pre></div>
<p>The <code>summary</code> function also reports t-statistics (<code>t value</code>) and p-values (<code>Pr(&gt;|t|)</code>). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the <span class="math inline">\(\varepsilon\)</span>s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, <span class="math inline">\(\hat{\beta}_0 / \hat{\mbox{SE}}(\hat{\beta}_0 )\)</span> and <span class="math inline">\(\hat{\beta}_1 / \hat{\mbox{SE}}(\hat{\beta}_1 )\)</span>, follow a t-distribution with <span class="math inline">\(N-p\)</span> degrees of freedom, with <span class="math inline">\(p\)</span> the number of parameters in our model. In the case of height <span class="math inline">\(p=2\)</span>, the two p-values are testing the null hypothesis that <span class="math inline">\(\beta_0 = 0\)</span> and <span class="math inline">\(\beta_1=0\)</span> respectively.</p>
<p>Remember that, as we described in Section <a href="models.html#t-dist">17.10</a> for large enough <span class="math inline">\(N\)</span>, the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about <strong>broom</strong>, an add-on package that makes this easy.</p>
<p>Although we do not show examples in this book, hypothesis testing with regression models is commonly used in Epidemiology and Economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y and Z”. However, several assumptions have to hold for these statements to be true.</p>
</div>
<div id="predicted-values-are-random-variables" class="section level3">
<h3><span class="header-section-number">19.3.5</span> Predicted values are random variables</h3>
<p>Once we fit our model, we can obtain prediction of <span class="math inline">\(Y\)</span> by plugging in the estimates into the regression model. For example, if the father’s height is <span class="math inline">\(x\)</span>, then our prediction <span class="math inline">\(\hat{Y}\)</span> for the son’s height we will:</p>
<p><span class="math display">\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x\]</span></p>
<p>When we plot <span class="math inline">\(\hat{Y}\)</span> versus <span class="math inline">\(x\)</span>, we see the regression line.</p>
<p>Keep in mind that the prediction <span class="math inline">\(\hat{Y}\)</span> is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the <strong>ggplot2</strong> layer <code>geom_smooth(method = &quot;lm&quot;)</code> that we previously used plots <span class="math inline">\(\hat{Y}\)</span> and surrounds it by confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(son, father)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/father-son-regression-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The R function <code>predict</code> takes an <code>lm</code> object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> .) 

Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>)

<span class="kw">names</span>(Y_hat)
<span class="co">#&gt; [1] &quot;fit&quot;            &quot;se.fit&quot;         &quot;df&quot;             &quot;residual.scale&quot;</span></code></pre></div>
</div>
</div>
<div id="exercises-32" class="section level2">
<h2><span class="header-section-number">19.4</span> Exercises</h2>
<p>We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.</p>
<ol style="list-style-type: decimal">
<li><p>Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lahman)
dat &lt;-<span class="st"> </span>Batting <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">==</span><span class="st"> </span><span class="dv">2002</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pa =</span> AB <span class="op">+</span><span class="st"> </span>BB, <span class="dt">singles =</span> (H <span class="op">-</span><span class="st"> </span>X2B <span class="op">-</span><span class="st"> </span>X3B <span class="op">-</span><span class="st"> </span>HR)<span class="op">/</span>pa, <span class="dt">bb =</span> BB<span class="op">/</span>pa) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(pa <span class="op">&gt;=</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, singles, bb)</code></pre></div>
<p>Now compute a similar table but with rates computed over 1999-2001.</p></li>
<li><p>In Section <a href="joining-tables.html#joins">23.1</a> we learn about th the <code>inner_join</code>, which you can use to have the 2001 data and averages in the same table:</p>
<p><code>r      dat &lt;- inner_join(dat, avg, by = &quot;playerID&quot;)</code></p>
<p>Compute the correlation between 2002 and the previous seasons for singles and BB.</p></li>
<li><p>Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.</p></li>
<li><p>Now fit a linear model for each metric and use the <code>confint</code> function to compare the estimates.</p></li>
</ol>
</div>
<div id="linear-regression-in-the-tidyverse" class="section level2">
<h2><span class="header-section-number">19.5</span> Linear regression in the tidyverse</h2>
<p>To see how we use the <code>lm</code> function in a more complex analysis, let’s go back to the baseball example. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HR =</span> <span class="kw">round</span>(HR<span class="op">/</span>G, <span class="dv">1</span>), 
         <span class="dt">BB =</span> BB<span class="op">/</span>G,
         <span class="dt">R =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(HR, BB, R) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(HR <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.4</span> <span class="op">&amp;</span><span class="st"> </span>HR<span class="op">&lt;=</span><span class="fl">1.2</span>) </code></pre></div>
<p>Since we didn’t know the <code>lm</code> function, to compute the regression line in each strata, we used the formula directly like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">slope =</span> <span class="kw">cor</span>(BB,R)<span class="op">*</span><span class="kw">sd</span>(R)<span class="op">/</span><span class="kw">sd</span>(BB))
<span class="co">#&gt; # A tibble: 9 x 2</span>
<span class="co">#&gt;      HR slope</span>
<span class="co">#&gt;   &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1   0.4 0.734</span>
<span class="co">#&gt; 2   0.5 0.566</span>
<span class="co">#&gt; 3   0.6 0.412</span>
<span class="co">#&gt; 4   0.7 0.285</span>
<span class="co">#&gt; 5   0.8 0.365</span>
<span class="co">#&gt; 6   0.9 0.261</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the <code>lm</code> function provides enough information to construct them.</p>
<p>First, note that if we try to use the <code>lm</code> function to get the estimated slope like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> .) <span class="op">%&gt;%</span>
<span class="st">  </span>.<span class="op">$</span>coef
<span class="co">#&gt; (Intercept)          BB </span>
<span class="co">#&gt;       2.199       0.638</span></code></pre></div>
<p>we don’t get the result we want. The <code>lm</code> function ignores the <code>group_by</code>. This is expected because <code>lm</code> is not part of the <strong>tidyverse</strong> and does not know how to handle the outcome of a grouped tibble.</p>
<p>The <strong>tidyverse</strong> functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe <code>%&gt;%</code>, <strong>tidyverse</strong> functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The <code>lm</code> function is an example. The <code>do</code> functions serves as a bridge between R functions, such as <code>lm</code>, and the <strong>tidyverse</strong>. The <code>do</code> function understands grouped tibbles and always returns a data frame.</p>
<p>So, let’s try to use the <code>do</code> function to fit a regression line to each HR strata:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="dt">fit =</span> <span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> .))
<span class="co">#&gt; Source: local data frame [9 x 2]</span>
<span class="co">#&gt; Groups: &lt;by row&gt;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; # A tibble: 9 x 2</span>
<span class="co">#&gt;      HR fit     </span>
<span class="co">#&gt; * &lt;dbl&gt; &lt;list&gt;  </span>
<span class="co">#&gt; 1   0.4 &lt;S3: lm&gt;</span>
<span class="co">#&gt; 2   0.5 &lt;S3: lm&gt;</span>
<span class="co">#&gt; 3   0.6 &lt;S3: lm&gt;</span>
<span class="co">#&gt; 4   0.7 &lt;S3: lm&gt;</span>
<span class="co">#&gt; 5   0.8 &lt;S3: lm&gt;</span>
<span class="co">#&gt; 6   0.9 &lt;S3: lm&gt;</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>Notice that we did in fact fit a regression line to each strata. The <code>do</code> function will create a data frame with the first column being the strata value and a column named <code>fit</code> (we chose the name, but it can be anything). The column will contain the result of the <code>lm</code> call. Therefore, the returned tibble has a column with <code>lm</code> objects, which is not very useful.</p>
<p>Also, if we do not name a column (note above we named it <code>fit</code>), then <code>do</code> will return the actual output of <code>lm</code>, not a data frame, and this will result in an error since <code>do</code> is expecting a data frame as output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> .))</code></pre></div>
<p><code>Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm</code></p>
<p>For a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_slope &lt;-<span class="st"> </span><span class="cf">function</span>(data){
  fit &lt;-<span class="st"> </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> data)
  <span class="kw">data.frame</span>(<span class="dt">slope =</span> fit<span class="op">$</span>coefficients[<span class="dv">2</span>], 
             <span class="dt">se =</span> <span class="kw">summary</span>(fit)<span class="op">$</span>coefficient[<span class="dv">2</span>,<span class="dv">2</span>])
}</code></pre></div>
<p>And then use <code>do</code> <strong>without</strong> naming the output, since we are already getting a data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">get_slope</span>(.))
<span class="co">#&gt; # A tibble: 9 x 3</span>
<span class="co">#&gt; # Groups:   HR [9]</span>
<span class="co">#&gt;      HR slope     se</span>
<span class="co">#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;</span>
<span class="co">#&gt; 1   0.4 0.734 0.208 </span>
<span class="co">#&gt; 2   0.5 0.566 0.110 </span>
<span class="co">#&gt; 3   0.6 0.412 0.0974</span>
<span class="co">#&gt; 4   0.7 0.285 0.0705</span>
<span class="co">#&gt; 5   0.8 0.365 0.0652</span>
<span class="co">#&gt; 6   0.9 0.261 0.0754</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>If we name the output, then we get something we do not want, a column containing a data frames:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="dt">slope =</span> <span class="kw">get_slope</span>(.))
<span class="co">#&gt; Source: local data frame [9 x 2]</span>
<span class="co">#&gt; Groups: &lt;by row&gt;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; # A tibble: 9 x 2</span>
<span class="co">#&gt;      HR slope               </span>
<span class="co">#&gt; * &lt;dbl&gt; &lt;list&gt;              </span>
<span class="co">#&gt; 1   0.4 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; 2   0.5 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; 3   0.6 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; 4   0.7 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; 5   0.8 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; 6   0.9 &lt;data.frame [1 × 2]&gt;</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>This is not very useful.</p>
<p>Now, let’s cover one last feature of <code>do</code>. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_lse &lt;-<span class="st"> </span><span class="cf">function</span>(data){
  fit &lt;-<span class="st"> </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> data)
  <span class="kw">data.frame</span>(<span class="dt">term =</span> <span class="kw">names</span>(fit<span class="op">$</span>coefficients),
    <span class="dt">slope =</span> fit<span class="op">$</span>coefficients, 
    <span class="dt">se =</span> <span class="kw">summary</span>(fit)<span class="op">$</span>coefficient[,<span class="dv">2</span>])
}

dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">get_lse</span>(.))
<span class="co">#&gt; # A tibble: 18 x 4</span>
<span class="co">#&gt; # Groups:   HR [9]</span>
<span class="co">#&gt;      HR term        slope     se</span>
<span class="co">#&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;</span>
<span class="co">#&gt; 1   0.4 (Intercept) 1.36  0.631 </span>
<span class="co">#&gt; 2   0.4 BB          0.734 0.208 </span>
<span class="co">#&gt; 3   0.5 (Intercept) 2.01  0.344 </span>
<span class="co">#&gt; 4   0.5 BB          0.566 0.110 </span>
<span class="co">#&gt; 5   0.6 (Intercept) 2.53  0.305 </span>
<span class="co">#&gt; 6   0.6 BB          0.412 0.0974</span>
<span class="co">#&gt; # ... with 12 more rows</span></code></pre></div>
<p>If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the <strong>broom</strong> package which was designed to facilitate the use of model fitting functions, such as <code>lm</code>, with the <strong>tidyverse</strong>.</p>
<div id="the-broom-package" class="section level3">
<h3><span class="header-section-number">19.5.1</span> The broom package</h3>
<p>Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The <strong>broom</strong> package will make this quite easy.</p>
<p>The <strong>broom</strong> package has three main functions, all of which extract information from the object returned by <code>lm</code> and return it in a <strong>tidyverse</strong> friendly data frame. These functions are <code>tidy</code>, <code>glance</code> and <code>augment</code>. The <code>tidy</code> function returns estimates and related information as a data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> dat)
<span class="kw">tidy</span>(fit)
<span class="co">#&gt; # A tibble: 2 x 5</span>
<span class="co">#&gt;   term        estimate std.error statistic  p.value</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)    2.20     0.113       19.4 1.06e-70</span>
<span class="co">#&gt; 2 BB             0.638    0.0344      18.5 1.37e-65</span></code></pre></div>
<p>We can add other important summaries, such as confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(fit, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)
<span class="co">#&gt; # A tibble: 2 x 7</span>
<span class="co">#&gt;   term        estimate std.error statistic  p.value conf.low conf.high</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)    2.20     0.113       19.4 1.06e-70    1.98      2.42 </span>
<span class="co">#&gt; 2 BB             0.638    0.0344      18.5 1.37e-65    0.570     0.705</span></code></pre></div>
<p>Because the outcome is a data frame, we can immediately use it with <code>do</code> to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">tidy</span>(<span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> .), <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;BB&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(HR, estimate, conf.low, conf.high)
<span class="co">#&gt; # A tibble: 9 x 4</span>
<span class="co">#&gt; # Groups:   HR [9]</span>
<span class="co">#&gt;      HR estimate conf.low conf.high</span>
<span class="co">#&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1   0.4    0.734    0.308     1.16 </span>
<span class="co">#&gt; 2   0.5    0.566    0.346     0.786</span>
<span class="co">#&gt; 3   0.6    0.412    0.219     0.605</span>
<span class="co">#&gt; 4   0.7    0.285    0.146     0.425</span>
<span class="co">#&gt; 5   0.8    0.365    0.236     0.494</span>
<span class="co">#&gt; 6   0.9    0.261    0.112     0.410</span>
<span class="co">#&gt; # ... with 3 more rows</span></code></pre></div>
<p>A table like this can then be easily visualized with <strong>ggplot2</strong>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(HR) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">tidy</span>(<span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB, <span class="dt">data =</span> .), <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;BB&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(HR, estimate, conf.low, conf.high) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(HR, <span class="dt">y =</span> estimate, <span class="dt">ymin =</span> conf.low, <span class="dt">ymax =</span> conf.high)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="book_files/figure-html/do-tidy-example-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now we return to discussing our original task of determining if slopes changed. The plot we just made, using <code>do</code> and <code>tidy</code>, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.</p>
<p>The other functions provided by <strong>broom</strong>, <code>glance</code> and <code>augment</code>, relate to model specific and observation specific outcomes respectively. Here, we can see the model fit summaries <code>glance</code> returns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(fit)
<span class="co">#&gt; # A tibble: 1 x 11</span>
<span class="co">#&gt;   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC</span>
<span class="co">#&gt; *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1     0.266         0.265 0.454      343. 1.37e-65     2  -596. 1199. 1213.</span>
<span class="co">#&gt; # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</span></code></pre></div>
<p>You can learn more about these summaries in any regression text book.</p>
<p>We will see an example of <code>augment</code> in the next section.</p>
</div>
</div>
<div id="exercises-33" class="section level2">
<h2><span class="header-section-number">19.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>In a previous section, we computed the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons, and noticed that the highest correlation is between fathers and sons and the lowest is between mothers and sons. We can compute these correlations using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;GaltonFamilies&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
galton_heights &lt;-<span class="st"> </span>GaltonFamilies <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family, gender) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

cors &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(parent, parentHeight, father<span class="op">:</span>mother) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">child =</span> <span class="kw">ifelse</span>(gender <span class="op">==</span><span class="st"> &quot;female&quot;</span>, <span class="st">&quot;daughter&quot;</span>, <span class="st">&quot;son&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unite</span>(pair, <span class="kw">c</span>(<span class="st">&quot;parent&quot;</span>, <span class="st">&quot;child&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(pair) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">cor =</span> <span class="kw">cor</span>(parentHeight, childHeight))</code></pre></div>
<p>Are these differences statistically significant? To answer this, we will compute the slopes of the regression line along with their standard errors. Start by using <code>lm</code> and the <strong>broom</strong> package to compute the slopes LSE and the standard errors.</p></li>
<li><p>Repeat the exercise above, but compute a confidence interval as well.</p></li>
<li><p>Plot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex.</p></li>
<li><p>Because we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: use similar code to what we used with simulations.</p></li>
<li><p>Fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the <code>tidy</code> function in the <strong>broom</strong> package to obtain the results in a data frame.</p></li>
<li><p>Now let’s repeat the above for each year since 1961 and make a plot. Use <code>do</code> and the <strong>broom</strong> package to fit this model for every year since 1961.</p></li>
<li><p>Use the results of the previous exercise to plot the estimated effects of BB on runs.</p></li>
<li><p><strong>Advanced</strong>. Write a function that takes R, HR and BB as arguments and fits two linear models: <code>R ~ BB</code> and <code>R~BB+HR</code>. Then use the <code>do</code> function to obtain the <code>BB</code> for both models for each year since 1961. Then plot these against each other as a function of time.</p></li>
</ol>
</div>
<div id="case-study-moneyball-continued" class="section level2">
<h2><span class="header-section-number">19.7</span> Case study: Moneyball (continued)</h2>
<p>In trying to answer how well BB predict runs, data exploration led us to a model:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>Here, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> runs per game for team <span class="math inline">\(i\)</span>, <span class="math inline">\(x_{i,1}\)</span> walks per game, and <span class="math inline">\(x_{i,2}\)</span>. To use <code>lm</code> here, we need to let the function know we have two predictor variables. So we use the <code>+</code> symbol as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BB =</span> BB<span class="op">/</span>G, <span class="dt">HR =</span> HR<span class="op">/</span>G,  <span class="dt">R =</span> R<span class="op">/</span>G) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB <span class="op">+</span><span class="st"> </span>HR, <span class="dt">data =</span> .)</code></pre></div>
<p>We can use <code>tidy</code> to see a nice summary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(fit, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>) 
<span class="co">#&gt; # A tibble: 3 x 7</span>
<span class="co">#&gt;   term        estimate std.error statistic   p.value conf.low conf.high</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)    1.74     0.0823      21.2 7.30e- 83    1.58      1.91 </span>
<span class="co">#&gt; 2 BB             0.387    0.0270      14.3 1.20e- 42    0.334     0.440</span>
<span class="co">#&gt; 3 HR             1.56     0.0490      31.9 1.75e-155    1.47      1.66</span></code></pre></div>
<p>When we fit the model with only one variable, the estimated slopes were 0.735 and 1.845 for BB and HR respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.</p>
<p>Now we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?</p>
<p>We now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3}+ \beta_4 x_{i,4} + \beta_5 x_{i,5} + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\)</span> representing BB, singles, doubles, triples, and HR respectively.</p>
<p>Using <code>lm</code>, we can quickly find the LSE for the parameters using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1961</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BB =</span> BB <span class="op">/</span><span class="st"> </span>G, 
         <span class="dt">singles =</span> (H <span class="op">-</span><span class="st"> </span>X2B <span class="op">-</span><span class="st"> </span>X3B <span class="op">-</span><span class="st"> </span>HR) <span class="op">/</span><span class="st"> </span>G, 
         <span class="dt">doubles =</span> X2B <span class="op">/</span><span class="st"> </span>G, 
         <span class="dt">triples =</span> X3B <span class="op">/</span><span class="st"> </span>G, 
         <span class="dt">HR =</span> HR <span class="op">/</span><span class="st"> </span>G,
         <span class="dt">R =</span> R <span class="op">/</span><span class="st"> </span>G) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">lm</span>(R <span class="op">~</span><span class="st"> </span>BB <span class="op">+</span><span class="st"> </span>singles <span class="op">+</span><span class="st"> </span>doubles <span class="op">+</span><span class="st"> </span>triples <span class="op">+</span><span class="st"> </span>HR, <span class="dt">data =</span> .)</code></pre></div>
<p>We can see the coefficients using <code>tidy</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs &lt;-<span class="st"> </span><span class="kw">tidy</span>(fit, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)

coefs
<span class="co">#&gt; # A tibble: 6 x 7</span>
<span class="co">#&gt;   term        estimate std.error statistic   p.value conf.low conf.high</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)   -2.77     0.0862     -32.1 5.32e-157   -2.94     -2.60 </span>
<span class="co">#&gt; 2 BB             0.371    0.0117      31.6 2.08e-153    0.348     0.394</span>
<span class="co">#&gt; 3 singles        0.519    0.0127      40.8 9.81e-217    0.494     0.544</span>
<span class="co">#&gt; 4 doubles        0.771    0.0226      34.1 8.93e-171    0.727     0.816</span>
<span class="co">#&gt; 5 triples        1.24     0.0768      16.1 2.24e- 52    1.09      1.39 </span>
<span class="co">#&gt; 6 HR             1.44     0.0244      59.3 0.           1.40      1.49</span></code></pre></div>
<p>To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function <code>predict</code>, then make a plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Teams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">2002</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BB =</span> BB<span class="op">/</span>G, 
         <span class="dt">singles =</span> (H<span class="op">-</span>X2B<span class="op">-</span>X3B<span class="op">-</span>HR)<span class="op">/</span>G, 
         <span class="dt">doubles =</span> X2B<span class="op">/</span>G, 
         <span class="dt">triples =</span>X3B<span class="op">/</span>G, 
         <span class="dt">HR=</span>HR<span class="op">/</span>G,
         <span class="dt">R=</span>R<span class="op">/</span>G)  <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">R_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> .)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(R_hat, R, <span class="dt">label =</span> teamID)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">nudge_x=</span><span class="fl">0.1</span>, <span class="dt">cex =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre></div>
<p><img src="book_files/figure-html/model-predicts-runs-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.</p>
<p>So instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.769 + 0.371 <span class="math inline">\(\times\)</span> BB + 0.519 <span class="math inline">\(\times\)</span> singles + 0.771 <span class="math inline">\(\times\)</span> doubles + 1.24 <span class="math inline">\(\times\)</span> triples + 1.443 <span class="math inline">\(\times\)</span> HR.</p>
<p>To define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets less opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.</p>
<p>To make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pa_per_game &lt;-<span class="st"> </span>Batting <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">==</span><span class="st"> </span><span class="dv">2002</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(teamID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">pa_per_game =</span> <span class="kw">sum</span>(AB<span class="op">+</span>BB)<span class="op">/</span><span class="kw">max</span>(G)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(pa_per_game) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>mean</code></pre></div>
<p>We compute the per-plate-appearance rates for players available in 2002 on data from 1999-2001. To avoid small sample artifacts, we filter players with few plate appearances. Here is the entire calculation in one line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">players &lt;-<span class="st"> </span>Batting <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">1999</span><span class="op">:</span><span class="dv">2001</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">PA =</span> BB <span class="op">+</span><span class="st"> </span>AB) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">G =</span> <span class="kw">sum</span>(PA)<span class="op">/</span>pa_per_game,
    <span class="dt">BB =</span> <span class="kw">sum</span>(BB)<span class="op">/</span>G,
    <span class="dt">singles =</span> <span class="kw">sum</span>(H<span class="op">-</span>X2B<span class="op">-</span>X3B<span class="op">-</span>HR)<span class="op">/</span>G,
    <span class="dt">doubles =</span> <span class="kw">sum</span>(X2B)<span class="op">/</span>G, 
    <span class="dt">triples =</span> <span class="kw">sum</span>(X3B)<span class="op">/</span>G, 
    <span class="dt">HR =</span> <span class="kw">sum</span>(HR)<span class="op">/</span>G,
    <span class="dt">AVG =</span> <span class="kw">sum</span>(H)<span class="op">/</span><span class="kw">sum</span>(AB),
    <span class="dt">PA =</span> <span class="kw">sum</span>(PA)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(PA <span class="op">&gt;=</span><span class="st"> </span><span class="dv">300</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>G) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">R_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> .))</code></pre></div>
<p>The player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(R_hat, <span class="dt">data =</span> players, <span class="dt">geom =</span> <span class="st">&quot;histogram&quot;</span>, <span class="dt">binwidth =</span> <span class="fl">0.5</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</code></pre></div>
<p><img src="book_files/figure-html/r-hat-hist-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div id="adding-salary-and-position-information" class="section level3">
<h3><span class="header-section-number">19.7.1</span> Adding salary and position information</h3>
<p>To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the <code>players</code> data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function we learned in Section <a href="joining-tables.html#joins">23.1</a>.</p>
<p>Start by adding the 2002 salary of each player:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">players &lt;-<span class="st"> </span>Salaries <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">==</span><span class="st"> </span><span class="dv">2002</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, salary) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">right_join</span>(players, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>)</code></pre></div>
<p>Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The <strong>Lahman</strong> package table <code>Appearances</code> tells how many games each player played in each position, so we can pick the position that was most played using <code>which.max</code> on each row. We use <code>apply</code> to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams. Here, we pick the one position the player most played using the <code>top_n</code> function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the <code>OF</code> position which stands for outfielder, a generalization of three positions left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">position_names &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;G_p&quot;</span>,<span class="st">&quot;G_c&quot;</span>,<span class="st">&quot;G_1b&quot;</span>,<span class="st">&quot;G_2b&quot;</span>,<span class="st">&quot;G_3b&quot;</span>,<span class="st">&quot;G_ss&quot;</span>,<span class="st">&quot;G_lf&quot;</span>,<span class="st">&quot;G_cf&quot;</span>,<span class="st">&quot;G_rf&quot;</span>)

tmp_tab &lt;-<span class="st"> </span>Appearances <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">==</span><span class="st"> </span><span class="dv">2002</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize_at</span>(position_names, sum) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()
  
pos &lt;-<span class="st"> </span>tmp_tab <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(position_names) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">apply</span>(., <span class="dv">1</span>, which.max) 

players &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">playerID =</span> tmp_tab<span class="op">$</span>playerID, <span class="dt">POS =</span> position_names[pos]) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">POS =</span> <span class="kw">str_to_upper</span>(<span class="kw">str_remove</span>(POS, <span class="st">&quot;G_&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(POS <span class="op">!=</span><span class="st"> &quot;P&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">right_join</span>(players, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(POS)  <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(salary))</code></pre></div>
<p>Finally, we add their first and last name:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">players &lt;-<span class="st"> </span>Master <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, nameFirst, nameLast, debut) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">debut =</span> <span class="kw">as.Date</span>(debut)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">right_join</span>(players, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>)</code></pre></div>
<p>If you are a baseball fan, you will recognize the top 10 players:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">players <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(nameFirst, nameLast, POS, salary, R_hat) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(R_hat)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>) 
<span class="co">#&gt; Selecting by R_hat</span>
<span class="co">#&gt;    nameFirst    nameLast POS   salary R_hat</span>
<span class="co">#&gt; 1      Barry       Bonds  LF 15000000  9.05</span>
<span class="co">#&gt; 2       Todd      Helton  1B  5000000  8.23</span>
<span class="co">#&gt; 3      Manny     Ramirez  LF 15462727  8.20</span>
<span class="co">#&gt; 4      Sammy        Sosa  RF 15000000  8.19</span>
<span class="co">#&gt; 5      Larry      Walker  RF 12666667  8.15</span>
<span class="co">#&gt; 6      Jason      Giambi  1B 10428571  7.99</span>
<span class="co">#&gt; 7    Chipper       Jones  LF 11333333  7.64</span>
<span class="co">#&gt; 8      Brian       Giles  LF  8063003  7.57</span>
<span class="co">#&gt; 9     Albert      Pujols  LF   600000  7.54</span>
<span class="co">#&gt; 10     Nomar Garciaparra  SS  9000000  7.51</span></code></pre></div>
</div>
<div id="picking-9-players" class="section level3">
<h3><span class="header-section-number">19.7.2</span> Picking 9 players</h3>
<p>Notice the very high salaries for most players. In fact, we see that, on average, players with a higher metric have higher salaries:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">players <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(salary, R_hat, <span class="dt">color =</span> POS)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_log10</span>()</code></pre></div>
<p><img src="book_files/figure-html/predicted-runs-vs-salary-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We do see some low cost players with very high metrics. These will be great for our team. Unfortunately, many of these are likely young players that have not yet been able to negotiate a salary and are unavailable. For example, the lowest earner in our top 10 list, Albert Pujols, was a rookie in 2001 and could not negotiate with other teams. Here we remake plot without players that debuted before 1998. We use the <strong>lubridate</strong> function <code>year</code>, introduced in Section <a href="parsing-dates-and-times.html#lubridate">26.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lubridate)
players <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">year</span>(debut) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1998</span>) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(salary, R_hat, <span class="dt">color =</span> POS)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_log10</span>()</code></pre></div>
<p><img src="book_files/figure-html/predicted-runs-vs-salary-no-rookies-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can search for good deals by looking at players that produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists called linear programming. This is not something we teach, but we include the code anyway:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reshape2)
<span class="kw">library</span>(lpSolve)

players &lt;-<span class="st"> </span>players <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(lubridate<span class="op">::</span><span class="kw">year</span>(debut) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1998</span>) 
constraint_matrix &lt;-<span class="st"> </span><span class="kw">acast</span>(players, POS <span class="op">~</span><span class="st"> </span>playerID, <span class="dt">fun.aggregate =</span> length)
npos &lt;-<span class="st"> </span><span class="kw">nrow</span>(constraint_matrix)
constraint_matrix &lt;-<span class="st"> </span><span class="kw">rbind</span>(constraint_matrix, <span class="dt">salary =</span> players<span class="op">$</span>salary)
constraint_dir &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;==&quot;</span>, npos), <span class="st">&quot;&lt;=&quot;</span>)
constraint_limit &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, npos), <span class="dv">50</span><span class="op">*</span><span class="dv">10</span><span class="op">^</span><span class="dv">6</span>)
lp_solution &lt;-<span class="st"> </span><span class="kw">lp</span>(<span class="st">&quot;max&quot;</span>, players<span class="op">$</span>R_hat,
                  constraint_matrix, constraint_dir, constraint_limit,
                  <span class="dt">all.int =</span> <span class="ot">TRUE</span>) </code></pre></div>
<p>This algorithm chooses these 9 players:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">our_team &lt;-<span class="st"> </span>players <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(lp_solution<span class="op">$</span>solution <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(R_hat))
our_team <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(nameFirst, nameLast, POS, salary, R_hat)
<span class="co">#&gt;   nameFirst    nameLast POS   salary R_hat</span>
<span class="co">#&gt; 1      Todd      Helton  1B  5000000  8.23</span>
<span class="co">#&gt; 2     Nomar Garciaparra  SS  9000000  7.51</span>
<span class="co">#&gt; 3  Vladimir    Guerrero  RF  8000000  7.45</span>
<span class="co">#&gt; 4      Luis    Gonzalez  LF  4333333  7.40</span>
<span class="co">#&gt; 5      Mike      Piazza   C 10571429  7.16</span>
<span class="co">#&gt; 6       Jim     Edmonds  CF  7333333  6.90</span>
<span class="co">#&gt; 7      Phil       Nevin  3B  2600000  6.75</span>
<span class="co">#&gt; 8     Terry    Shumpert  2B   775000  6.04</span></code></pre></div>
<p>We see that most of these players have above average BB and HR rates, while the same is not true for singles:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_scale &lt;-<span class="st"> </span><span class="cf">function</span>(x) (x <span class="op">-</span><span class="st"> </span><span class="kw">median</span>(x))<span class="op">/</span><span class="kw">mad</span>(x)
players <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">BB =</span> <span class="kw">my_scale</span>(BB), 
                   <span class="dt">singles =</span> <span class="kw">my_scale</span>(singles),
                   <span class="dt">doubles =</span> <span class="kw">my_scale</span>(doubles),
                   <span class="dt">triples =</span> <span class="kw">my_scale</span>(triples),
                   <span class="dt">HR =</span> <span class="kw">my_scale</span>(HR),
                   <span class="dt">AVG =</span> <span class="kw">my_scale</span>(AVG),
                   <span class="dt">R_hat =</span> <span class="kw">my_scale</span>(R_hat)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(playerID <span class="op">%in%</span><span class="st"> </span>our_team<span class="op">$</span>playerID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(nameFirst, nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(R_hat))
<span class="co">#&gt;   nameFirst    nameLast      BB singles doubles triples     HR   AVG R_hat</span>
<span class="co">#&gt; 1      Todd      Helton  1.2171 -0.3502   2.927  0.0195  1.898 2.562 3.081</span>
<span class="co">#&gt; 2     Nomar Garciaparra  0.0654  1.6452   2.987  0.3530  0.650 3.201 2.309</span>
<span class="co">#&gt; 3  Vladimir    Guerrero -0.0997  0.1243   0.785  1.8126  1.833 1.783 2.255</span>
<span class="co">#&gt; 4      Luis    Gonzalez  0.7330  0.0272   1.352  0.5682  1.438 1.831 2.193</span>
<span class="co">#&gt; 5      Mike      Piazza  0.3468 -0.0269  -0.235 -1.3687  2.172 1.254 1.941</span>
<span class="co">#&gt; 6       Jim     Edmonds  1.8200 -1.1005   0.644 -0.7273  1.340 0.580 1.661</span>
<span class="co">#&gt; 7      Phil       Nevin  0.5223 -0.6132   0.729 -1.1802  1.646 0.729 1.497</span>
<span class="co">#&gt; 8     Terry    Shumpert -0.1170  0.1478   1.268  4.1720 -0.156 0.861 0.749</span></code></pre></div>
</div>
</div>
<div id="the-regression-fallacy" class="section level2">
<h2><span class="header-section-number">19.8</span> The regression fallacy</h2>
<p>Wikipedia defines the <em>sophomore slump</em> as:</p>
<blockquote>
<p>A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).</p>
</blockquote>
<p>In Major League Baseball, the rookie of the year (ROY) award is given to the first year player that is judged to have performed the best. The <em>sophmore slump</em> phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this recent Fox Sports <a href="http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715">article</a> asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.</p>
<p>Does the data confirm the existence of a sophomore slump? Let’s take a look.</p>
<p>Examining the data for batting average, we see that this observation holds true. The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lahman)
playerInfo &lt;-<span class="st"> </span>Fielding <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(G)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(Master, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(playerID, nameFirst, nameLast, POS)</code></pre></div>
<p>Now, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ROY &lt;-<span class="st"> </span>AwardsPlayers <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(awardID <span class="op">==</span><span class="st"> &quot;Rookie of the Year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(playerInfo, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">rookie_year =</span> yearID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">right_join</span>(Batting, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">AVG =</span> H<span class="op">/</span>AB) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(POS <span class="op">!=</span><span class="st"> &quot;P&quot;</span>)</code></pre></div>
<p>We also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ROY &lt;-<span class="st"> </span>ROY <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">==</span><span class="st"> </span>rookie_year <span class="op">|</span><span class="st"> </span>yearID <span class="op">==</span><span class="st"> </span>rookie_year<span class="op">+</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rookie =</span> <span class="kw">ifelse</span>(yearID <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(yearID), <span class="st">&quot;rookie&quot;</span>, <span class="st">&quot;sophomore&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, rookie_year, rookie, nameFirst, nameLast, AVG) </code></pre></div>
<p>Finally, we will use the <code>spread</code> function to have one column for the rookie and sophomore years batting averages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ROY &lt;-<span class="st"> </span>ROY <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spread</span>(rookie, AVG) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(rookie))</code></pre></div>
<p>We can see the top performers in their first year:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ROY
<span class="co">#&gt; # A tibble: 99 x 6</span>
<span class="co">#&gt;   playerID  rookie_year nameFirst nameLast rookie sophomore</span>
<span class="co">#&gt;   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 mccovwi01        1959 Willie    McCovey   0.354     0.238</span>
<span class="co">#&gt; 2 suzukic01        2001 Ichiro    Suzuki    0.350     0.321</span>
<span class="co">#&gt; 3 bumbral01        1973 Al        Bumbry    0.337     0.233</span>
<span class="co">#&gt; 4 lynnfr01         1975 Fred      Lynn      0.331     0.314</span>
<span class="co">#&gt; 5 pujolal01        2001 Albert    Pujols    0.329     0.314</span>
<span class="co">#&gt; 6 troutmi01        2012 Mike      Trout     0.326     0.323</span>
<span class="co">#&gt; # ... with 93 more rows</span></code></pre></div>
<p>and just by eyeballing, we see the sophomore slump. In fact, the proportion of players that have a lower batting average their sophomore year is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(ROY<span class="op">$</span>sophomore <span class="op">-</span><span class="st"> </span>ROY<span class="op">$</span>rookie <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>)
<span class="co">#&gt; [1] 0.677</span></code></pre></div>
<p>So is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year). We perform similar operations to what we did above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">two_years &lt;-<span class="st"> </span>Batting <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(yearID <span class="op">%in%</span><span class="st"> </span><span class="dv">2013</span><span class="op">:</span><span class="dv">2014</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(playerID, yearID) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">sum</span>(AB) <span class="op">&gt;=</span><span class="st"> </span><span class="dv">130</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">AVG =</span> <span class="kw">sum</span>(H)<span class="op">/</span><span class="kw">sum</span>(AB)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(yearID, AVG) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(<span class="st">`</span><span class="dt">2013</span><span class="st">`</span>) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(<span class="st">`</span><span class="dt">2014</span><span class="st">`</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(playerInfo, <span class="dt">by=</span><span class="st">&quot;playerID&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(POS<span class="op">!=</span><span class="st">&quot;P&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>POS) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="st">`</span><span class="dt">2013</span><span class="st">`</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(nameFirst, nameLast, <span class="st">`</span><span class="dt">2013</span><span class="st">`</span>, <span class="st">`</span><span class="dt">2014</span><span class="st">`</span>)</code></pre></div>
<p>The same pattern arises when we look at the top performers: batting averages go down for the most of the top performers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">two_years
<span class="co">#&gt; # A tibble: 312 x 4</span>
<span class="co">#&gt;   nameFirst nameLast `2013` `2014`</span>
<span class="co">#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;</span>
<span class="co">#&gt; 1 Miguel    Cabrera   0.348  0.313</span>
<span class="co">#&gt; 2 Hanley    Ramirez   0.345  0.283</span>
<span class="co">#&gt; 3 Michael   Cuddyer   0.331  0.332</span>
<span class="co">#&gt; 4 Scooter   Gennett   0.324  0.289</span>
<span class="co">#&gt; 5 Joe       Mauer     0.324  0.277</span>
<span class="co">#&gt; 6 Mike      Trout     0.323  0.287</span>
<span class="co">#&gt; # ... with 306 more rows</span></code></pre></div>
<p>But these are not rookies! Also, look at what happens to the worst performers of 2013:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">arrange</span>(two_years, <span class="st">`</span><span class="dt">2013</span><span class="st">`</span>)
<span class="co">#&gt; # A tibble: 312 x 4</span>
<span class="co">#&gt;   nameFirst nameLast `2013` `2014`</span>
<span class="co">#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;</span>
<span class="co">#&gt; 1 Danny     Espinosa  0.158  0.219</span>
<span class="co">#&gt; 2 Dan       Uggla     0.179  0.149</span>
<span class="co">#&gt; 3 Jeff      Mathis    0.181  0.2  </span>
<span class="co">#&gt; 4 Melvin    Upton     0.184  0.208</span>
<span class="co">#&gt; 5 Adam      Rosales   0.190  0.262</span>
<span class="co">#&gt; 6 Aaron     Hicks     0.192  0.215</span>
<span class="co">#&gt; # ... with 306 more rows</span></code></pre></div>
<p>Their batting averages go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(<span class="st">`</span><span class="dt">2013</span><span class="st">`</span>, <span class="st">`</span><span class="dt">2014</span><span class="st">`</span>, <span class="dt">data =</span> two_years)</code></pre></div>
<p><img src="book_files/figure-html/regression-fallacy-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The correlation is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summarize</span>(two_years, <span class="kw">cor</span>(<span class="st">`</span><span class="dt">2013</span><span class="st">`</span>,<span class="st">`</span><span class="dt">2014</span><span class="st">`</span>))
<span class="co">#&gt; # A tibble: 1 x 1</span>
<span class="co">#&gt;   `cor(\`2013\`, \`2014\`)`</span>
<span class="co">#&gt;                       &lt;dbl&gt;</span>
<span class="co">#&gt; 1                     0.460</span></code></pre></div>
<p>The data look very much like a bivariate normal distribution, which means we predict a 2014 batting average <span class="math inline">\(Y\)</span> for any given player that had a 2013 batting average <span class="math inline">\(X\)</span> with:</p>
<p><span class="math display">\[ \frac{Y - .255}{.032} = 0.46 \left( \frac{X - .261}{.023}\right) \]</span></p>
<p>Because the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of <span class="math inline">\(X\)</span> so it is expected that <span class="math inline">\(Y\)</span> will regress to the mean.</p>
</div>
<div id="measurement-error-models" class="section level2">
<h2><span class="header-section-number">19.9</span> Measurement error models</h2>
<p>Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.</p>
<p>To understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The <strong>dslabs</strong> function <code>rfalling_object</code> generates these simulations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dslabs)
falling_object &lt;-<span class="st"> </span><span class="kw">rfalling_object</span>()</code></pre></div>
<p>The assistants hand the data to Galileo and this is what he sees:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">falling_object <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(time, observed_distance)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Distance in meters&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Time in seconds&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/gravity-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:</p>
<p><span class="math display">\[ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2\]</span></p>
<p>The data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n \]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> representing distance in meters, <span class="math inline">\(x_i\)</span> representing time in seconds, and <span class="math inline">\(\varepsilon\)</span> accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each <span class="math inline">\(i\)</span>. We also assume that there is no bias, which means the expected value <span class="math inline">\(\mbox{E}[\varepsilon] = 0\)</span>.</p>
<p>Note that this is a linear model because it is a linear combination of known quantities (<span class="math inline">\(x\)</span> and <span class="math inline">\(x^2\)</span> are known) and unknown parameters (the <span class="math inline">\(\beta\)</span> s are unknown parameters to Galileo). Unlike our previous examples, here <span class="math inline">\(x\)</span> is a fixed quantity; we are not conditioning.</p>
<p>To pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. The LSE seem like a reasonable approach. How do we find the LSE?</p>
<p>LSE calculations do not require the errors to be approximately normal. The <code>lm</code> function will find the <span class="math inline">\(\beta\)</span> s that will minimize the residual sum of squares:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>falling_object <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">time_sq =</span> time<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">lm</span>(observed_distance<span class="op">~</span>time<span class="op">+</span>time_sq, <span class="dt">data=</span>.)
<span class="kw">tidy</span>(fit)
<span class="co">#&gt; # A tibble: 3 x 5</span>
<span class="co">#&gt;   term        estimate std.error statistic  p.value</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)    56.9      0.580     98.0  1.56e-17</span>
<span class="co">#&gt; 2 time           -1.04     0.829     -1.25 2.36e- 1</span>
<span class="co">#&gt; 3 time_sq        -4.73     0.246    -19.2  8.17e-10</span></code></pre></div>
<p>Let’s check if the estimated parabola fits the data. The <strong>broom</strong> function <code>augment</code> lets us do this easily:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(fit) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(time, observed_distance)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(time, .fitted), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/falling-object-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:</p>
<p><span class="math display">\[d = h_0 + v_0 t -  0.5 \times 9.8 t^2\]</span></p>
<p>with <span class="math inline">\(h_0\)</span> and <span class="math inline">\(v_0\)</span> the starting height and velocity respectively. The data we simulated above followed this equation and added measurement error to simulate <code>n</code> observations for dropping the ball <span class="math inline">\((v_0=0)\)</span> from the tower of Pisa <span class="math inline">\((h_0=55.86)\)</span>.</p>
<p>These are consistent with the parameter estimates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(fit, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)
<span class="co">#&gt; # A tibble: 3 x 7</span>
<span class="co">#&gt;   term        estimate std.error statistic  p.value conf.low conf.high</span>
<span class="co">#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 (Intercept)    56.9      0.580     98.0  1.56e-17    55.6     58.2  </span>
<span class="co">#&gt; 2 time           -1.04     0.829     -1.25 2.36e- 1    -2.86     0.784</span>
<span class="co">#&gt; 3 time_sq        -4.73     0.246    -19.2  8.17e-10    -5.27    -4.19</span></code></pre></div>
<p>The Tower of Pisa height is within the confidence interval for <span class="math inline">\(\beta_0\)</span>, the initial velocity 0 is in the confidence interval for <span class="math inline">\(\beta_1\)</span> (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for <span class="math inline">\(-2 \times \beta_2\)</span>.</p>
</div>
<div id="exercises-34" class="section level2">
<h2><span class="header-section-number">19.10</span> Exercises</h2>
<p>Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples and HR, should be weighed more than singles. As a result, they proposed the following metric:</p>
<p><span class="math display">\[
\frac{\mbox{BB}}{\mbox{PA}} + \frac{\mbox{Singles} + 2 \mbox{Doubles} + 3 \mbox{Triples} + 4\mbox{HR}}{\mbox{AB}}
\]</span></p>
<p>They called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.</p>
<ol style="list-style-type: decimal">
<li><p>Compute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.</p></li>
<li><p>For every year since 1961, compute the correlation between runs per game and OPS then plot these correlations as a function of year.</p></li>
<li><p>Note that we can rewrite OPS as a weighted average of BB, singles, doubles, triples and HR. We know that the weight for doubles, triples, and HR are 2, 3 and 4 times that of singles. But what about BB? What is the weight for BB relative to singles. Hint: the weight for BB relative to singles will be a function of AB and PA.</p></li>
<li><p>Note that the weight for BB, <span class="math inline">\(\frac{\mbox{AB}}{\mbox{PA}}\)</span>, will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.</p></li>
<li><p>So now we know that the formula for OPS is proportional to <span class="math inline">\(0.91 \times \mbox{BB} + \mbox{singles} + 2 \times \mbox{doubles} + 3 \times \mbox{triples} + 4 \times \mbox{HR}\)</span>. Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.</p></li>
<li><p>We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.</p></li>
<li><p>We see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1961 season and after, compute the OPS and the predicted runs from our model for each player and plot them. Use the PA per game correction we used in the previous chapter:</p></li>
<li><p>What players have show the largest difference between their rank by predicted runs and OPS?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="association-is-not-causation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/regression/moneyball-motivation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
